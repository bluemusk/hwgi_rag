import os
import re
import time
import json
import hashlib
import logging
import traceback
import requests
import torch
import argparse
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import streamlit as st
from dotenv import load_dotenv
import asyncio
import time
import io
import base64
import aiohttp
from io import StringIO
from datetime import datetime
import glob
import sys
import tabula
import random
import shutil
import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€
import math
from uuid import uuid4
from concurrent.futures import ThreadPoolExecutor

# ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
DEBUG_MODE = False

# OLLAMA_AVAILABLE ë³€ìˆ˜ ì •ì˜
import ollama
OLLAMA_AVAILABLE = True
   
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
SCRIPT_DIR = os.getcwd()
BASE_DIR = os.path.dirname(SCRIPT_DIR)  # ìƒìœ„ ë””ë ‰í† ë¦¬
print(f"í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ê²½ë¡œ: {SCRIPT_DIR}")
print(f"ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ: {BASE_DIR}")


# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenMP ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ (FAISSì™€ Java ì¶©ëŒ ë°©ì§€)
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pypdf
import tabula
import pdfplumber

# RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from sentence_transformers import SentenceTransformer
import faiss

try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from rank_bm25 import BM25Okapi
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ langchain_community.embeddings ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# RAG í‰ê°€ ê´€ë ¨ ë©”íŠ¸ë¦­ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from rank_bm25 import BM25Okapi
    EVAL_LIBS_AVAILABLE = True
except ImportError:
    EVAL_LIBS_AVAILABLE = False

# í™˜ê²½ ì„¤ì •
PDF_PATH = os.path.join(SCRIPT_DIR, "[í•œí™”ì†í•´ë³´í—˜]ì‚¬ì—…ë³´ê³ ì„œ(2025.03.11).pdf")
INDEX_DIR = os.path.join(SCRIPT_DIR, "Index")  # ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ê¸°ë³¸ ê²½ë¡œ
METADATA_FILE = os.path.join(SCRIPT_DIR, "Index/document_metadata_bge.json")  # ë©”íƒ€ë°ì´í„° íŒŒì¼
LOG_FILE = os.path.join(SCRIPT_DIR, "Log/hwgi_rag_streamlit.log")
CACHE_FILE = os.path.join(SCRIPT_DIR, "cache.json")
EVALUATION_FILE = os.path.join(SCRIPT_DIR, "Log/model_evaluations.json")  # ëª¨ë¸ í‰ê°€ ê²°ê³¼ ì €ì¥ íŒŒì¼

# Ollama API ê¸°ë³¸ URL ì„¤ì •
OLLAMA_API_BASE = "http://localhost:11434/api"

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„¤ì •
AVAILABLE_MODELS = ["gemma3:12b"]

# ëª¨ë¸ ì„¤ì •
EMBEDDING_MODELS = {
    "bge-m3": {
        "name": "BAAI/bge-m3",
        "index_dir": INDEX_DIR,
        "metadata_file": METADATA_FILE
    }
}

# ë¡œê¹… ì„¤ì •
def setup_logging(log_level=logging.DEBUG):
    logger = logging.getLogger('hwgi_rag')
    logger.setLevel(log_level)
    if not logger.handlers:
        file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
        file_handler.setLevel(log_level)
        console_handler = logging.StreamHandler()
        console_handler.setLevel(log_level)
        log_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(log_format)
        console_handler.setFormatter(log_format)
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
    return logger

logger = setup_logging()

# E5Embeddings í´ë˜ìŠ¤ë¥¼ BGE-M3 ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
class BGEM3Embeddings(Embeddings):
    def __init__(self, model_name: str = "BAAI/bge-m3"):
        print(f"âœ“ BGE-M3 ì„ë² ë”© ëª¨ë¸ '{model_name}' ì´ˆê¸°í™” ì¤‘...")
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("âœ“ Apple Silicon GPU (MPS) ì‚¬ìš© ê°€ëŠ¥")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print("âœ“ NVIDIA GPU (CUDA) ì‚¬ìš© ê°€ëŠ¥")
        else:
            self.device = torch.device("cpu")
            print("âœ“ CPU ì‚¬ìš©")
        
        self.model.to(self.device)
        print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (32 â†’ 64)
            batch_size = 64
            all_embeddings = []
            
            # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¡œ ë°°ì¹˜ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = [self._preprocess_text(text) for text in texts[i:i + batch_size]]
                with torch.inference_mode():
                    embeddings = self.model.encode(
                        batch,
                        convert_to_tensor=True,
                        device=self.device,
                        normalize_embeddings=True  # L2 ì •ê·œí™” ì ìš©
                    )
                    if self.device.type == "mps":
                        embeddings = embeddings.to("cpu")
                    all_embeddings.extend(embeddings.cpu().numpy().tolist())
            
            return all_embeddings
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ì¿¼ë¦¬ìš© ì ‘ë‘ì‚¬ ì¶”ê°€
            query_text = f"query: {text}"
            with torch.inference_mode():
                embedding = self.model.encode(
                    [query_text],
                    convert_to_tensor=True,
                    device=self.device,
                    normalize_embeddings=True
                )
                # MPS ë””ë°”ì´ìŠ¤ì—ì„œ CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜
                if self.device.type == "mps":
                    embedding = embedding.to("cpu")
                return embedding.cpu().numpy().tolist()[0]
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

# OpenAI ì„ë² ë”© í´ë˜ìŠ¤ ì •ì˜
class OpenAIEmbeddings(Embeddings):
    def __init__(self, model_name: str = "text-embedding-3-small"):
        print(f"âœ“ OpenAI ì„ë² ë”© ëª¨ë¸ '{model_name}' ì´ˆê¸°í™” ì¤‘...")
        load_dotenv()  # .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ë¡œë“œ
        self.model_name = model_name
        self.client = OpenAI()
        print("âœ“ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë°°ì¹˜ í¬ê¸° ì„¤ì • (OpenAI API ì œí•œ ê³ ë ¤)
            batch_size = 100
            all_embeddings = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=batch,
                    encoding_format="float"
                )
                batch_embeddings = [data.embedding for data in response.data]
                all_embeddings.extend(batch_embeddings)
            
            return all_embeddings
        except Exception as e:
            print(f"âŒ OpenAI ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            response = self.client.embeddings.create(
                model=self.model_name,
                input=[text],
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ OpenAI ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

# --- PDF ì²˜ë¦¬ ë° ë¬¸ì„œ ë¶„í•  ---
class PDFProcessor:
    def __init__(self, pdf_path: str):
        # ê²½ë¡œê°€ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
        if not os.path.isabs(pdf_path):
            self.pdf_path = os.path.join(SCRIPT_DIR, pdf_path)
        else:
            self.pdf_path = pdf_path
        self.text_content = []  # í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥
        self.tables = []  # í‘œ ë°ì´í„° ì €ì¥
        self.page_count = 0  # ì´ í˜ì´ì§€ ìˆ˜
        self.pdf_hash = self._calculate_pdf_hash()  # PDF íŒŒì¼ í•´ì‹œ
        self.hash_file = os.path.join(SCRIPT_DIR, "pdf_hash.json")  # í•´ì‹œ ì €ì¥ íŒŒì¼
        logger.info(f"PDFProcessor ì´ˆê¸°í™”: '{self.pdf_path}' íŒŒì¼ ì²˜ë¦¬ ì¤€ë¹„")
    
    def _calculate_pdf_hash(self) -> str:
        """PDF íŒŒì¼ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_hash = hashlib.md5(file.read()).hexdigest()
            return pdf_hash
        except Exception as e:
            logger.error(f"PDF í•´ì‹œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return ""
    
    def _load_previous_hash(self) -> str:
        """ì´ì „ì— ì²˜ë¦¬í•œ PDFì˜ í•´ì‹œê°’ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            if os.path.exists(self.hash_file):
                with open(self.hash_file, 'r') as f:
                    data = json.load(f)
                    return data.get('pdf_hash', '')
            return ''
        except Exception as e:
            logger.error(f"ì´ì „ í•´ì‹œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return ''
    
    def _save_current_hash(self):
        """í˜„ì¬ PDF í•´ì‹œê°’ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            os.makedirs(os.path.dirname(self.hash_file), exist_ok=True)
            data = {
                'pdf_hash': self.pdf_hash,
                'pdf_path': self.pdf_path,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            with open(self.hash_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ í˜„ì¬ PDF í•´ì‹œ ì €ì¥ ì™„ë£Œ: {self.hash_file}")
        except Exception as e:
            print(f"âš ï¸ í˜„ì¬ í•´ì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"í˜„ì¬ í•´ì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def needs_processing(self) -> bool:
        """PDF íŒŒì¼ì˜ ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. í•´ì‹œê°’ì´ ë‹¤ë¥´ë©´ ì¬ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
        previous_hash = self._load_previous_hash()
        needs_processing = previous_hash != self.pdf_hash
        
        if not needs_processing:
            logger.info("ì´ì „ì— ì²˜ë¦¬ëœ ë™ì¼í•œ PDF íŒŒì¼ ê°ì§€. ë³€ê²½ ì—†ìŒìœ¼ë¡œ íŒë‹¨.")
        
        return needs_processing
    
    def force_processing(self) -> bool:
        """PDF íŒŒì¼ì˜ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë„ë¡ ê°•ì œ ì„¤ì •í•©ë‹ˆë‹¤."""
        # í•´ì‹œ íŒŒì¼ ì‚­ì œë¥¼ í†µí•´ ê°•ì œ ì²˜ë¦¬
        if os.path.exists(self.hash_file):
            try:
                os.remove(self.hash_file)
                logger.info(f"PDF í•´ì‹œ íŒŒì¼ ì‚­ì œ: {self.hash_file}")
                print(f"âœ“ PDF í•´ì‹œ íŒŒì¼ ì‚­ì œë¨ - ê°•ì œ ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™”")
                return True
            except Exception as e:
                logger.error(f"PDF í•´ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                print(f"âš ï¸ PDF í•´ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                return False
        return True
    
    def extract_text(self) -> List[Document]:
        logger.info("ğŸ“„ PDF í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ ì‹œì‘")
        print("ğŸ“„ PDF í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
        documents = []
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)
                self.page_count = len(pdf_reader.pages)
                
                for page_num, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    if text.strip():
                        doc_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                        
                        # í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥
                        self.text_content.append({
                            "page": page_num + 1,
                            "content": text,
                            "hash": doc_hash
                        })
                        
                        documents.append(
                            Document(
                                page_content=text,
                                metadata={"page": page_num + 1, "source": "text", "hash": doc_hash}
                            )
                        )
            logger.info(f"âœ… ì´ {self.page_count}í˜ì´ì§€ì—ì„œ {len(documents)}ê°œì˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ì¶œ ì™„ë£Œ")
            return documents
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def table_id_generation(self, element):
        """í…Œì´ë¸” ID ìƒì„± í•¨ìˆ˜"""
        if "Table" not in element:
            return {}
        else:
            values = element['Table']  # list of tables
            keys = [f"element{element['id']}-table{i}" for i in range(len(values))]
            return dict(zip(keys, values))

    def extract_cell_color(self, table):
        """í…Œì´ë¸”ì˜ ì²« ì…€ê³¼ ë§ˆì§€ë§‰ ì…€ ìƒ‰ìƒ ì¶”ì¶œ í•¨ìˆ˜"""
        page_image = table.page.to_image()
        cell_image_first = page_image.original.crop(table.cells[0])
        cell_image_last = page_image.original.crop(table.cells[-1])
        
        res_list = []
        for cell_image in [cell_image_first, cell_image_last]:
            width, height = cell_image.size
            background_pixel = cell_image.getpixel((width/5, height/5))
            res_list.append(background_pixel)
        
        return res_list

    def extract_image(self, table, resolution=300):
        """í…Œì´ë¸” ì´ë¯¸ì§€ ì¶”ì¶œ í•¨ìˆ˜"""
        scale_factor = resolution / 72
        x0, y0, x1, y1 = table.bbox
        
        x0 *= scale_factor
        y0 *= scale_factor
        x1 *= scale_factor
        y1 *= scale_factor
        
        img = table.page.to_image(resolution=resolution)
        table_img = img.original.crop((x0, y0, x1, y1))
        
        return table_img

    def extract_table_info(self, table):
        """í…Œì´ë¸” ë©”íƒ€ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜"""
        table_dict = {
            'page': self.extract_page_number(str(table.page)),
            'bbox': table.bbox,
            'ncol': len(table.columns),
            'nrow': len(table.rows),
            'content': table.extract(),
            'cell_color': self.extract_cell_color(table),
            'img': self.extract_image(table)
        }
        
        return table_dict

    def compare_tables(self, table_A, table_B):
        """ë‘ í…Œì´ë¸”ì´ ê°™ì€ í…Œì´ë¸”ì¸ì§€ ë¹„êµ"""
        prev_info = self.extract_table_info(table_A)
        curr_info = self.extract_table_info(table_B)
        
        counter = 0
        # ë‘ í…Œì´ë¸”ì˜ í˜ì´ì§€ê°€ ì¸ì ‘í•´ ìˆëŠ”ê°€?
        if curr_info['page'] - prev_info['page'] == 1:
            counter += 1
        
        # í…Œì´ë¸” ìœ„ì¹˜ê°€ ì´ì–´ì§€ëŠ”ê°€?
        if (np.round(prev_info['bbox'][3], 0) > 780) and (np.round(curr_info['bbox'][1], 0) == 50):
            counter += 1
        
        # ì…€ ìƒ‰ìƒì´ ê°™ì€ê°€?
        if prev_info['cell_color'][1] == curr_info['cell_color'][0]:
            counter += 1
        
        # ì»¬ëŸ¼ ìˆ˜ê°€ ê°™ì€ê°€?
        if prev_info['ncol'] == curr_info['ncol']:
            counter += 1
        
        decision = 'same table' if counter == 4 else 'different table'
        return [(counter, decision)]

    def find_table_location_in_text(self, element_content):
        """ì½˜í…ì¸  ë‚´ í…Œì´ë¸” ìœ„ì¹˜ ì°¾ê¸°"""
        start_pattern = '<table>'
        table_start_position = re.finditer(start_pattern, element_content)
        start_positions = [(match.start(), match.end()) for match in table_start_position]
        
        end_pattern = '</table>'
        table_end_position = re.finditer(end_pattern, element_content)
        end_positions = [(match.start(), match.end()) for match in table_end_position]
        
        table_location_in_text = [(start[0], end[1]) 
                                for start, end in zip(start_positions, end_positions)]
        
        return table_location_in_text

    def group_table_position(self, element_table):
        """ì—°ì†ëœ í…Œì´ë¸”ì˜ í¬ì§€ì…˜ì„ ë¬¶ì–´ì£¼ëŠ” í•¨ìˆ˜"""
        pos = 0
        counter = 0
        result = []
        
        for i in range(1, len(element_table)):
            counter += 1
            table_comparison_result = self.compare_tables(element_table[i-1], element_table[i])[0][1]
            
            if table_comparison_result != 'same table':
                result.append([pos, pos+counter])
                pos += counter
                counter = 0
        
        # ë§ˆì§€ë§‰ ê·¸ë£¹ ì¶”ê°€
        result.append([pos, pos + counter + 1])
        return result

    def merge_dicts(self, dict_list):
        """ì—¬ëŸ¬ ê°œì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜"""
        merged_dict = {
            'page': [],
            'bbox': [],
            'ncol': 0,
            'nrow': 0,
            'content': [],
            'cell_color': [],
            'img': [],
            'obj_type': ['table']
        }
        
        for d in dict_list:
            merged_dict['page'].append(d['page'])
            merged_dict['bbox'].append(d['bbox'])
            merged_dict['ncol'] = max(merged_dict['ncol'], d['ncol'])
            merged_dict['nrow'] += d['nrow']
            merged_dict['content'] += (d['content'])
            merged_dict['cell_color'].append(d['cell_color'])
            merged_dict['img'].append(d['img'])
        
        return merged_dict

    def extract_page_number(self, text):
        """í…Œì´ë¸”ì´ ìœ„ì¹˜í•œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ í•¨ìˆ˜"""
        match = re.search(r"<Page:(\d+)>", text)
        return int(match.group(1)) if match else None

    def extract_tables(self) -> List[Document]:
        """PDFì—ì„œ í‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  Documentë¡œ ë³€í™˜"""
        logger.info("ğŸ“Š PDF í‘œ ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
        print("ğŸ“Š PDF í‘œ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        try:
            table_documents = []
            
            with pdfplumber.open(self.pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    tables = page.extract_tables()
                    if not tables:
                        continue
                    
                    for table_idx, table in enumerate(tables):
                        # ë¹ˆ í–‰/ì—´ ì œê±° ë° ë¬¸ìì—´ ë³€í™˜
                        table_content = []
                        for row in table:
                            if any(cell for cell in row):  # ë¹ˆ í–‰ ì œì™¸
                                cleaned_row = [str(cell).strip() if cell else "" for cell in row]
                                table_content.append(cleaned_row)
                        
                        if not table_content:
                            continue
                        
                        # CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        table_text = '\n'.join([','.join(row) for row in table_content])
                        table_hash = hashlib.md5(table_text.encode('utf-8')).hexdigest()
                        
                        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        metadata = {
                            'table_id': f'table_p{page_num}_t{table_idx + 1}',
                            'page': page_num,
                            'source': 'table',
                            'hash': table_hash,
                            'row_count': len(table_content),
                            'col_count': len(table_content[0]) if table_content else 0
                        }
                        
                        # í‘œ ì •ë³´ ì €ì¥
                        self.tables.append({
                            'table_id': metadata['table_id'],
                            'content': table_text,
                            'raw_data': table_content,
                            'hash': table_hash,
                            'metadata': metadata
                        })
                        
                        # Document ê°ì²´ ìƒì„±
                        table_documents.append(
                            Document(
                                page_content=f"í‘œ {metadata['table_id']}:\n{table_text}",
                                metadata=metadata
                            )
                        )
            
            logger.info(f"âœ… {len(table_documents)}ê°œì˜ í‘œ ì²˜ë¦¬ ì™„ë£Œ")
            return table_documents
            
        except Exception as e:
            logger.error(f"âŒ í‘œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def process(self) -> List[Document]:
        """PDFë¥¼ ì²˜ë¦¬í•˜ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        print(f"\n{'â”€'*60}")
        print("ğŸ“Œ 1ë‹¨ê³„: PDF ë¬¸ì„œ ì²˜ë¦¬")
        print(f"{'â”€'*60}")
        
        if not self.needs_processing():
            logger.info("ì´ì „ì— ì²˜ë¦¬ëœ ë™ì¼í•œ PDF íŒŒì¼ ê°ì§€. ë³€ê²½ ì—†ìŒìœ¼ë¡œ íŒë‹¨.")
            print("âœ“ ì´ë¯¸ ì²˜ë¦¬ëœ PDF íŒŒì¼ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ì²˜ë¦¬ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.")
            # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš©í•˜ë„ë¡ í•¨
            return []
        
        logger.info("===== PDF ì²˜ë¦¬ ì‹œì‘ =====")
        text_docs = self.extract_text()
        table_docs = self.extract_tables()
        all_docs = text_docs + table_docs
        
        if not all_docs:
            print("âš ï¸ PDFì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []
        
        # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ë©´ í˜„ì¬ í•´ì‹œ ì €ì¥
        self._save_current_hash()
        logger.info(f"ğŸ“š {len(all_docs)}ê°œì˜ ë¬¸ì„œ ì¡°ê° ìƒì„±ë¨")
        print(f"ğŸ“š PDF ì²˜ë¦¬ ì™„ë£Œ: {len(text_docs)}ê°œ í…ìŠ¤íŠ¸ ë¬¸ì„œ, {len(table_docs)}ê°œ í‘œ ë¬¸ì„œ ìƒì„±")
        return all_docs
    
    def visualize_table(self, table_id: int):
        """íŠ¹ì • í‘œë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤ (matplotlib ì‚¬ìš©)"""
        if not self.tables:
            print("âš ï¸ í‘œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìœ íš¨í•œ table_id í™•ì¸
        table_index = table_id - 1
        if table_index < 0 or table_index >= len(self.tables):
            print(f"âš ï¸ í‘œ #{table_id}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        try:
            table_data = self.tables[table_index]
            df = pd.DataFrame(table_data["raw_data"])
            
            # í‘œ ì‹œê°í™”
            fig, ax = plt.figure(figsize=(12, 6)), plt.gca()
            ax.axis('off')
            ax.table(
                cellText=df.values,
                colLabels=df.columns,
                cellLoc='center',
                loc='center'
            )
            plt.title(f"í‘œ {table_data['table_id']}", fontsize=14)
            plt.tight_layout()
            plt.show()
            
            # í…Œì´ë¸” ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“Š í‘œ {table_data['table_id']} ì •ë³´:")
            print(f"  - í–‰ ìˆ˜: {df.shape[0]}")
            print(f"  - ì—´ ìˆ˜: {df.shape[1]}")
            print(f"  - ì—´ ì´ë¦„: {', '.join(df.columns)}")
            
        except Exception as e:
            print(f"âŒ í‘œ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

class DocumentSplitter:
    def __init__(self, chunk_size=800, chunk_overlap=200):  # ì²­í¬ ì‚¬ì´ì¦ˆ ì¦ê°€ (500 â†’ 800), ê²¹ì¹¨ í¬ê¸° ì¦ê°€ (150 â†’ 200)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            # êµ¬ë¶„ì ìµœì í™”: ë¬¸ë‹¨ > ë¬¸ì¥ > êµ¬ë‘ì  > ê³µë°± ìˆœì„œë¡œ ì‹œë„
            separators=[
                "\n\n",  # ë¬¸ë‹¨ êµ¬ë¶„
                "\n",    # ì¤„ë°”ê¿ˆ
                ".",     # ë¬¸ì¥ ë
                "!",     # ê°íƒ„ë¬¸
                "?",     # ì˜ë¬¸ë¬¸
                ";",     # ì„¸ë¯¸ì½œë¡ 
                ":",     # ì½œë¡ 
                ",",     # ì‰¼í‘œ
                " ",     # ê³µë°±
                ""       # ë§ˆì§€ë§‰ ìˆ˜ë‹¨
            ]
        )
        logger.info(f"DocumentSplitter ì´ˆê¸°í™”: ì²­í¬ í¬ê¸°={chunk_size}, ê²¹ì¹¨={chunk_overlap}")
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        logger.info(f"ğŸ”ª ë¬¸ì„œ ë¶„í•  ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
        print("ğŸ”ª ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
        try:
            chunks = self.text_splitter.split_documents(documents)
            for i, chunk in enumerate(chunks):
                chunk.metadata['chunk_id'] = f"chunk_{i}"
            logger.info(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")
            return chunks
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜: {e}")
            return documents

# --- ì¿¼ë¦¬ í™•ì¥ê¸° í´ë˜ìŠ¤ ---
class QueryExpander:
    """ì¿¼ë¦¬ í™•ì¥ í´ë˜ìŠ¤"""
    def __init__(self, model_name: str = None):
        """QueryExpander ì´ˆê¸°í™”"""
        # ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€
        if hasattr(QueryExpander, '_instance'):
            self.model = QueryExpander._instance.model
            self.models = QueryExpander._instance.models
            logger.info(f"QueryExpander ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©: {self.model}")
            return
        
        # ëª¨ë¸ ì„¤ì •
        self.models = check_ollama_models()
        
        # ëª¨ë¸ ìë™ ì„ íƒ ë˜ëŠ” ì§€ì •ëœ ëª¨ë¸ ì‚¬ìš©
        if model_name and model_name in self.models:
            self.model = model_name
            logger.info(f"QueryExpander ëª¨ë¸ ì§€ì •: {model_name}")
        elif self.models:
            self.model = self.models[0]
            logger.info(f"QueryExpander ëª¨ë¸ ìë™ ì„ íƒ: {self.model}")
        else:
            self.model = "gemma3:7b" # ê¸°ë³¸ ëª¨ë¸
            logger.warning(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ì–´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {self.model}")
        
        # ì¸ìŠ¤í„´ìŠ¤ ì €ì¥
        QueryExpander._instance = self
    
    def expand_query(self, query: str, max_queries: int = 5) -> List[str]:
        """
        ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ì–‘í•œ ê²€ìƒ‰ìš© ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        - ì›ë³¸ ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¬êµ¬ì„±í•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ í™•ì¥
        - ë°˜í™˜ê°’ì€ ì›ë³¸ ì¿¼ë¦¬ë¥¼ í¬í•¨í•œ í™•ì¥ ì¿¼ë¦¬ ëª©ë¡
        """
        logger.info(f"ğŸ” ì¿¼ë¦¬ í™•ì¥ ì‹œì‘: '{query}'")
        print(f"ğŸ” ì¿¼ë¦¬ í™•ì¥ ìƒì„± ì¤‘: '{query}'")
        
        try:
            if not query.strip():
                logger.warning("ë¹ˆ ì¿¼ë¦¬ëŠ” í™•ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return [query]
            
            # ì—”í‹°í‹° íƒ€ì… ê°ì§€ (ì¸ë¬¼, ìƒí’ˆ, ê¸°ìˆ  ë“±)
            entity_type, entity_name = self._detect_entity(query)
            
            # íŠ¹ìˆ˜ ì—”í‹°í‹°ê°€ ê°ì§€ëœ ê²½ìš°
            if entity_type and entity_name:
                logger.info(f"íŠ¹ìˆ˜ ì—”í‹°í‹° ê°ì§€: ìœ í˜•={entity_type}, ì´ë¦„='{entity_name}'")
                print(f"ğŸ” {entity_type} ê²€ìƒ‰ ê°ì§€: '{entity_name}'")
                
                # ì—”í‹°í‹° ìœ í˜•ë³„ íŠ¹í™” ì¿¼ë¦¬ ìƒì„±
                return self._generate_entity_specific_queries(entity_type, entity_name, query, max_queries)
            
            # ì¼ë°˜ ì¿¼ë¦¬ í™•ì¥ í”„ë¡¬í”„íŠ¸
            prompt = f"""
ë‹¹ì‹ ì€ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ëŠ” RAG ì‹œìŠ¤í…œì˜ ì¿¼ë¦¬ í™•ì¥ê¸°ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ì—ì„œ ë‹µì„ ì°¾ê¸° ìœ„í•œ ê´€ë ¨ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ìƒì„±í•´ ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: "{query}"

ì¤‘ìš”: ë°˜ë“œì‹œ ì›ë³¸ ì§ˆë¬¸ê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ìˆëŠ” ì¿¼ë¦¬ë§Œ ìƒì„±í•˜ì„¸ìš”. 
ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ì ì¸ ì¿¼ë¦¬ëŠ” ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë³€í˜•í•´ ì£¼ì„¸ìš”:
1. ë™ì˜ì–´ë‚˜ ìœ ì‚¬ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ê¸°
2. êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ ë³€í™˜í•˜ê¸°
3. ê´€ë ¨ëœ ì„¸ë¶€ ê°œë…ìœ¼ë¡œ í™•ì¥í•˜ê¸° (ë‹¨, ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ìœ ì§€í•´ì•¼ í•¨)
4. ì§ˆë¬¸í˜•/í‚¤ì›Œë“œí˜• ë“± ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•˜ê¸°

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ ì£¼ì„¸ìš”:
```json
{{
  "queries": [
    "ì²« ë²ˆì§¸ ì¿¼ë¦¬",
    "ë‘ ë²ˆì§¸ ì¿¼ë¦¬",
    "ì„¸ ë²ˆì§¸ ì¿¼ë¦¬",
    ...
  ]
}}
```
JSON í˜•ì‹ë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
ìµœëŒ€ {max_queries}ê°œì˜ ê´€ë ¨ì„± ë†’ì€ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
"""
            
            # Ollama APIë¥¼ í†µí•´ ì¿¼ë¦¬ í™•ì¥ ìƒì„±
            response_text = self._generate_query_expansion(prompt)
            
            # JSON íŒŒì‹±
            try:
                # JSON í˜•ì‹ ê²€ìƒ‰
                json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(1)
                
                # ì¤‘ê´„í˜¸ ê¸°ë°˜ JSON ì¶”ì¶œ
                json_match = re.search(r'{.*}', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(0)
                
                # JSON íŒŒì‹±
                response = json.loads(response_text)
                
                # ì¿¼ë¦¬ ëª©ë¡ ì¶”ì¶œ
                if "queries" in response:
                    expanded_queries = response["queries"]
                    
                    # ì¤‘ë³µ ì œê±° ë° ë¹ˆ ì¿¼ë¦¬ ì œê±°
                    expanded_queries = [q.strip() for q in expanded_queries if q.strip()]
                    
                    # ê´€ë ¨ì„± í•„í„°ë§ - ì›ë³¸ ì¿¼ë¦¬ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¿¼ë¦¬ë§Œ ìœ ì§€
                    keywords = self._extract_keywords(query)
                    if keywords:
                        # ì ì–´ë„ í•˜ë‚˜ì˜ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¿¼ë¦¬ë§Œ ìœ ì§€
                        filtered_queries = []
                        for q in expanded_queries:
                            if any(keyword.lower() in q.lower() for keyword in keywords):
                                filtered_queries.append(q)
                        expanded_queries = filtered_queries
                    
                    expanded_queries = list(dict.fromkeys(expanded_queries))
                    
                    # ì›ë³¸ ì¿¼ë¦¬ê°€ ëª©ë¡ì— ì—†ìœ¼ë©´ ì¶”ê°€
                    if query not in expanded_queries:
                        expanded_queries.insert(0, query)
                    
                    # ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜ ì œí•œ
                    expanded_queries = expanded_queries[:max_queries]
                    
                    logger.info(f"âœ… ì¿¼ë¦¬ í™•ì¥ ì™„ë£Œ: {len(expanded_queries)}ê°œ ìƒì„±")
                    logger.info(f"í™•ì¥ ì¿¼ë¦¬: {expanded_queries}")
                    print(f"âœ… í™•ì¥ ì¿¼ë¦¬ {len(expanded_queries)}ê°œ ìƒì„± ì™„ë£Œ")
                    return expanded_queries
                else:
                    logger.warning("ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼ì— 'queries' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return [query]
            
            except json.JSONDecodeError as e:
                logger.error(f"ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                logger.error(f"ì›ë³¸ ì‘ë‹µ: {response_text}")
                return [query]
        
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(traceback.format_exc())
            return [query]
    
    def _detect_entity(self, query: str) -> Tuple[Optional[str], Optional[str]]:
        """ì¿¼ë¦¬ì—ì„œ íŠ¹ìˆ˜ ì—”í‹°í‹°(ì¸ë¬¼, ìƒí’ˆ, ê¸°ìˆ  ë“±)ë¥¼ ê°ì§€í•©ë‹ˆë‹¤."""
        # 1. ì¸ë¬¼ ê°ì§€ (ì´ë¦„ + ì¡°ì‚¬ + ì˜ë¬¸ì‚¬)
        person_pattern = r'^([ê°€-í£]{2,4}|[a-zA-Z\s]{2,20})(?:ì€|ëŠ”|ì´|ê°€)?\s+(?:ëˆ„êµ¬|ì–´ë–¤|ì–´ëŠ|ë¬´ìŠ¨)'
        person_match = re.search(person_pattern, query)
        if person_match:
            name = person_match.group(1).strip()
            return "ì¸ë¬¼", name
            
        # 2. ìƒí’ˆ/ì„œë¹„ìŠ¤ ê°ì§€
        product_pattern = r'(?:ìƒí’ˆ|ì„œë¹„ìŠ¤|ë³´í—˜)\s+([ê°€-í£a-zA-Z0-9\s]{2,20})(?:ì´|ê°€|ì€|ëŠ”)?\s+(?:ë¬´ì—‡|ë­|ì–´ë–¤|ì–´ë–»ê²Œ)'
        product_match = re.search(product_pattern, query)
        if product_match:
            product = product_match.group(1).strip()
            return "ìƒí’ˆ", product
            
        # 3. ê¸°ìˆ /ê¸°ëŠ¥ ê°ì§€
        tech_pattern = r'(?:ê¸°ìˆ |ê¸°ëŠ¥|ì‹œìŠ¤í…œ|í”Œë«í¼)\s+([ê°€-í£a-zA-Z0-9\s]{2,20})(?:ì´|ê°€|ì€|ëŠ”)?\s+(?:ë¬´ì—‡|ë­|ì–´ë–¤|ì–´ë–»ê²Œ)'
        tech_match = re.search(tech_pattern, query)
        if tech_match:
            tech = tech_match.group(1).strip()
            return "ê¸°ìˆ ", tech
            
        # 4. ì¸ëª…ë§Œ ìˆëŠ” ê²½ìš° (ì§ì ‘ ì–¸ê¸‰)
        name_only_pattern = r'^([ê°€-í£]{2,4}|[a-zA-Z\s]{2,20})$'
        name_only_match = re.search(name_only_pattern, query)
        if name_only_match:
            name = name_only_match.group(1).strip()
            return "ì¸ë¬¼", name
            
        # ê°ì§€ëœ ì—”í‹°í‹°ê°€ ì—†ëŠ” ê²½ìš°
        return None, None
            
    def _extract_keywords(self, query: str) -> List[str]:
        """ì›ë³¸ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {"ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ì˜", "ê³¼", "ì™€", "ë¡œ", "ìœ¼ë¡œ", 
                    "ì´ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "í•˜ë‹¤", "ë˜ë‹¤", "í•œë‹¤", "ëœë‹¤", "ë¬´ì—‡", "ëˆ„êµ¬", "ì–´ë””", 
                    "ì–¸ì œ", "ì™œ", "ì–´ë–»ê²Œ", "ì–´ë–¤", "ì–¼ë§ˆë‚˜", "ëª‡", "ë¬´ìŠ¨"}
        
        # ì¡°ì‚¬ì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        cleaned_query = re.sub(r'[^\w\s]', ' ', query)
        words = cleaned_query.split()
        
        # ê¸¸ì´ê°€ 1ì¸ ë‹¨ì–´ì™€ ë¶ˆìš©ì–´ ì œê±°
        keywords = [word for word in words if len(word) > 1 and word not in stopwords]
        return keywords
    
    def _generate_entity_specific_queries(self, entity_type: str, entity_name: str, original_query: str, max_queries: int) -> List[str]:
        """íŠ¹ì • ì—”í‹°í‹° ìœ í˜•ì— íŠ¹í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        logger.info(f"'{entity_name}'({entity_type})ì— ëŒ€í•œ íŠ¹í™” ì¿¼ë¦¬ ìƒì„± ì¤‘")
        
        # ê¸°ë³¸ ì¿¼ë¦¬ëŠ” í•­ìƒ ì›ë³¸ ì¿¼ë¦¬ë¡œ ì‹œì‘
        result_queries = [original_query]
        
        # ì—”í‹°í‹° ìœ í˜•ë³„ íŠ¹í™” ì¿¼ë¦¬ ìƒì„±
        if entity_type == "ì¸ë¬¼":
            specific_queries = [
                f"{entity_name}",
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜",
                f"{entity_name} ì§ì±…",
                f"{entity_name} ì§ìœ„",
                f"{entity_name} ê²½ë ¥",
                f"{entity_name} ë‹´ë‹¹",
                f"{entity_name} í”„ë¡œí•„",
                f"{entity_name} ì´ë ¥",
                f"{entity_name} ì—…ë¬´",
                f"{entity_name} ì†Œê°œ",
                f"í•œí™”ì†í•´ë³´í—˜ {entity_name}",
                f"ì´ì‚¬ {entity_name}",
                f"ì„ì› {entity_name}",
                f"ì‚¬ì¥ {entity_name}",
                f"ë¶€ì‚¬ì¥ {entity_name}",
                f"ì „ë¬´ {entity_name}",
                f"ìƒë¬´ {entity_name}",
                f"ë³¸ë¶€ì¥ {entity_name}",
                f"ì‹¤ì¥ {entity_name}",
                f"íŒ€ì¥ {entity_name}"
            ]
            result_queries.extend(specific_queries)
            
        elif entity_type == "ìƒí’ˆ":
            specific_queries = [
                f"{entity_name}",
                f"{entity_name} ìƒí’ˆ",
                f"{entity_name} ë³´í—˜",
                f"{entity_name} ì„œë¹„ìŠ¤",
                f"{entity_name} íŠ¹ì§•",
                f"{entity_name} ì„¤ëª…",
                f"{entity_name} ì†Œê°œ",
                f"{entity_name} ì¥ì ",
                f"{entity_name} ì¡°ê±´",
                f"{entity_name} ê³„ì•½",
                f"í•œí™”ì†í•´ë³´í—˜ {entity_name}",
                f"{entity_name} ë³´ì¥",
                f"{entity_name} ë³´í—˜ë£Œ",
                f"{entity_name} íŒë§¤",
                f"{entity_name} ê°€ì…"
            ]
            result_queries.extend(specific_queries)
            
        elif entity_type == "ê¸°ìˆ ":
            specific_queries = [
                f"{entity_name}",
                f"{entity_name} ê¸°ìˆ ",
                f"{entity_name} ì‹œìŠ¤í…œ",
                f"{entity_name} í”Œë«í¼",
                f"{entity_name} ì ìš©",
                f"{entity_name} í™œìš©",
                f"{entity_name} êµ¬í˜„",
                f"{entity_name} ë„ì…",
                f"{entity_name} íŠ¹ì§•",
                f"{entity_name} íš¨ê³¼",
                f"í•œí™”ì†í•´ë³´í—˜ {entity_name}",
                f"{entity_name} ê°œë°œ",
                f"{entity_name} í˜ì‹ ",
                f"{entity_name} íˆ¬ì"
            ]
            result_queries.extend(specific_queries)
            
        else:
            # ê¸°íƒ€ ì—”í‹°í‹° ìœ í˜•ì— ëŒ€í•œ ì¼ë°˜ì ì¸ í™•ì¥
            specific_queries = [
                f"{entity_name}",
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜",
                f"í•œí™”ì†í•´ë³´í—˜ {entity_name}",
                f"{entity_name} ì„¤ëª…",
                f"{entity_name} ë‚´ìš©",
                f"{entity_name} ì†Œê°œ",
                f"{entity_name} ì •ë³´"
            ]
            result_queries.extend(specific_queries)
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        result_queries = list(dict.fromkeys(result_queries))[:max_queries]
        
        logger.info(f"{entity_type} íŠ¹í™” ì¿¼ë¦¬ {len(result_queries)}ê°œ ìƒì„± ì™„ë£Œ")
        print(f"âœ… '{entity_name}'({entity_type})ì— ê´€í•œ íŠ¹í™” ì¿¼ë¦¬ {len(result_queries)}ê°œ ìƒì„±")
        
        return result_queries
        
    def _generate_query_expansion(self, prompt: str) -> str:
        """Ollama APIë¥¼ í†µí•´ ì¿¼ë¦¬ í™•ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            url = "http://localhost:11434/api/generate"
            headers = {"Content-Type": "application/json"}
            data = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9
                }
            }
            
            logger.info(f"Ollama API í˜¸ì¶œ: ëª¨ë¸={self.model}")
            response = requests.post(url, headers=headers, json=data)
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "")
            else:
                logger.error(f"Ollama API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                return ""
        
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ í™•ì¥ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return ""

# --- RAG ì‹œìŠ¤í…œ (ë²¡í„° ê²€ìƒ‰) ---
class RAGSystem:
    """ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    def __init__(self, embedding_type: str = "bge-m3", use_hnsw: bool = True, ef_search: int = 200, ef_construction: int = 200, m: int = 64):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        print(f"  - ì„ë² ë”© ëª¨ë¸: {embedding_type}")
        
        # ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ë³€ìˆ˜
        if hasattr(RAGSystem, '_initialized'):
            logger.info("RAG ì‹œìŠ¤í…œì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        # ì´ˆê¸°í™” ì™„ë£Œ í‘œì‹œ
        RAGSystem._initialized = True
        
        # ì„ë² ë”© ëª¨ë¸ ìœ í˜• ì €ì¥
        self.embedding_type = embedding_type
        self.embedding_name = None
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        if embedding_type == "bge-m3":
            self.embeddings = BGEM3Embeddings(model_name="BAAI/bge-m3")
            self.embedding_name = "bge-m3"
        elif LANGCHAIN_AVAILABLE and embedding_type == "bge":
            # BGE-base ì„ë² ë”© ì‚¬ìš© (HuggingFaceEmbeddings í•„ìš”)
            print("âœ“ BGE-base ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            self.embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")
            self.embedding_name = "bge"
        else:
            # ê¸°ë³¸ìœ¼ë¡œ BGE-M3 ì„ë² ë”© ì‚¬ìš© (embedding_typeì´ "bge-m3"ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ê°’ì¸ ê²½ìš°ì—ë§Œ)
            if embedding_type != "bge-m3":
                print("âœ“ BGE-M3 ì„ë² ë”© ëª¨ë¸ 'BAAI/bge-m3' ì´ˆê¸°í™” ì¤‘ (ê¸°ë³¸ê°’)")
                self.embeddings = BGEM3Embeddings(model_name="BAAI/bge-m3")
                self.embedding_name = "bge-m3"
                self.embedding_type = "bge-m3"  # íƒ€ì… ì—…ë°ì´íŠ¸
            else:
                # ì´ë¯¸ "bge-m3"ë¡œ ì´ˆê¸°í™”ëœ ê²½ìš° ê±´ë„ˆëœë‹ˆë‹¤ (ì²« ë²ˆì§¸ if ì¡°ê±´ì—ì„œ ì²˜ë¦¬ë¨)
                pass

        # ì‘ë‹µ ìºì‹œ ì´ˆê¸°í™”
        self._cache = self._load_cache()
        
        # HNSW ì¸ë±ìŠ¤ ì˜µì…˜
        self.use_hnsw = use_hnsw
        self.ef_search = ef_search
        self.ef_construction = ef_construction
        self.m = m
        
        # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        self.vector_store = None
                
        # ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.index_dir = INDEX_DIR
        os.makedirs(self.index_dir, exist_ok=True)
        
        # Ollama REST API ì„¤ì •
        self.ollama_base_url = "http://localhost:11434/api"
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡
        self.available_models = check_ollama_models()
        
        if self.available_models:
            selected_model = self.available_models[0]
            print(f"âœ“ ì‚¬ìš©í•  ëª¨ë¸: {selected_model}")
            
            # ì¿¼ë¦¬ í™•ì¥ê¸° ì´ˆê¸°í™”
            self.query_expander = QueryExpander(model_name=selected_model)
            logger.info(f"QueryExpander ëª¨ë¸ ìë™ ì„ íƒ: {selected_model}")
            print(f"âœ“ ì¿¼ë¦¬ í™•ì¥ì— {selected_model} ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            print(f"  - ì¿¼ë¦¬ í™•ì¥: í™œì„±í™”ë¨")
        else:
            self.query_expander = None
            print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ì–´ ì¿¼ë¦¬ í™•ì¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            print(f"  - ì¿¼ë¦¬ í™•ì¥: ë¹„í™œì„±í™”ë¨")
        
        print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_cache(self) -> Dict[str, str]:
        """ìºì‹œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        cache_file = os.path.join(SCRIPT_DIR, "cache.json")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {}
    
    def _save_cache(self):
        """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        cache_file = os.path.join(SCRIPT_DIR, "cache.json")
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self._cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _generate_with_ollama(self, prompt: str, model: str, stream: bool = True) -> str:
        """Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        logger.debug(f"ë‹µë³€ ìƒì„± ìš”ì²­ - ì¿¼ë¦¬: '{prompt[:30]}...', ëª¨ë¸: {model}")
        logger.info(f"ì‘ë‹µ ìƒì„± ì‹œì‘: ëª¨ë¸={model}, ì¿¼ë¦¬='{prompt[:20]}...'")
        
        # Ollama ì„œë²„ ì—°ê²° í™•ì¸
        try:
            response = requests.get("http://localhost:11434/api/tags")
            if response.status_code != 200:
                logger.error(f"Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {response.status_code}")
                return "âš ï¸ Ollama ì„œë²„ ì—°ê²° ì˜¤ë¥˜: ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        except Exception as e:
            logger.error(f"Ollama ì„œë²„ ì ‘ì† ì˜¤ë¥˜: {e}")
            return f"âš ï¸ Ollama ì„œë²„ ì ‘ì† ì˜¤ë¥˜: {e}"
        
        # API ìš”ì²­ í¬ë§·
        api_json = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9
            }
        }
        
        # ì‘ë‹µ ì €ì¥í•  ë¬¸ìì—´
        response_text = ""
        
        try:
            if stream:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                print(f"\n================================================================================")
                print(f"ğŸ“ ëª¨ë¸: {model}")
                print(f"================================================================================")
                
                with requests.post("http://localhost:11434/api/generate", json=api_json, stream=True) as response:
                    for line in response.iter_lines():
                        if line:
                            data = json.loads(line)
                            chunk = data.get("response", "")
                            response_text += chunk
                            print(chunk, end="", flush=True)
                            
                            if data.get("done", False):
                                # ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚¬ì„ ë•Œ ì‘ë‹µ ì €ì¥
                                logger.debug(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ: ê¸¸ì´={len(response_text)}")
                                print()  # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë§ˆë¬´ë¦¬
                
            else:
                # ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                print(f"\n================================================================================")
                print(f"ğŸ“ ëª¨ë¸: {model}")
                print(f"================================================================================")
                
                response = requests.post("http://localhost:11434/api/generate", json=api_json)
                
                if response.status_code == 200:
                    response_data = response.json()
                    response_text = response_data.get("response", "")
                    
                    # JSON ì¶”ì¶œ ì‹œë„ (JSONìœ¼ë¡œ ìš”ì²­í•œ ê²½ìš°)
                    try:
                        if "format" in api_json and api_json["format"] == "json":
                            # ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ë¶€ë¶„ ì¶”ì¶œ ì‹œë„
                            json_match = re.search(r'({[\s\S]*})', response_text)
                            if json_match:
                                json_str = json_match.group(1)
                                parsed_json = json.loads(json_str)
                                logger.debug(f"JSON ì‘ë‹µ íŒŒì‹± ì„±ê³µ: {parsed_json}")
                            else:
                                # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
                                parsed_json = json.loads(response_text)
                                logger.debug(f"í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ JSONìœ¼ë¡œ íŒŒì‹± ì„±ê³µ")
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}, ì‘ë‹µ: {response_text[:200]}...")
                    
                    print(response_text)
                    logger.debug(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ: ê¸¸ì´={len(response_text)}")
                    
                else:
                    error_msg = f"Ollama API ì˜¤ë¥˜: {response.status_code}, {response.text}"
                    logger.error(error_msg)
                    response_text = f"âš ï¸ {error_msg}"
                    print(response_text)
            
            return response_text
            
        except Exception as e:
            error_msg = f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            print(f"\nâŒ {error_msg}")
            return f"âš ï¸ {error_msg}"
    
    def answer(self, query: str, model: str, context: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        logger.debug(f"ë‹µë³€ ìƒì„± ìš”ì²­ - ì¿¼ë¦¬: '{query}', ëª¨ë¸: {model}")
        
        # 1. ìºì‹œì—ì„œ ì‘ë‹µ í™•ì¸
        cache_key = f"{model}:{hashlib.md5((query + context[:200]).encode()).hexdigest()}"
        cached_answer = self._load_cache().get(cache_key)
        
        if cached_answer and not os.environ.get('DISABLE_CACHE'):
            logger.info(f"ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©: ëª¨ë¸={model}, ì¿¼ë¦¬='{query[:30]}...'")
            print(f"ğŸ’¾ ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©: {model}")
            return {"answer": cached_answer, "model": model, "cached": True}
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        try:
            # ì˜¤ëŠ˜ ë‚ ì§œ ì •ë³´ ì¶”ê°€
            today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì§€ëŠ¥ì ìœ¼ë¡œ ìë¥´ê¸°
            max_context_length = 10000  # í† í° í•œë„ë¥¼ ê³ ë ¤í•œ ê¸¸ì´ ì œí•œ
            if len(context) > max_context_length:
                logger.warning(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì´ˆê³¼: {len(context)}ì -> {max_context_length}ìë¡œ ì œí•œ")
                print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(context)}ì). {max_context_length}ìë¡œ ì œí•œí•©ë‹ˆë‹¤.")
                
                # ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
                sections = context.split("\n\n")
                
                # ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì •ë ¬
                scored_sections = []
                for section in sections:
                    # ì œëª© ì„¹ì…˜ì€ í•­ìƒ ìœ ì§€
                    if section.startswith("ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ"):
                        score = float('inf')  # ìµœê³  ì ìˆ˜ë¡œ ì„¤ì •
                    elif section.startswith("[ë¬¸ì„œ #"):
                        score = float('inf') - 1  # ë¬¸ì„œ ì œëª©ë„ ë†’ì€ ì ìˆ˜
                    else:
                        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                        query_terms = set(re.findall(r'\w+', query.lower()))
                        section_text = section.lower()
                        score = sum(1 for term in query_terms if term in section_text)
                    
                    scored_sections.append((section, score))
                
                # ì ìˆ˜ë³„ ì •ë ¬ (ë†’ì€ ê²ƒë¶€í„°)
                sorted_sections = sorted(scored_sections, key=lambda x: x[1], reverse=True)
                
                # ì»¨í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
                trimmed_context = ""
                for section, _ in sorted_sections:
                    if len(trimmed_context) + len(section) + 2 <= max_context_length:
                        trimmed_context += section + "\n\n"
                    else:
                        # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ì¤‘ë‹¨
                        break
                
                context = trimmed_context.strip()
                logger.info(f"ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ ì™„ë£Œ: {len(context)}ì")
                print(f"âœ“ ì»¨í…ìŠ¤íŠ¸ë¥¼ {len(context)}ìë¡œ ì¤‘ìš”ë„ ê¸°ì¤€ ì¶•ì†Œí–ˆìŠµë‹ˆë‹¤")
            
            # í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ íŠ¹í™” QA í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            qa_template = f"""ë‹¹ì‹ ì€ {today} ê¸°ì¤€ìœ¼ë¡œ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:
1. ì œê³µëœ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì œê³µëœ ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìœ¼ë©´ "í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
2. ì¬ë¬´ ë°ì´í„°, ë‚ ì§œ, ìˆ˜ì¹˜ ë“±ì€ ì‚¬ì—…ë³´ê³ ì„œì— ê¸°ì¬ëœ ì •í™•í•œ ê°’ì„ ì‚¬ìš©í•˜ì„¸ìš”.
3. ë‹µë³€ì€ ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°–ì¶”ê³ , í•„ìš”ì‹œ í•­ëª©ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.
4. í‘œ, ê·¸ë˜í”„, ìˆ«ì ë°ì´í„°ëŠ” í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ ì œì‹œí•˜ì„¸ìš”.
5. ë³´í—˜ì—… ì „ë¬¸ ìš©ì–´ê°€ ì‚¬ìš©ëœ ê²½ìš° ê°„ëµí•œ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”.
6. í•œí™”ì†í•´ë³´í—˜ì˜ ê²½ì˜ ì „ëµ, ì¬ë¬´ ìƒíƒœ, ë¦¬ìŠ¤í¬ ê´€ë¦¬, ê¸°ì—… ì§€ë°°êµ¬ì¡° ê´€ë ¨ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.
7. ë³µì¡í•œ ê°œë…ì€ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ë˜, í•µì‹¬ ì •ë³´ëŠ” ë¹ ëœ¨ë¦¬ì§€ ë§ˆì„¸ìš”.
8. ì£¼ê´€ì  ì˜ê²¬ì´ë‚˜ ì¶”ì¸¡ì€ ë°°ì œí•˜ê³ , ì‚¬ì—…ë³´ê³ ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ë‹µë³€í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©:
{context}

ìœ„ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ë‹µë³€:"""
            
            # 3. Ollama ë˜ëŠ” LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
            try:
                logger.info(f"ì‘ë‹µ ìƒì„± ì‹œì‘: ëª¨ë¸={model}, ì¿¼ë¦¬='{query[:30]}...'")
                start_time = time.time()
                
                # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
                result = self._generate_with_ollama(qa_template, model, stream=True)
                
                # ì†Œìš” ì‹œê°„ ì¸¡ì •
                elapsed_time = time.time() - start_time
                logger.info(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ: ì†Œìš”ì‹œê°„={elapsed_time:.2f}ì´ˆ")
                
                # ì‘ë‹µ ìºì‹±
                answer_content = result if isinstance(result, str) else result.get('answer', '')
                self._cache[cache_key] = answer_content
                self._save_cache()
                
                # ê²°ê³¼ ë°˜í™˜
                return {
                    "answer": answer_content,
                    "model": model,
                    "cached": False,
                    "elapsed_time": elapsed_time
                }
                
            except Exception as e:
                logger.error(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                return {"answer": f"ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "model": model, "error": True}
                
        except Exception as e:
            logger.error(f"âŒ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {"answer": f"ë‹µë³€ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "model": model, "error": True}

    def search(self, query: str, top_k: int = 5) -> List[Document]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"ê²€ìƒ‰ ìš”ì²­: '{query}', top_k={top_k}")
            print(f"\nğŸ” ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            if not hasattr(self, "vector_store") or self.vector_store is None:
                # FAISS ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”
                index_dir = os.path.join(self.index_dir, f"faiss_index_{self.embedding_type}")
                metadata_path = os.path.join(self.index_dir, f"document_metadata_{self.embedding_type}.json")
                
                logger.info(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„: {index_dir}")
                print(f"ğŸ“‚ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘: {index_dir}")
                
                try:
                    # HNSW ì¸ë±ìŠ¤ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¡œë“œ
                    if self.use_hnsw:
                        # HNSW ì¸ë±ìŠ¤ ë¡œë“œ
                        self.vector_store = FAISS.load_local(
                            folder_path=index_dir,
                            embeddings=self.embeddings,
                            allow_dangerous_deserialization=True,
                            index_name="index"
                        )
                        logger.info("HNSW ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ")
                        print("âœ… HNSW ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
                    else:
                        # L2 ì¸ë±ìŠ¤ ë¡œë“œ
                        self.vector_store = FAISS.load_local(
                            folder_path=index_dir,
                            embeddings=self.embeddings,
                            allow_dangerous_deserialization=True,
                            index_name="index"
                        )
                        logger.info("L2 ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ")
                        print("âœ… L2 ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
                    
                    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                    if os.path.exists(metadata_path):
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            self.document_metadata = json.load(f)
                            logger.info(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(self.document_metadata)}ê°œ ë¬¸ì„œ")
                except Exception as e:
                    logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    logger.error(traceback.format_exc())
                    print(f"âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    raise ValueError(f"ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            
            # ê²€ìƒ‰ ê°œì„ : í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê³¼ í‘œ ë°ì´í„° ê°€ì¤‘ì¹˜ ì ìš©
            try:
                # ë©€í‹°ì¿¼ë¦¬ ìƒì„± (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
                expander = QueryExpander()
                queries = expander.expand_query(query, max_queries=5)  # ìµœëŒ€ 5ê°œ ì¿¼ë¦¬ ìƒì„±
                
                logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰ ì‹¤í–‰: {len(queries)}ê°œ ì¿¼ë¦¬")
                print(f"ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰ ({len(queries)}ê°œ ì¿¼ë¦¬ ì‚¬ìš©)")
                
                # ê° ì¿¼ë¦¬ë³„ ê²°ê³¼ ì €ì¥
                all_results = {}
                query_results = {}
                
                for idx, q in enumerate(queries):
                    logger.info(f"ì¿¼ë¦¬ {idx+1} ê²€ìƒ‰: '{q}'")
                    print(f"  - ì¿¼ë¦¬ {idx+1}: '{q}'")
                    
                    # íƒ€ì´í‹€ê³¼ ë‚´ìš© ê°€ì¤‘ì¹˜ ë†’ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                    vector_results = self.vector_store.similarity_search_with_score(
                        q, 
                        k=top_k * 2  # ì¶©ë¶„í•œ í›„ë³´ ê²°ê³¼ ê²€ìƒ‰
                    )
                    
                    if vector_results:
                        # ê²°ê³¼ ì²˜ë¦¬ ë° ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                        processed_results = []
                        for doc, score in vector_results:
                            # ë¬¸ì„œ ID ìƒì„± (ì¤‘ë³µ í™•ì¸ìš©)
                            doc_id = self._get_doc_id(doc)
                            
                            # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (FAISSëŠ” ê±°ë¦¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ)
                            # ì •ê·œí™”ëœ ê´€ë ¨ì„± ì ìˆ˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ)
                            relevance = 1.0 / (1.0 + score)
                            
                            # í‘œ ë°ì´í„° ê°€ì¤‘ì¹˜ ì ìš© (1.5ë°°)
                            is_table = False
                            # í‘œ ë°ì´í„°ì¸ì§€ í™•ì¸: content_type=table ë˜ëŠ” ë©”íƒ€ë°ì´í„°ì—ì„œ í™•ì¸
                            if "content_type" in doc.metadata and doc.metadata["content_type"] == "table":
                                is_table = True
                            # í‘œ í‚¤ì›Œë“œ í™•ì¸ (page_contentì— í‘œ, í…Œì´ë¸” ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€)
                            elif "table" in doc.page_content.lower() or "í‘œ " in doc.page_content:
                                is_table = True
                                
                            if is_table:
                                relevance *= 1.5  # í‘œ ë°ì´í„°ëŠ” 1.5ë°° ê°€ì¤‘ì¹˜
                                logger.info(f"í‘œ ë°ì´í„° ê°€ì¤‘ì¹˜ ì ìš©: {doc_id}, ì›ë˜ì ìˆ˜={1.0/(1.0+score):.4f}, ê°€ì¤‘ì¹˜ì ìš©={relevance:.4f}")
                            
                            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìš”ì†Œ)
                            keyword_score = self._calculate_keyword_score(q, doc.page_content)
                            
                            # ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°: ë²¡í„° ì ìˆ˜ 70% + í‚¤ì›Œë“œ ì ìˆ˜ 30%
                            hybrid_score = (relevance * 0.7) + (keyword_score * 0.3)
                            
                            # ì¿¼ë¦¬ë³„ ë¡œê¹…
                            if doc_id not in all_results:
                                all_results[doc_id] = {
                                    "doc": doc,
                                    "scores": {},
                                    "queries": [],
                                    "is_table": is_table
                                }
                            
                            # ì¿¼ë¦¬ë³„ ì ìˆ˜ ì €ì¥
                            all_results[doc_id]["scores"][q] = hybrid_score
                            all_results[doc_id]["queries"].append(q)
                            processed_results.append((doc, doc_id, hybrid_score))
                        
                        query_results[q] = processed_results
                        logger.info(f"ì¿¼ë¦¬ '{q}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼: {len(processed_results)}ê°œ ë¬¸ì„œ")
                
                # ê²°ê³¼ ë³‘í•© ë° ì •ë ¬ ë¡œì§ ê°œì„ 
                # 1. ì—¬ëŸ¬ ì¿¼ë¦¬ì—ì„œ ë°œê²¬ëœ ë¬¸ì„œì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
                # 2. ê°€ì¥ ë†’ì€ ì ìˆ˜ ì‚¬ìš©
                # 3. ìœ ì‚¬í•œ ë¬¸ì„œ ì¤‘ë³µ ì œê±° (MMRê³¼ ìœ ì‚¬í•œ íš¨ê³¼)
                
                # ìµœì¢… ì ìˆ˜ ê³„ì‚°
                final_scores = {}
                for doc_id, data in all_results.items():
                    # ê° ë¬¸ì„œê°€ ë§¤ì¹­ëœ ì¿¼ë¦¬ ìˆ˜ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
                    query_match_weight = min(len(data["queries"]) / len(queries), 1.0)
                    
                    # ì¿¼ë¦¬ë³„ ìµœê³  ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                    max_score = max(data["scores"].values())
                    
                    # ìµœì¢… ì ìˆ˜ = ìµœê³ ì ìˆ˜ * (1 + ì¿¼ë¦¬ë§¤ì¹­ê°€ì¤‘ì¹˜)
                    # ì´ë ‡ê²Œ í•˜ë©´ ì—¬ëŸ¬ ì¿¼ë¦¬ì— ë§¤ì¹­ëœ ë¬¸ì„œê°€ ë” ë†’ì€ ì ìˆ˜ë¥¼ ë°›ìŒ
                    final_score = max_score * (1 + query_match_weight)
                    
                    # í‘œ ë°ì´í„° ì—¬ë¶€ í‘œì‹œ (ì´ë¯¸ ê°œë³„ ì ìˆ˜ì— ê°€ì¤‘ì¹˜ ì ìš©ë¨)
                    if data.get("is_table", False):
                        logger.info(f"í‘œ ë¬¸ì„œ ìµœì¢… ì ìˆ˜: {doc_id}, ì ìˆ˜={final_score:.4f}")
                        
                    final_scores[doc_id] = final_score
                
                # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
                sorted_results = sorted(
                    [(doc_id, all_results[doc_id]["doc"], score) for doc_id, score in final_scores.items()],
                    key=lambda x: x[2],  # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
                    reverse=True  # ë†’ì€ ì ìˆ˜ë¶€í„°
                )
                
                # top_kê°œ ê²°ê³¼ ì„ íƒ
                final_docs = []
                for doc_id, doc, score in sorted_results[:top_k]:
                    # ê´€ë ¨ ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
                    doc.metadata["relevance_score"] = f"{score:.4f}"
                    doc.metadata["matched_queries"] = ", ".join(all_results[doc_id]["queries"])
                    
                    # í‘œ ë°ì´í„° ì—¬ë¶€ í‘œì‹œ
                    if all_results[doc_id].get("is_table", False):
                        doc.metadata["is_table"] = "true"
                        
                    final_docs.append(doc)
                
                # ê²°ê³¼ ë¡œê¹…
                logger.info(f"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(final_docs)}ê°œ ë¬¸ì„œ")
                print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(final_docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
                
                # í‘œ ë°ì´í„° ê°œìˆ˜ ì¶œë ¥
                table_count = sum(1 for doc in final_docs if doc.metadata.get("is_table") == "true")
                if table_count > 0:
                    print(f"   - í‘œ ë°ì´í„°: {table_count}ê°œ (1.5ë°° ê°€ì¤‘ì¹˜ ì ìš©)")
                
                return final_docs
                
            except Exception as e:
                logger.error(f"ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                logger.error(traceback.format_exc())
                print(f"âš ï¸ ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨, ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ëŒ€ì²´: {e}")
                
                # í™•ì¥ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                results = self.vector_store.similarity_search_with_score(query, k=top_k)
                
                if results:
                    logger.info(f"ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì„œ")
                    
                    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ê²°ê³¼ ë°˜í™˜
                    final_results = []
                    for doc, score in results:
                        # ì ìˆ˜ ì •ê·œí™” (FAISSëŠ” ê±°ë¦¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ)
                        relevance = 1.0 / (1.0 + score)
                        
                        # í‘œ ë°ì´í„° ê°€ì¤‘ì¹˜ ì ìš©
                        is_table = False
                        if "content_type" in doc.metadata and doc.metadata["content_type"] == "table":
                            is_table = True
                        elif "table" in doc.page_content.lower() or "í‘œ " in doc.page_content:
                            is_table = True
                            
                        if is_table:
                            relevance *= 1.5  # í‘œ ë°ì´í„° ê°€ì¤‘ì¹˜
                            doc.metadata["is_table"] = "true"
                            
                        doc.metadata["relevance_score"] = f"{relevance:.4f}"
                        final_results.append(doc)
                    
                    # í‘œ ë°ì´í„° ê°œìˆ˜ ì¶œë ¥
                    table_count = sum(1 for doc in final_results if doc.metadata.get("is_table") == "true")
                    if table_count > 0:
                        print(f"   - í‘œ ë°ì´í„°: {table_count}ê°œ (1.5ë°° ê°€ì¤‘ì¹˜ ì ìš©)")
                        
                    print(f"âœ… ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ë¬¸ì„œ ë°œê²¬")
                    return final_results
                else:
                    logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: '{query}'")
                    print("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    return []
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
            
    def _get_doc_id(self, doc: Document) -> str:
        """ë¬¸ì„œì˜ ê³ ìœ  IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
        source = doc.metadata.get("source", "")
        page = doc.metadata.get("page", "")
        chunk_id = doc.metadata.get("chunk_id", "")
        
        # ê³ ìœ  ID ìƒì„±
        doc_id = f"{source}:{page}:{chunk_id}"
        return doc_id
    
    def load_or_create_vector_store(self, documents: List[Document], force_update: bool = False) -> bool:
        """ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ì„ë² ë”© ëª¨ë¸ì— ë”°ë¼ ì¸ë±ìŠ¤ í´ë” ê²°ì •
            if self.embedding_type == "bge-m3":
                index_folder = os.path.join(INDEX_DIR, "faiss_index_bge-m3")
                metadata_file = os.path.join(INDEX_DIR, "document_metadata_bge-m3.json")
            else:
                index_folder = os.path.join(INDEX_DIR, "faiss_index_bge")
                metadata_file = os.path.join(INDEX_DIR, "document_metadata_bge.json")
            
            # ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸ ì—¬ë¶€
            if force_update:
                logger.info("ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸ ëª¨ë“œ í™œì„±í™”")
                print("ğŸ”„ ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸ ëª¨ë“œ í™œì„±í™”ë¨")
                return self._create_new_vector_store(documents, index_folder, metadata_file)
            
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            if os.path.exists(os.path.join(index_folder, "index.faiss")) and os.path.exists(os.path.join(index_folder, "index.pkl")):
                try:
                    # HNSW ì¸ë±ìŠ¤ ì‚¬ìš© ì˜µì…˜
                    if self.use_hnsw:
                        print(f"âœ“ HNSW ì¸ë±ìŠ¤ ì‚¬ìš© (ef_search={self.ef_search})")
                        logger.info(f"HNSW ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„: ef_search={self.ef_search}")
                        
                        self.vector_store = FAISS.load_local(
                            index_folder,
                            self.embeddings,
                            normalize_L2=True,
                            allow_dangerous_deserialization=True  # ì•ˆì „í•˜ì§€ ì•Šì€ ì—­ì§ë ¬í™” í—ˆìš©
                        )
                        
                        # HNSW ì¸ë±ìŠ¤ ì„¤ì • ì¡°ì •
                        if hasattr(self.vector_store, 'index'):
                            # HNSW ì¸ë±ìŠ¤ íƒ€ì…ì¸ì§€ í™•ì¸
                            if hasattr(self.vector_store.index, 'hnsw'):
                                self.vector_store.index.hnsw.efSearch = self.ef_search
                                logger.info(f"HNSW ì¸ë±ìŠ¤ ë§¤ê°œë³€ìˆ˜ ì„¤ì • ì™„ë£Œ: ef_search={self.ef_search}")
                            else:
                                logger.info("L2 ì¸ë±ìŠ¤ê°€ ë¡œë“œë¨ (HNSW ì¸ë±ìŠ¤ ì•„ë‹˜)")
                                print(f"âœ“ L2 ì¸ë±ìŠ¤ê°€ ë¡œë“œë¨ (HNSW ì¸ë±ìŠ¤ ì•„ë‹˜)")
                    else:
                        logger.info("ê¸°ë³¸ L2 ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„")
                        print(f"âœ“ ê¸°ë³¸ L2 ì¸ë±ìŠ¤ ì‚¬ìš©")
                        
                        self.vector_store = FAISS.load_local(
                            index_folder,
                            self.embeddings,
                            normalize_L2=True,
                            allow_dangerous_deserialization=True  # ì•ˆì „í•˜ì§€ ì•Šì€ ì—­ì§ë ¬í™” í—ˆìš©
                        )
                        
                    # ì„ë² ë”© ëª¨ë¸ ì´ë¦„ ì €ì¥
                    self.embedding_model_name = self.embedding_name
                    
                    logger.info(f"ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ: {index_folder}")
                    print(f"âœ“ ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ: {index_folder}")
                    return True
                    
                except Exception as e:
                    logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    logger.error(traceback.format_exc())
                    print(f"âŒ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨ - ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ì‹œë„: {e}")
                    return self._create_new_vector_store(documents, index_folder, metadata_file)
            else:
                # ì¸ë±ìŠ¤ê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±
                logger.info(f"ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {index_folder}")
                print(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                return self._create_new_vector_store(documents, index_folder, metadata_file)
                
        except Exception as e:
            logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(traceback.format_exc())
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    def _create_new_vector_store(self, documents: List[Document], index_folder: str, metadata_file: str) -> bool:
        """ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        try:
            # ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
            os.makedirs(os.path.dirname(index_folder), exist_ok=True)
            os.makedirs(index_folder, exist_ok=True)
            
            # ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if not documents or len(documents) == 0:
                logger.error("ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                print("âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
                
            # ë¬¸ì„œ ìœ íš¨ì„± ê²€ì¦
            valid_documents = []
            for doc in documents:
                if doc.page_content and len(doc.page_content.strip()) > 0:
                    valid_documents.append(doc)
                else:
                    logger.warning(f"ë¹ˆ ë¬¸ì„œ ë¬´ì‹œ: {doc.metadata}")
            
            # ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if not valid_documents:
                logger.error("ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                print("âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
                
            logger.info(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹œì‘: {len(valid_documents)}ê°œ ë¬¸ì„œ, HNSW ì‚¬ìš©: {self.use_hnsw}")
            print(f"ğŸ”„ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘: {len(valid_documents)}ê°œ ë¬¸ì„œ")
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥
            document_metadata = {}
            for i, doc in enumerate(valid_documents):
                doc_id = f"doc_{i}"
                doc.metadata["doc_id"] = doc_id
                document_metadata[doc_id] = doc.metadata
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            os.makedirs(os.path.dirname(metadata_file), exist_ok=True)
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(document_metadata, f, ensure_ascii=False, indent=2)
            
            # HNSW ì¸ë±ìŠ¤ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
            if self.use_hnsw:
                # HNSW íŒŒë¼ë¯¸í„° ë¡œê¹…
                logger.info(f"HNSW ì¸ë±ìŠ¤ ìƒì„±: ef_construction={self.ef_construction}, M={self.m}")
                print(f"ğŸ’¡ HNSW ì¸ë±ìŠ¤ ìƒì„± ì¤‘: ef_construction={self.ef_construction}, M={self.m}")
                
                # HNSW ì¸ë±ìŠ¤ ìƒì„± (í–¥ìƒëœ ì„±ëŠ¥ì„ ìœ„í•´)
                self.vector_store = FAISS.from_documents(
                    valid_documents, 
                    self.embeddings,
                    normalize_L2=True
                )
                
                # HNSW íŒŒë¼ë¯¸í„° ì„¤ì •
                if hasattr(self.vector_store, 'index'):
                    # ê¸°ë³¸ ì¸ë±ìŠ¤ë¥¼ HNSWë¡œ ë³€í™˜
                    dimension = self.vector_store.index.d
                    logger.info(f"FAISS ì¸ë±ìŠ¤ ì°¨ì›: {dimension}")
                    
                    # ê¸°ì¡´ ë²¡í„° ì €ì¥
                    vectors = []
                    for i in range(self.vector_store.index.ntotal):
                        vector = self.vector_store.index.reconstruct(i)
                        vectors.append(vector)
                    
                    # HNSW ì¸ë±ìŠ¤ ìƒì„±
                    hnsw_index = faiss.IndexHNSWFlat(dimension, self.m)
                    hnsw_index.hnsw.efConstruction = self.ef_construction
                    hnsw_index.hnsw.efSearch = self.ef_search
                    
                    # ë²¡í„° ì¶”ê°€
                    if vectors:
                        hnsw_index.add(np.array(vectors))
                    
                    # ì¸ë±ìŠ¤ êµì²´
                    self.vector_store.index = hnsw_index
                    logger.info(f"HNSW ì¸ë±ìŠ¤ë¡œ ì„±ê³µì ìœ¼ë¡œ ë³€í™˜: {self.vector_store.index.ntotal}ê°œ ë²¡í„°")
                    print(f"âœ… HNSW ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {self.vector_store.index.ntotal}ê°œ ë²¡í„°")
                
                # ì¸ë±ìŠ¤ ì €ì¥
                self.vector_store.save_local(index_folder, index_name="index")
                logger.info(f"HNSW ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_folder}")
                print(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_folder}")
            else:
                # ê¸°ë³¸ L2 ì¸ë±ìŠ¤ ìƒì„±
                logger.info("ê¸°ë³¸ L2 ì¸ë±ìŠ¤ ìƒì„± ì¤‘")
                print("ğŸ’¡ ê¸°ë³¸ L2 ì¸ë±ìŠ¤ ìƒì„± ì¤‘")
                
                self.vector_store = FAISS.from_documents(
                    valid_documents, 
                    self.embeddings,
                    normalize_L2=True
                )
                
                # ì¸ë±ìŠ¤ ì €ì¥
                self.vector_store.save_local(index_folder, index_name="index")
                logger.info(f"L2 ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_folder}")
                print(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_folder}")
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì„¤ì •
            self.document_metadata = document_metadata
            
            logger.info(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {len(valid_documents)}ê°œ ë¬¸ì„œ")
            print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {len(valid_documents)}ê°œ ë¬¸ì„œ")
            return True
        
        except Exception as e:
            logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    def format_context_for_model(self, documents: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ëª¨ë¸ ì…ë ¥ìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not documents:
            return "í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì†ŒìŠ¤ë³„ ë¬¸ì„œ ê·¸ë£¹í™” ë° ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        grouped_docs = {}
        for doc in documents:
            source = doc.metadata.get("source", "")
            page = doc.metadata.get("page", "")
            key = f"{source}:{page}"
            
            # ê´€ë ¨ì„± ì ìˆ˜ ì¶”ì¶œ (ë¬¸ìì—´ì—ì„œ ìˆ«ìë¡œ ë³€í™˜)
            relevance_score = float(doc.metadata.get("relevance_score", "0.0"))
            
            if key not in grouped_docs:
                grouped_docs[key] = {
                    "docs": [],
                    "scores": [],
                    "queries": set()
                }
            
            grouped_docs[key]["docs"].append(doc)
            grouped_docs[key]["scores"].append(relevance_score)
            
            # ë§¤ì¹­ëœ ì¿¼ë¦¬ ì •ë³´ ìˆ˜ì§‘
            if "matched_queries" in doc.metadata:
                for q in doc.metadata["matched_queries"].split(", "):
                    grouped_docs[key]["queries"].add(q)
        
        # ê° ê·¸ë£¹ì˜ í‰ê·  ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        for key in grouped_docs:
            scores = grouped_docs[key]["scores"]
            grouped_docs[key]["avg_score"] = sum(scores) / len(scores) if scores else 0.0
        
        # ê·¸ë£¹ì„ í‰ê·  ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ë¶€í„°)
        sorted_groups = sorted(grouped_docs.items(), key=lambda x: x[1]["avg_score"], reverse=True)
        
        # ë¬¸ì„œ ìš”ì•½ ì •ë³´ ë¨¼ì € í‘œì‹œ
        doc_summary = "ğŸ“‘ ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½:\n"
        for i, (key, group) in enumerate(sorted_groups):
            source, page = key.split(":")
            source_name = os.path.basename(source) if source else "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ"
            page_info = f"í˜ì´ì§€: {page}" if page else "í˜ì´ì§€ ì •ë³´ ì—†ìŒ"
            avg_score = group["avg_score"]
            
            # ê´€ë ¨ ì¿¼ë¦¬ ì •ë³´
            query_info = f", ê´€ë ¨ ì¿¼ë¦¬: {', '.join(list(group['queries'])[:2])}" if group["queries"] else ""
            
            doc_summary += f"{i+1}. {source_name} ({page_info}, ê´€ë ¨ì„±: {avg_score:.2f}{query_info})\n"
        
        # ê·¸ë£¹ë³„ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        formatted_docs = []
        
        for i, (key, group) in enumerate(sorted_groups):
            source, page = key.split(":")
            source_name = os.path.basename(source) if source else "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ"
            
            # ë©”íƒ€ë°ì´í„° ì •ë³´
            meta_info = [
                f"ì¶œì²˜: {os.path.basename(source)}" if source else "ì¶œì²˜: ì•Œ ìˆ˜ ì—†ìŒ",
                f"í˜ì´ì§€: {page}" if page else "í˜ì´ì§€: ì •ë³´ ì—†ìŒ",
                f"ê´€ë ¨ì„±: {group['avg_score']:.2f}"
            ]
            
            # ê´€ë ¨ ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
            if group["queries"]:
                meta_info.append(f"ê´€ë ¨ ì¿¼ë¦¬: {', '.join(list(group['queries'])[:3])}")
            
            # ê·¸ë£¹ ë‚´ ë¬¸ì„œ ë‚´ìš© ë³‘í•© (ì¤‘ë³µ ì œê±°)
            # ì¢€ ë” ìŠ¤ë§ˆíŠ¸í•œ í…ìŠ¤íŠ¸ ë³‘í•©: ì²­í¬ ê°„ ì¤‘ë³µë˜ëŠ” ë¶€ë¶„ ì œê±°
            content_parts = []
            
            # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ë¬¸ì„œ ì •ë ¬
            sorted_docs = sorted(
                zip(group["docs"], group["scores"]), 
                key=lambda x: x[1], 
                reverse=True
            )
            
            for doc, _ in sorted_docs:
                content = doc.page_content.strip()
                if content:
                    # ì´ë¯¸ ì¶”ê°€ëœ ë‚´ìš©ê³¼ ì¤‘ë³µ ê²€ì‚¬
                    if not any(self._has_significant_overlap(content, existing) for existing in content_parts):
                        content_parts.append(content)
            
            # ìµœì¢… ë‚´ìš© ë³‘í•©
            content = "\n\n".join(content_parts)
            
            # ìµœì¢… í¬ë§·íŒ…ëœ ë¬¸ì„œ ìƒì„±
            meta_str = " | ".join(meta_info)
            formatted_docs.append(f"[ë¬¸ì„œ #{i+1}] {meta_str}\n{content}")
        
        # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = "ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n"
        context += doc_summary + "\n\n" # ë¬¸ì„œ ìš”ì•½ ì •ë³´ ì¶”ê°€
        context += "\n\n---\n\n".join(formatted_docs)
        
        # ë¡œê·¸ ë° ì½˜ì†”ì— ì¶œë ¥
        logger.info(f"í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {len(formatted_docs)}ê°œ ë¬¸ì„œ, {len(context)}ì")
        print(f"\nğŸ“„ ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ: {len(formatted_docs)}ê°œ")
        
        for i, (key, group) in enumerate(sorted_groups):
            source, page = key.split(":")
            source_name = os.path.basename(source) if source else "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ"
            page_info = f", í˜ì´ì§€: {page}" if page else ""
            avg_score = group["avg_score"]
            
            # ê´€ë ¨ ì¿¼ë¦¬ ì •ë³´
            query_list = list(group["queries"])
            query_info = f", ì¿¼ë¦¬: {', '.join(query_list[:2])}" if query_list else ""
            
            print(f"  - ë¬¸ì„œ #{i+1}: {source_name}{page_info} (ê´€ë ¨ì„±: {avg_score:.2f}{query_info})")
        
        return context
    
    def _has_significant_overlap(self, text1: str, text2: str, threshold: float = 0.7) -> bool:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì— ìƒë‹¹í•œ ì¤‘ë³µì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        # ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì¤‘ë³µ ê²€ì‚¬ì—ì„œ ì œì™¸
        if len(text1) < 50 or len(text2) < 50:
            return False
        
        # ë” ì§§ì€ í…ìŠ¤íŠ¸ ê¸¸ì´ì˜ 70%ê°€ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        shorter = text1 if len(text1) < len(text2) else text2
        longer = text2 if len(text1) < len(text2) else text1
        
        # ë” ì§§ì€ í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ì¤‘ ì–¼ë§ˆë‚˜ ë§ì€ ë¹„ìœ¨ì´ ë” ê¸´ í…ìŠ¤íŠ¸ì— ë‚˜íƒ€ë‚˜ëŠ”ì§€ ê³„ì‚°
        shorter_words = set(shorter.split())
        longer_words = set(longer.split())
        
        if not shorter_words:
            return False
            
        # ê³µí†µ ë‹¨ì–´ ë¹„ìœ¨ ê³„ì‚°
        common_words = shorter_words.intersection(longer_words)
        overlap_ratio = len(common_words) / len(shorter_words)
        
        return overlap_ratio > threshold
        
    def _calculate_keyword_score(self, query: str, text: str) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš©)"""
        # ì¿¼ë¦¬ ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        query_clean = re.sub(r'[^\w\s]', ' ', query.lower())
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±°)
        stopwords = {"ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ì˜", "ê³¼", "ì™€", "ë¡œ", "ìœ¼ë¡œ", 
                    "ì´ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "í•˜ë‹¤", "ë˜ë‹¤", "í•œë‹¤", "ëœë‹¤", "ë¬´ì—‡", "ëˆ„êµ¬", "ì–´ë””", 
                    "ì–¸ì œ", "ì™œ", "ì–´ë–»ê²Œ", "ì–´ë–¤", "ì–¼ë§ˆë‚˜", "ëª‡", "ë¬´ìŠ¨"}
        
        query_words = [w for w in query_clean.split() if w not in stopwords and len(w) > 1]
        
        if not query_words:
            return 0.0
        
        # ë‹¨ì–´ ë‹¨ìœ„ ë§¤ì¹­
        matched_words = sum(1 for word in query_words if word in text_clean)
        if matched_words == 0:
            return 0.0
            
        # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨
        keyword_score = matched_words / len(query_words)
        
        # TF-IDFì™€ ìœ ì‚¬í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬: ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ í…ìŠ¤íŠ¸ì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨
        text_words = text_clean.split()
        if text_words:
            # ë‹¨ì–´ ë¹ˆë„ë¥¼ ê³ ë ¤í•œ ë°€ë„ ìŠ¤ì½”ì–´
            word_density = matched_words / len(text_words)
            # ë¬¸ì„œê°€ ë§¤ìš° ê¸¸ë©´ ë°€ë„ê°€ ë‚®ì•„ì§€ë¯€ë¡œ, ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ì¡°ì •
            adjusted_density = min(1.0, word_density * 20)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
            
            # ìµœì¢… í‚¤ì›Œë“œ ì ìˆ˜: ë§¤ì¹­ ë¹„ìœ¨ 70% + ë‹¨ì–´ ë°€ë„ 30%
            keyword_score = 0.7 * keyword_score + 0.3 * adjusted_density
        
        return keyword_score

def main():
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description='í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ RAG ì‹œìŠ¤í…œ')
    parser.add_argument('--pdf', type=str, default=PDF_PATH, help='PDF íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--chunk-size', type=int, default=500, help='ì²­í¬ í¬ê¸°')
    parser.add_argument('--chunk-overlap', type=int, default=150, help='ì²­í¬ ê²¹ì¹¨')
    parser.add_argument('--top-k', type=int, default=10, help='ê²€ìƒ‰ ê²°ê³¼ ìˆ˜')
    parser.add_argument('--force-update', action='store_true', help='ë²¡í„° ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸')
    parser.add_argument('--flat-index', action='store_true', help='L2 ì¸ë±ìŠ¤ ì‚¬ìš© (HNSW ë¹„í™œì„±í™”)')
    parser.add_argument('--auto-eval', action='store_true', help='ìë™ í‰ê°€ í™œì„±í™”')
    parser.add_argument('--auto-test', action='store_true', help='ìë™ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”')
    parser.add_argument('--num-questions', type=int, default=5, help='ìë™ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìˆ˜')
    args = parser.parse_args()
    
    # ë¡œê·¸ ì„¤ì •
    global logger
    logger = setup_logging()
    
    # Ollama ì„œë²„ ì—°ê²° ë° ëª¨ë¸ í™•ì¸
    print("ğŸ”„ Ollama ëª¨ë¸ í™•ì¸ ì¤‘...")
    available_models = check_ollama_models()
    
    # ìë™ í…ŒìŠ¤íŠ¸ ëª¨ë“œì¸ ê²½ìš° ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    if args.auto_test:
        print(f"\n{'='*80}")
        print(f"ğŸ“ ìë™ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™” (ì§ˆë¬¸ ìˆ˜: {args.num_questions})")
        print(f"{'='*80}")
        
        try:
            # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            rag = RAGSystem(
                use_hnsw=not args.flat_index,
                ef_search=200,
                ef_construction=200,
                m=64
            )
            
            # PDF ì²˜ë¦¬
            processor = PDFProcessor(args.pdf)
            documents = processor.process()
            
            # ë¬¸ì„œ ë¶„í• 
            splitter = DocumentSplitter(chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap)
            chunks = splitter.split_documents(documents)
            
            # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            rag.load_or_create_vector_store(chunks, force_update=args.force_update)
            
            # ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            evaluator = ModelEvaluator()
            auto_evaluator = AutoEvaluator(model_name="gemma3:12b")
            auto_question_generator = AutoQuestionGenerator(model_name="gemma3:12b")
            
            auto_test_manager = AutoTestManager(
                rag_system=rag,
                auto_question_generator=auto_question_generator,
                auto_evaluator=auto_evaluator,
                evaluator=evaluator,
                available_models=AVAILABLE_MODELS
            )
            
            auto_test_manager.run_auto_test(num_questions=args.num_questions, top_k=args.top_k, stream=True)
            
            return 0
            
        except Exception as e:
            logger.error(f"ìë™ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"\nâŒ ìë™ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            return 1
    
    # ì¼ë°˜ ëª¨ë“œ ì‹¤í–‰
    try:
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        rag = RAGSystem(
            use_hnsw=not args.flat_index,
            ef_search=200,
            ef_construction=200,
            m=64
        )
        
        # PDF ì²˜ë¦¬
        processor = PDFProcessor(args.pdf)
        documents = processor.process()
        
        # ë¬¸ì„œ ë¶„í• 
        splitter = DocumentSplitter(chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap)
        chunks = splitter.split_documents(documents)
        
        # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        rag.load_or_create_vector_store(chunks, force_update=args.force_update)
        
        # ì§ˆë¬¸ ì…ë ¥ ë°›ê¸° (í‘œì¤€ ì…ë ¥ì—ì„œ ì½ê¸°)
        print("\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (Ctrl+D ë˜ëŠ” ë¹ˆ ì¤„ë¡œ ì¢…ë£Œ):")
        lines = []
        try:
            for line in sys.stdin:
                if not line.strip():
                    break
                lines.append(line)
            question = ''.join(lines).strip()
        except EOFError:
            question = ''.join(lines).strip()
        
        if not question:
            print("âŒ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 1
        
        # ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = rag.search(question, top_k=args.top_k)
        context = rag.format_context_for_model(retrieved_docs)
        
        # ê²°ê³¼ ë³€ìˆ˜
        results = {}
        auto_evaluations = {}
        evaluation_id = None
        
        # í‰ê°€ ì„¤ì •
        evaluator = ModelEvaluator()
        
        # ìë™ í‰ê°€ í™œì„±í™”ëœ ê²½ìš°
        if args.auto_eval:
            auto_evaluator = AutoEvaluator(model_name="gemma3:12b")
            evaluation_id = evaluator.save_evaluation(question, context, {}, {
                "top_k": args.top_k,
                "has_auto_eval": True
            })
        
        # ê° ëª¨ë¸ì— ëŒ€í•´ ì‘ë‹µ ìƒì„±
        for model in AVAILABLE_MODELS:
            print(f"\n{'='*80}")
            print(f"ğŸ“ ëª¨ë¸: {model}")
            print(f"{'='*80}")
            
            # ëª¨ë¸ ì‘ë‹µ ìƒì„±
            result = rag.answer(question, model, context)
            answer = result["answer"]
            results[model] = answer
            
            # ìë™ í‰ê°€ ì‹¤í–‰
            if args.auto_eval:
                print(f"\nğŸ¤– ìë™ í‰ê°€ ì¤‘...")
                auto_evaluation = auto_evaluator.evaluate_answer(question, context, answer)
                auto_evaluations[model] = auto_evaluation
                
                # í‰ê°€ ê²°ê³¼ ì €ì¥
                if evaluation_id:
                    score = auto_evaluation.get("score", 0)
                    comments = auto_evaluation.get("reason", "")
                    evaluator.add_evaluation_score(evaluation_id, model, score, comments)
                
                print(f"í‰ê°€ ì ìˆ˜: {auto_evaluation.get('score', 'í‰ê°€ ì‹¤íŒ¨')}/5")
                print(f"í‰ê°€ ì´ìœ : {auto_evaluation.get('reason', 'í‰ê°€ ì‹¤íŒ¨')}")
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥ (Streamlit ì•±ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•¨)
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        query_id = re.sub(r'\W+', '_', question)[:30]
        query_results_dir = os.path.join(SCRIPT_DIR, "query_results")
        os.makedirs(query_results_dir, exist_ok=True)
        query_results_file = os.path.join(query_results_dir, f"query_{timestamp}_{query_id}.json")
        
        query_result_data = {
            "query": question,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "evaluation_id": evaluation_id,
            "results": results,
            "auto_evaluations": auto_evaluations if args.auto_eval else None,
            "context": context
        }
        
        with open(query_results_file, 'w', encoding='utf-8') as f:
            json.dump(query_result_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ì‘ë‹µì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ íŒŒì¼: {query_results_file}")
        return 0
        
    except Exception as e:
        logger.error(f"ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ì„¸ë¶€ ì˜¤ë¥˜ ë‚´ìš©:", traceback.format_exc())
        return 1

def streamlit_main():
    import streamlit as st
    
    # Streamlit í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ RAG ì‹œìŠ¤í…œ",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    # ì œëª© ë° ì„¤ëª…
    st.title("ğŸ“Š í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ RAG ì‹œìŠ¤í…œ")
    st.subheader("(Ollama ëª¨ë¸ ë¹„êµ: Llama3.1 vs Gemma3)")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ í™˜ê²½ ì„¤ì •")
    
    # PDF íŒŒì¼ ì„ íƒ
    pdf_files = glob.glob(os.path.join(SCRIPT_DIR, "*.pdf"))
    if not pdf_files:
        st.sidebar.warning("PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        selected_pdf = PDF_PATH  # ê¸°ë³¸ PDF ê²½ë¡œ ì‚¬ìš©
    else:
        selected_pdf = st.sidebar.selectbox(
            "PDF íŒŒì¼ ì„ íƒ",
            options=pdf_files,
            index=0
        )
    
    # ì²­í¬ í¬ê¸° ë° ê²¹ì¹¨ ì„¤ì •
    chunk_size = st.sidebar.slider("ì²­í¬ í¬ê¸°", min_value=100, max_value=1000, value=500, step=50)
    chunk_overlap = st.sidebar.slider("ì²­í¬ ê²¹ì¹¨", min_value=50, max_value=300, value=150, step=25)
    
    # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì„¤ì •
    top_k = st.sidebar.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (Top-K)", min_value=3, max_value=20, value=10)
    
    # ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸ ì˜µì…˜
    force_update = st.sidebar.checkbox("ë²¡í„° ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸", value=False)
    
    # HNSW ì¸ë±ìŠ¤ ì‚¬ìš© ì˜µì…˜
    use_hnsw = st.sidebar.checkbox("HNSW ì¸ë±ìŠ¤ ì‚¬ìš© (ì •í™•ë„ í–¥ìƒ)", value=True)
    
    # HNSW íŒŒë¼ë¯¸í„° ì„¤ì • (ê³ ê¸‰ ì„¤ì • ì„¹ì…˜)
    with st.sidebar.expander("HNSW ê³ ê¸‰ ì„¤ì •", expanded=False):
        ef_search = st.slider("ef_search (ê²€ìƒ‰ ì •í™•ë„)", min_value=50, max_value=500, value=200, step=50)
        ef_construction = st.slider("ef_construction (êµ¬ì¶• ì •í™•ë„)", min_value=50, max_value=500, value=200, step=50)
        m_param = st.slider("M (ì—°ê²°ì„±)", min_value=8, max_value=128, value=64, step=8)
        st.info("ë†’ì€ ê°’ = ë†’ì€ ì •í™•ë„ & ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
    
    # ì¸ë±ìŠ¤ ê²½ë¡œ í‘œì‹œ
    index_dir = os.path.join(SCRIPT_DIR, "Index")
    st.sidebar.info(f"ğŸ“ ì¸ë±ìŠ¤ ê²½ë¡œ: {index_dir}")
    
    # ìë™ í‰ê°€ ì˜µì…˜
    auto_eval = st.sidebar.checkbox("ìë™ í‰ê°€ í™œì„±í™” (gemma3:12b í•„ìš”)", value=True)
    
    # íƒ­ ì„¤ì •
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ ì§ˆë¬¸ ì‘ë‹µ", "ğŸ”„ ìë™ í…ŒìŠ¤íŠ¸", "ğŸ” ë””ë²„ê·¸"])
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    logger = setup_logging()
    evaluator = ModelEvaluator()
    auto_evaluator = AutoEvaluator(model_name="gemma3:12b")
    auto_question_generator = AutoQuestionGenerator(model_name="gemma3:12b")
    
    # Ollama ì„œë²„ ì—°ê²° ë° ëª¨ë¸ í™•ì¸
    print("ğŸ”„ Ollama ëª¨ë¸ í™•ì¸ ì¤‘...")
    available_models = check_ollama_models()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ìŠ¤íŠ¸ë¦¼ë¦¿ì— í‘œì‹œ
    if available_models:
        st.sidebar.success(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(available_models)}")
    else:
        st.sidebar.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (HNSW íŒŒë¼ë¯¸í„° ì ìš©)
    rag = RAGSystem(
        use_hnsw=use_hnsw,
        ef_search=ef_search if 'ef_search' in locals() else 200,
        ef_construction=ef_construction if 'ef_construction' in locals() else 200,
        m=m_param if 'm_param' in locals() else 64
    )
    
    # ... (rest of the code remains unchanged)

# í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ìƒìˆ˜ë¡œ ë³€ê²½)
EVAL_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€ ìŒì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì—„ê²©í•œ í‰ê°€ìì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ˆë¬¸, ê´€ë ¨ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸, ëª¨ë¸ ë‹µë³€ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€ì´ ì •í™•í•˜ê³  ê´€ë ¨ì„±ì´ ë†’ì€ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 1-5ì  ì²™ë„ë¡œ í‰ê°€í•˜ì„¸ìš”:
1: ì™„ì „íˆ ì˜ëª»ëœ ì •ë³´ë¥¼ ì œê³µí•˜ê±°ë‚˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‹µë³€
2: ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ì€ ìˆì§€ë§Œ ë¶€ì •í™•í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•œ ë‹µë³€
3: ê¸°ë³¸ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í–ˆì§€ë§Œ ì„¸ë¶€ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì•½ê°„ì˜ ì˜¤ë¥˜ê°€ ìˆìŒ
4: ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ë‹µë³€ì´ì§€ë§Œ ì™„ë²½í•˜ì§€ ì•ŠìŒ
5: ì™„ë²½í•˜ê²Œ ì •í™•í•˜ê³  í¬ê´„ì ì´ë©° ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œ ë‹µë³€

[ì§ˆë¬¸]
{question}

[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]
{context}

[ëª¨ë¸ ë‹µë³€]
{answer}

[í‰ê°€]
ìœ„ ë‹µë³€ì— ëŒ€í•œ í‰ê°€ë¥¼ 1-5ì  ì²™ë„ë¡œ ìˆ˜í–‰í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë‹µë³€ í˜•ì‹:
ì ìˆ˜: (1-5 ì‚¬ì´ì˜ ì •ìˆ˜ë§Œ ì…ë ¥)
ì´ìœ : (í‰ê°€ ì´ìœ  ì„¤ëª…)"""

class AutoEvaluator:
    def __init__(self, model_name: str = "gemma3:12b"):
        self.model_name = model_name
        
    def evaluate_answer(self, question: str, context: str, answer: str, stream: bool = True) -> Dict[str, Any]:
        """ë‹µë³€ì˜ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ í‰ê°€"""
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í‰ê°€ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê³ ë ¤)
        max_context_length = 1500
        if len(context) > max_context_length:
            context_parts = context.split("\n\n")
            optimized_context = []
            current_length = 0
            
            for part in context_parts:
                if current_length + len(part) <= max_context_length:
                    optimized_context.append(part)
                    current_length += len(part)
                else:
                    break
            
            context = "\n\n".join(optimized_context)
        
        prompt = EVAL_PROMPT_TEMPLATE.format(
            question=question,
            context=context,
            answer=answer
        )
        
        # í‰ê°€ ê²°ê³¼ë¥¼ í•˜ë“œì½”ë”©ëœ ì˜ˆì‹œë¡œ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” LLMì„ í˜¸ì¶œí•´ì•¼ í•¨)
        # ì´ ë¶€ë¶„ì€ ì„ì‹œë¡œ ì¶”ê°€í•˜ì—¬ ì½”ë“œê°€ ë™ì‘í•˜ë„ë¡ í•¨
        example_evaluation = {
            "score": 3,
            "reason": "ë‹µë³€ì´ ê¸°ë³¸ì ì¸ ì •ë³´ëŠ” ì œê³µí•˜ì§€ë§Œ, ì„¸ë¶€ ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.",
            "raw_evaluation": "í‰ê°€ ì „ì²´ í…ìŠ¤íŠ¸"
        }
        
        return example_evaluation


# ì§ˆë¬¸ ìƒì„±ê¸° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
QUESTION_GENERATOR_TEMPLATE = """ë‹¹ì‹ ì€ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì— ê´€í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”:
1. ì¬ë¬´/ì‹¤ì  ê´€ë ¨ ì§ˆë¬¸ (ìˆ˜ìµ, ì†ì‹¤, ì„±ì¥ë¥  ë“±)
2. ì‚¬ì—… ì „ëµ ê´€ë ¨ ì§ˆë¬¸
3. ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê´€ë ¨ ì§ˆë¬¸
4. ì§€ë°°êµ¬ì¡° ê´€ë ¨ ì§ˆë¬¸
5. ìƒí’ˆ/ì„œë¹„ìŠ¤ ê´€ë ¨ ì§ˆë¬¸

[ë¬¸ì„œ ë‚´ìš©]
{context}

ìœ„ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§ˆë¬¸ 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”:
["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3", "ì§ˆë¬¸4", "ì§ˆë¬¸5"]"""


class AutoQuestionGenerator:
    def __init__(self, model_name: str = "gemma3:12b"):
        self.model_name = model_name
    
    def generate_questions(self, context: str, stream: bool = True) -> List[str]:
        """ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        max_context_length = 2000
        if len(context) > max_context_length:
            context_parts = context.split("\n\n")
            optimized_context = []
            current_length = 0
            
            for part in context_parts:
                if current_length + len(part) <= max_context_length:
                    optimized_context.append(part)
                    current_length += len(part)
                else:
                    break
            
            context = "\n\n".join(optimized_context)
        
        prompt = QUESTION_GENERATOR_TEMPLATE.format(context=context)
        
        # ì„ì‹œë¡œ í•˜ë“œì½”ë”©ëœ ì§ˆë¬¸ ëª©ë¡ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” LLMì„ í˜¸ì¶œí•´ì•¼ í•¨)
        # ì´ ë¶€ë¶„ì€ ì„ì‹œë¡œ ì¶”ê°€í•˜ì—¬ ì½”ë“œê°€ ë™ì‘í•˜ë„ë¡ í•¨
        example_questions = [
            "í•œí™”ì†í•´ë³´í—˜ì˜ 2024ë…„ 1ë¶„ê¸° ë‹¹ê¸°ìˆœì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?",
            "í•œí™”ì†í•´ë³´í—˜ì˜ ì£¼ìš” ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì „ëµì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "í•œí™”ì†í•´ë³´í—˜ì˜ ë””ì§€í„¸ ì „í™˜ ì „ëµì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "í•œí™”ì†í•´ë³´í—˜ì˜ ì§€ë°°êµ¬ì¡° íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "í•œí™”ì†í•´ë³´í—˜ì˜ ì£¼ìš” ë³´í—˜ ìƒí’ˆ ë¼ì¸ì—…ì€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?"
        ]
        
        return example_questions


class AutoTestManager:
    def __init__(self, rag_system: RAGSystem, test_count: int = 5):
        """ìë™ í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.rag_system = rag_system
        self.test_count = test_count
        self.question_generator = AutoQuestionGenerator()
        self.evaluator = AutoEvaluator()
        self.available_models = ["gemma3:12b", "gemma3:7.8b", "claude3:sonnet", "claude3:haiku"]
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self.results = {
            "tests": [],
            "summary": {
                "avg_score": 0,
                "count": 0,
                "score_distribution": {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
            }
        }
    
    def run_auto_test(self, use_random_docs: bool = True, test_count: Optional[int] = None) -> Dict[str, Any]:
        """ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if test_count is not None:
            self.test_count = test_count
        
        test_results = []
        
        if use_random_docs:
            # ëœë¤ ë¬¸ì„œì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            docs = self.rag_system.vectorstore.get_random_documents(min(20, self.test_count * 2))
            for doc in docs[:self.test_count]:
                test_results.extend(self._run_test_on_document(doc))
        else:
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            # êµ¬í˜„ í•„ìš”ì‹œ ì¶”ê°€
            pass
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì—…ë°ì´íŠ¸
        self._update_summary()
        
        return self.results
    
    def _run_test_on_document(self, document) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ë¬¸ì„œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        context = document.page_content
        source = document.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
        
        # ì§ˆë¬¸ ìƒì„±
        try:
            questions = self.question_generator.generate_questions(context)
        except Exception as e:
            logging.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return []
        
        test_results = []
        
        # ê° ì§ˆë¬¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for question in questions[:1]:  # ë¬¸ì„œë‹¹ ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ ì‚¬ìš© (ë¶€í•˜ ì œí•œ)
            try:
                # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
                retrieved_docs = self.rag_system.search(question, top_k=5)
                answer = self.rag_system.answer(question, retrieved_docs)
                
                # ë‹µë³€ í‰ê°€
                evaluation = self.evaluator.evaluate_answer(
                    question=question,
                    context=context,
                    answer=answer
                )
                
                # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
                test_result = {
                    "question": question,
                    "source": source,
                    "context": context[:500] + ("..." if len(context) > 500 else ""),
                    "answer": answer,
                    "evaluation": evaluation,
                    "score": evaluation["score"],
                    "timestamp": datetime.now().isoformat()
                }
                
                test_results.append(test_result)
                self.results["tests"].append(test_result)
                
            except Exception as e:
                logging.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        
        return test_results
    
    def _update_summary(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì—…ë°ì´íŠ¸"""
        tests = self.results["tests"]
        
        if not tests:
            return
        
        # ì ìˆ˜ ë¶„í¬ ì´ˆê¸°í™”
        score_distribution = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
        
        # ê° í…ŒìŠ¤íŠ¸ ê²°ê³¼ì˜ ì ìˆ˜ ì¹´ìš´íŠ¸
        total_score = 0
        for test in tests:
            score = test.get("score", 0)
            if 1 <= score <= 5:
                score_distribution[score] += 1
                total_score += score
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = total_score / len(tests) if tests else 0
        
        # ìš”ì•½ ì—…ë°ì´íŠ¸
        self.results["summary"] = {
            "avg_score": round(avg_score, 2),
            "count": len(tests),
            "score_distribution": score_distribution
        }

def check_ollama_models() -> List[str]:
    """Ollama APIë¥¼ í†µí•´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    # ì´ë¯¸ ì‹¤í–‰ëœ ê²½ìš° ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
    if hasattr(check_ollama_models, '_cached_models'):
        return check_ollama_models._cached_models
    
    api_url = "http://localhost:11434/api/tags"
    try:
        logger.info("Ollama REST APIë¡œ ëª¨ë¸ ì¡°íšŒ ì‹œë„ ì¤‘...")
        print("ğŸ”„ Ollama REST APIë¡œ ëª¨ë¸ ì¡°íšŒ ì‹œë„ ì¤‘...")
        
        response = requests.get(api_url, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            models = [model["name"] for model in data.get("models", [])]
            
            if models:
                logger.info(f"Ollama REST API ì—°ê²° ì„±ê³µ: {len(models)}ê°œ ëª¨ë¸ ë°œê²¬")
                print(f"âœ“ Ollama REST API ì—°ê²° ì„±ê³µ: {len(models)}ê°œ ëª¨ë¸ ë°œê²¬")
                
                # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ìºì‹œ
                check_ollama_models._cached_models = models
                return models
            else:
                logger.warning("Ollama APIì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                print("âš ï¸ Ollama APIì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.error(f"Ollama API ì˜¤ë¥˜: {response.status_code}")
            print(f"âš ï¸ Ollama API ì˜¤ë¥˜: {response.status_code}")
    except Exception as e:
        logger.error(f"Ollama API ì—°ê²° ì‹¤íŒ¨: {e}")
        print(f"âš ï¸ Ollama API ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # ê¸°ë³¸ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ì—°ê²° ì‹¤íŒ¨ì‹œ)
    check_ollama_models._cached_models = []
    return []

# --- ëª¨ë¸ í‰ê°€ ê´€ë¦¬ í´ë˜ìŠ¤ ---
class ModelEvaluator:
    def __init__(self):
        """ëª¨ë¸ í‰ê°€ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.evaluation_file = EVALUATION_FILE
        self.evaluations = self._load_evaluations()
    
    def _load_evaluations(self) -> Dict:
        """ì €ì¥ëœ í‰ê°€ ë°ì´í„° ë¡œë“œ"""
        os.makedirs(os.path.dirname(self.evaluation_file), exist_ok=True)
        if os.path.exists(self.evaluation_file):
            try:
                with open(self.evaluation_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ í‰ê°€ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
                return {"evaluations": []}
        return {"evaluations": []}
    
    def _save_evaluations(self):
        """í‰ê°€ ë°ì´í„° ì €ì¥"""
        os.makedirs(os.path.dirname(self.evaluation_file), exist_ok=True)
        try:
            with open(self.evaluation_file, 'w', encoding='utf-8') as f:
                json.dump(self.evaluations, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def save_evaluation(self, question: str, context: str, answers: Dict[str, str], metadata: Dict = None) -> str:
        """ìƒˆ í‰ê°€ ì„¸ì…˜ ì €ì¥"""
        # í‰ê°€ ID ìƒì„±
        evaluation_id = f"eval_{int(time.time())}_{hashlib.md5(question.encode()).hexdigest()[:8]}"
        
        # ìƒˆ í‰ê°€ ë°ì´í„° ìƒì„±
        evaluation = {
            "id": evaluation_id,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "question": question,
            "context": context[:1000] + "..." if len(context) > 1000 else context,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            "answers": answers,
            "scores": {},
            "metadata": metadata or {}
        }
        
        # í‰ê°€ ëª©ë¡ì— ì¶”ê°€
        self.evaluations["evaluations"].append(evaluation)
        self._save_evaluations()
        
        return evaluation_id
    
    def add_evaluation_score(self, evaluation_id: str, model_name: str, score: int, comments: str = ""):
        """íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ í‰ê°€ ì ìˆ˜ ì¶”ê°€"""
        for eval_item in self.evaluations["evaluations"]:
            if eval_item["id"] == evaluation_id:
                eval_item["scores"][model_name] = {
                    "score": score,
                    "comments": comments,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                }
                self._save_evaluations()
                return True
        return False
    
    def get_evaluation(self, evaluation_id: str) -> Dict:
        """íŠ¹ì • í‰ê°€ ë°ì´í„° ì¡°íšŒ"""
        for eval_item in self.evaluations["evaluations"]:
            if eval_item["id"] == evaluation_id:
                return eval_item
        return {}
    
    def get_all_evaluations(self) -> List[Dict]:
        """ëª¨ë“  í‰ê°€ ë°ì´í„° ì¡°íšŒ"""
        return self.evaluations["evaluations"]
    
    def get_model_avg_scores(self) -> Dict[str, float]:
        """ëª¨ë¸ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°"""
        model_scores = {}
        model_counts = {}
        
        for eval_item in self.evaluations["evaluations"]:
            for model, score_data in eval_item["scores"].items():
                score = score_data.get("score", 0)
                if model not in model_scores:
                    model_scores[model] = 0
                    model_counts[model] = 0
                model_scores[model] += score
                model_counts[model] += 1
        
        # í‰ê·  ê³„ì‚°
        avg_scores = {}
        for model, total_score in model_scores.items():
            count = model_counts.get(model, 0)
            avg_scores[model] = round(total_score / count, 2) if count > 0 else 0
        
        return avg_scores

if __name__ == "__main__":
    try:
        # ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main í•¨ìˆ˜ í˜¸ì¶œ
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ ì¢…ë£Œ (Ctrl+C)")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        sys.exit(1)
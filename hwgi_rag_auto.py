import os
import re
import time
import json
import hashlib
import logging
import traceback
import requests
import torch
import argparse
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import streamlit as st
from dotenv import load_dotenv
import asyncio
import time
import io
import base64
import aiohttp
from io import StringIO
from datetime import datetime
import glob
import sys
import tabula
import random

# ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
DEBUG_MODE = False

# OLLAMA_AVAILABLE ë³€ìˆ˜ ì •ì˜
OLLAMA_AVAILABLE = False

# ollama ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹œë„
try:
    import ollama
    OLLAMA_AVAILABLE = True
    
    # ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì˜ˆì œ (ì°¸ê³ ìš©)
    """
    # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
    models = ollama.list()
    
    # ì±„íŒ… ì‘ë‹µ ìƒì„±
    response = ollama.chat(model='gemma3:4b', messages=[
        {'role': 'user', 'content': 'ì§ˆë¬¸ ë‚´ìš©'}
    ])
    print(response['message']['content'])
    
    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
    for chunk in ollama.chat(
        model='gemma3:4b',
        messages=[{'role': 'user', 'content': 'ì§ˆë¬¸ ë‚´ìš©'}],
        stream=True,
    ):
        print(chunk['message']['content'], end='', flush=True)
    """
except ImportError:
    print("âš ï¸ ollama ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜ ì¶”ê°€
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
SCRIPT_DIR = os.getcwd()
BASE_DIR = os.path.dirname(SCRIPT_DIR)  # ìƒìœ„ ë””ë ‰í† ë¦¬
print(f"í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ê²½ë¡œ: {SCRIPT_DIR}")
print(f"ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ: {BASE_DIR}")


# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenMP ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ (FAISSì™€ Java ì¶©ëŒ ë°©ì§€)
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pypdf
import tabula
import pdfplumber

# RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from sentence_transformers import SentenceTransformer
import faiss

# RAG í‰ê°€ ê´€ë ¨ ë©”íŠ¸ë¦­ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from rank_bm25 import BM25Okapi
    EVAL_LIBS_AVAILABLE = True
except ImportError:
    EVAL_LIBS_AVAILABLE = False

# í™˜ê²½ ì„¤ì •
PDF_PATH = os.path.join(SCRIPT_DIR, "[í•œí™”ì†í•´ë³´í—˜]ì‚¬ì—…ë³´ê³ ì„œ(2025.03.11).pdf")
INDEX_DIR = os.path.join(SCRIPT_DIR, "Index/faiss_index_bge")  # ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬
METADATA_FILE = os.path.join(SCRIPT_DIR, "Index/document_metadata_bge.json")  # ë©”íƒ€ë°ì´í„° íŒŒì¼
LOG_FILE = os.path.join(SCRIPT_DIR, "Log/hwgi_rag_streamlit.log")
CACHE_FILE = os.path.join(SCRIPT_DIR, "Log/query_cache_streamlit.json")
EVALUATION_FILE = os.path.join(SCRIPT_DIR, "Log/model_evaluations.json")  # ëª¨ë¸ í‰ê°€ ê²°ê³¼ ì €ì¥ íŒŒì¼

# Ollama API ê¸°ë³¸ URL ì„¤ì •
OLLAMA_API_BASE = "http://localhost:11434/api"

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„¤ì •
AVAILABLE_MODELS = ["gemma3:4b", "llama3.1:8b", "gemma3:12b"]

# ëª¨ë¸ ì„¤ì •
EMBEDDING_MODELS = {
    "bge-m3": {
        "name": "BAAI/bge-m3",
        "index_dir": INDEX_DIR,
        "metadata_file": METADATA_FILE
    }
}

# ë¡œê¹… ì„¤ì •
def setup_logging(log_level=logging.DEBUG):
    logger = logging.getLogger('hwgi_rag')
    logger.setLevel(log_level)
    if not logger.handlers:
        file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
        file_handler.setLevel(log_level)
        console_handler = logging.StreamHandler()
        console_handler.setLevel(log_level)
        log_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(log_format)
        console_handler.setFormatter(log_format)
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
    return logger

logger = setup_logging()

# E5Embeddings í´ë˜ìŠ¤ë¥¼ BGE-M3 ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
class BGEM3Embeddings(Embeddings):
    def __init__(self, model_name: str = "BAAI/bge-m3"):
        print(f"âœ“ BGE-M3 ì„ë² ë”© ëª¨ë¸ '{model_name}' ì´ˆê¸°í™” ì¤‘...")
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("âœ“ Apple Silicon GPU (MPS) ì‚¬ìš© ê°€ëŠ¥")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print("âœ“ NVIDIA GPU (CUDA) ì‚¬ìš© ê°€ëŠ¥")
        else:
            self.device = torch.device("cpu")
            print("âœ“ CPU ì‚¬ìš©")
        
        self.model.to(self.device)
        print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (32 â†’ 64)
            batch_size = 64
            all_embeddings = []
            
            # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¡œ ë°°ì¹˜ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = [self._preprocess_text(text) for text in texts[i:i + batch_size]]
                with torch.inference_mode():
                    embeddings = self.model.encode(
                        batch,
                        convert_to_tensor=True,
                        device=self.device,
                        normalize_embeddings=True  # L2 ì •ê·œí™” ì ìš©
                    )
                    if self.device.type == "mps":
                        embeddings = embeddings.to("cpu")
                    all_embeddings.extend(embeddings.cpu().numpy().tolist())
            
            return all_embeddings
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ì¿¼ë¦¬ìš© ì ‘ë‘ì‚¬ ì¶”ê°€
            query_text = f"query: {text}"
            with torch.inference_mode():
                embedding = self.model.encode(
                    [query_text],
                    convert_to_tensor=True,
                    device=self.device,
                    normalize_embeddings=True
                )
                # MPS ë””ë°”ì´ìŠ¤ì—ì„œ CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜
                if self.device.type == "mps":
                    embedding = embedding.to("cpu")
                return embedding.cpu().numpy().tolist()[0]
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

# OpenAI ì„ë² ë”© í´ë˜ìŠ¤ ì •ì˜
class OpenAIEmbeddings(Embeddings):
    def __init__(self, model_name: str = "text-embedding-3-small"):
        print(f"âœ“ OpenAI ì„ë² ë”© ëª¨ë¸ '{model_name}' ì´ˆê¸°í™” ì¤‘...")
        load_dotenv()  # .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ë¡œë“œ
        self.model_name = model_name
        self.client = OpenAI()
        print("âœ“ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë°°ì¹˜ í¬ê¸° ì„¤ì • (OpenAI API ì œí•œ ê³ ë ¤)
            batch_size = 100
            all_embeddings = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=batch,
                    encoding_format="float"
                )
                batch_embeddings = [data.embedding for data in response.data]
                all_embeddings.extend(batch_embeddings)
            
            return all_embeddings
        except Exception as e:
            print(f"âŒ OpenAI ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            response = self.client.embeddings.create(
                model=self.model_name,
                input=[text],
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ OpenAI ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

# --- PDF ì²˜ë¦¬ ë° ë¬¸ì„œ ë¶„í•  ---
class PDFProcessor:
    def __init__(self, pdf_path: str):
        # ê²½ë¡œê°€ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
        if not os.path.isabs(pdf_path):
            self.pdf_path = os.path.join(SCRIPT_DIR, pdf_path)
        else:
            self.pdf_path = pdf_path
        self.text_content = []  # í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥
        self.tables = []  # í‘œ ë°ì´í„° ì €ì¥
        self.page_count = 0  # ì´ í˜ì´ì§€ ìˆ˜
        self.pdf_hash = self._calculate_pdf_hash()  # PDF íŒŒì¼ í•´ì‹œ
        self.hash_file = os.path.join(SCRIPT_DIR, "pdf_hash.json")  # í•´ì‹œ ì €ì¥ íŒŒì¼
        logger.info(f"PDFProcessor ì´ˆê¸°í™”: '{self.pdf_path}' íŒŒì¼ ì²˜ë¦¬ ì¤€ë¹„")
    
    def _calculate_pdf_hash(self) -> str:
        """PDF íŒŒì¼ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_hash = hashlib.md5(file.read()).hexdigest()
            return pdf_hash
        except Exception as e:
            logger.error(f"PDF í•´ì‹œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return ""
    
    def _load_previous_hash(self) -> str:
        """ì´ì „ì— ì²˜ë¦¬í•œ PDFì˜ í•´ì‹œê°’ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            if os.path.exists(self.hash_file):
                with open(self.hash_file, 'r') as f:
                    data = json.load(f)
                    return data.get('pdf_hash', '')
            return ''
        except Exception as e:
            logger.error(f"ì´ì „ í•´ì‹œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return ''
    
    def _save_current_hash(self):
        """í˜„ì¬ PDF í•´ì‹œê°’ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            os.makedirs(os.path.dirname(self.hash_file), exist_ok=True)
            data = {
                'pdf_hash': self.pdf_hash,
                'pdf_path': self.pdf_path,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            with open(self.hash_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ í˜„ì¬ PDF í•´ì‹œ ì €ì¥ ì™„ë£Œ: {self.hash_file}")
        except Exception as e:
            print(f"âš ï¸ í˜„ì¬ í•´ì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"í˜„ì¬ í•´ì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def needs_processing(self) -> bool:
        """PDFê°€ ìƒˆë¡œìš´ ë°ì´í„°ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        previous_hash = self._load_previous_hash()
        return previous_hash != self.pdf_hash
    
    def extract_text(self) -> List[Document]:
        logger.info("ğŸ“„ PDF í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ ì‹œì‘")
        print("ğŸ“„ PDF í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
        documents = []
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)
                self.page_count = len(pdf_reader.pages)
                
                for page_num, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    if text.strip():
                        doc_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                        
                        # í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥
                        self.text_content.append({
                            "page": page_num + 1,
                            "content": text,
                            "hash": doc_hash
                        })
                        
                        documents.append(
                            Document(
                                page_content=text,
                                metadata={"page": page_num + 1, "source": "text", "hash": doc_hash}
                            )
                        )
            logger.info(f"âœ… ì´ {self.page_count}í˜ì´ì§€ì—ì„œ {len(documents)}ê°œì˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ì¶œ ì™„ë£Œ")
            return documents
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def table_id_generation(self, element):
        """í…Œì´ë¸” ID ìƒì„± í•¨ìˆ˜"""
        if "Table" not in element:
            return {}
        else:
            values = element['Table']  # list of tables
            keys = [f"element{element['id']}-table{i}" for i in range(len(values))]
            return dict(zip(keys, values))

    def extract_cell_color(self, table):
        """í…Œì´ë¸”ì˜ ì²« ì…€ê³¼ ë§ˆì§€ë§‰ ì…€ ìƒ‰ìƒ ì¶”ì¶œ í•¨ìˆ˜"""
        page_image = table.page.to_image()
        cell_image_first = page_image.original.crop(table.cells[0])
        cell_image_last = page_image.original.crop(table.cells[-1])
        
        res_list = []
        for cell_image in [cell_image_first, cell_image_last]:
            width, height = cell_image.size
            background_pixel = cell_image.getpixel((width/5, height/5))
            res_list.append(background_pixel)
        
        return res_list

    def extract_image(self, table, resolution=300):
        """í…Œì´ë¸” ì´ë¯¸ì§€ ì¶”ì¶œ í•¨ìˆ˜"""
        scale_factor = resolution / 72
        x0, y0, x1, y1 = table.bbox
        
        x0 *= scale_factor
        y0 *= scale_factor
        x1 *= scale_factor
        y1 *= scale_factor
        
        img = table.page.to_image(resolution=resolution)
        table_img = img.original.crop((x0, y0, x1, y1))
        
        return table_img

    def extract_table_info(self, table):
        """í…Œì´ë¸” ë©”íƒ€ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜"""
        table_dict = {
            'page': self.extract_page_number(str(table.page)),
            'bbox': table.bbox,
            'ncol': len(table.columns),
            'nrow': len(table.rows),
            'content': table.extract(),
            'cell_color': self.extract_cell_color(table),
            'img': self.extract_image(table)
        }
        
        return table_dict

    def compare_tables(self, table_A, table_B):
        """ë‘ í…Œì´ë¸”ì´ ê°™ì€ í…Œì´ë¸”ì¸ì§€ ë¹„êµ"""
        prev_info = self.extract_table_info(table_A)
        curr_info = self.extract_table_info(table_B)
        
        counter = 0
        # ë‘ í…Œì´ë¸”ì˜ í˜ì´ì§€ê°€ ì¸ì ‘í•´ ìˆëŠ”ê°€?
        if curr_info['page'] - prev_info['page'] == 1:
            counter += 1
        
        # í…Œì´ë¸” ìœ„ì¹˜ê°€ ì´ì–´ì§€ëŠ”ê°€?
        if (np.round(prev_info['bbox'][3], 0) > 780) and (np.round(curr_info['bbox'][1], 0) == 50):
            counter += 1
        
        # ì…€ ìƒ‰ìƒì´ ê°™ì€ê°€?
        if prev_info['cell_color'][1] == curr_info['cell_color'][0]:
            counter += 1
        
        # ì»¬ëŸ¼ ìˆ˜ê°€ ê°™ì€ê°€?
        if prev_info['ncol'] == curr_info['ncol']:
            counter += 1
        
        decision = 'same table' if counter == 4 else 'different table'
        return [(counter, decision)]

    def find_table_location_in_text(self, element_content):
        """ì½˜í…ì¸  ë‚´ í…Œì´ë¸” ìœ„ì¹˜ ì°¾ê¸°"""
        start_pattern = '<table>'
        table_start_position = re.finditer(start_pattern, element_content)
        start_positions = [(match.start(), match.end()) for match in table_start_position]
        
        end_pattern = '</table>'
        table_end_position = re.finditer(end_pattern, element_content)
        end_positions = [(match.start(), match.end()) for match in table_end_position]
        
        table_location_in_text = [(start[0], end[1]) 
                                for start, end in zip(start_positions, end_positions)]
        
        return table_location_in_text

    def group_table_position(self, element_table):
        """ì—°ì†ëœ í…Œì´ë¸”ì˜ í¬ì§€ì…˜ì„ ë¬¶ì–´ì£¼ëŠ” í•¨ìˆ˜"""
        pos = 0
        counter = 0
        result = []
        
        for i in range(1, len(element_table)):
            counter += 1
            table_comparison_result = self.compare_tables(element_table[i-1], element_table[i])[0][1]
            
            if table_comparison_result != 'same table':
                result.append([pos, pos+counter])
                pos += counter
                counter = 0
        
        # ë§ˆì§€ë§‰ ê·¸ë£¹ ì¶”ê°€
        result.append([pos, pos + counter + 1])
        return result

    def merge_dicts(self, dict_list):
        """ì—¬ëŸ¬ ê°œì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜"""
        merged_dict = {
            'page': [],
            'bbox': [],
            'ncol': 0,
            'nrow': 0,
            'content': [],
            'cell_color': [],
            'img': [],
            'obj_type': ['table']
        }
        
        for d in dict_list:
            merged_dict['page'].append(d['page'])
            merged_dict['bbox'].append(d['bbox'])
            merged_dict['ncol'] = max(merged_dict['ncol'], d['ncol'])
            merged_dict['nrow'] += d['nrow']
            merged_dict['content'] += (d['content'])
            merged_dict['cell_color'].append(d['cell_color'])
            merged_dict['img'].append(d['img'])
        
        return merged_dict

    def extract_page_number(self, text):
        """í…Œì´ë¸”ì´ ìœ„ì¹˜í•œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ í•¨ìˆ˜"""
        match = re.search(r"<Page:(\d+)>", text)
        return int(match.group(1)) if match else None

    def extract_tables(self) -> List[Document]:
        """PDFì—ì„œ í‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  Documentë¡œ ë³€í™˜"""
        logger.info("ğŸ“Š PDF í‘œ ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
        print("ğŸ“Š PDF í‘œ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        try:
            table_documents = []
            
            with pdfplumber.open(self.pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    tables = page.extract_tables()
                    if not tables:
                        continue
                    
                    for table_idx, table in enumerate(tables):
                        # ë¹ˆ í–‰/ì—´ ì œê±° ë° ë¬¸ìì—´ ë³€í™˜
                        table_content = []
                        for row in table:
                            if any(cell for cell in row):  # ë¹ˆ í–‰ ì œì™¸
                                cleaned_row = [str(cell).strip() if cell else "" for cell in row]
                                table_content.append(cleaned_row)
                        
                        if not table_content:
                            continue
                        
                        # CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        table_text = '\n'.join([','.join(row) for row in table_content])
                        table_hash = hashlib.md5(table_text.encode('utf-8')).hexdigest()
                        
                        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        metadata = {
                            'table_id': f'table_p{page_num}_t{table_idx + 1}',
                            'page': page_num,
                            'source': 'table',
                            'hash': table_hash,
                            'row_count': len(table_content),
                            'col_count': len(table_content[0]) if table_content else 0
                        }
                        
                        # í‘œ ì •ë³´ ì €ì¥
                        self.tables.append({
                            'table_id': metadata['table_id'],
                            'content': table_text,
                            'raw_data': table_content,
                            'hash': table_hash,
                            'metadata': metadata
                        })
                        
                        # Document ê°ì²´ ìƒì„±
                        table_documents.append(
                            Document(
                                page_content=f"í‘œ {metadata['table_id']}:\n{table_text}",
                                metadata=metadata
                            )
                        )
            
            logger.info(f"âœ… {len(table_documents)}ê°œì˜ í‘œ ì²˜ë¦¬ ì™„ë£Œ")
            return table_documents
            
        except Exception as e:
            logger.error(f"âŒ í‘œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def process(self) -> List[Document]:
        """PDFë¥¼ ì²˜ë¦¬í•˜ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        print(f"\n{'â”€'*60}")
        print("ğŸ“Œ 1ë‹¨ê³„: PDF ë¬¸ì„œ ì²˜ë¦¬")
        print(f"{'â”€'*60}")
        
        if not self.needs_processing():
            logger.info("ì´ì „ì— ì²˜ë¦¬ëœ ë™ì¼í•œ PDF íŒŒì¼ ê°ì§€. ë³€ê²½ ì—†ìŒìœ¼ë¡œ íŒë‹¨.")
            print("âœ“ ì´ë¯¸ ì²˜ë¦¬ëœ PDF íŒŒì¼ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ì²˜ë¦¬ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.")
            # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš©í•˜ë„ë¡ í•¨
            return []
        
        logger.info("===== PDF ì²˜ë¦¬ ì‹œì‘ =====")
        text_docs = self.extract_text()
        table_docs = self.extract_tables()
        all_docs = text_docs + table_docs
        
        if not all_docs:
            print("âš ï¸ PDFì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []
        
        # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ë©´ í˜„ì¬ í•´ì‹œ ì €ì¥
        self._save_current_hash()
        logger.info(f"ğŸ“š {len(all_docs)}ê°œì˜ ë¬¸ì„œ ì¡°ê° ìƒì„±ë¨")
        print(f"ğŸ“š PDF ì²˜ë¦¬ ì™„ë£Œ: {len(text_docs)}ê°œ í…ìŠ¤íŠ¸ ë¬¸ì„œ, {len(table_docs)}ê°œ í‘œ ë¬¸ì„œ ìƒì„±")
        return all_docs
    
    def visualize_table(self, table_id: int):
        """íŠ¹ì • í‘œë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤ (matplotlib ì‚¬ìš©)"""
        if not self.tables:
            print("âš ï¸ í‘œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìœ íš¨í•œ table_id í™•ì¸
        table_index = table_id - 1
        if table_index < 0 or table_index >= len(self.tables):
            print(f"âš ï¸ í‘œ #{table_id}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        try:
            table_data = self.tables[table_index]
            df = pd.DataFrame(table_data["raw_data"])
            
            # í‘œ ì‹œê°í™”
            fig, ax = plt.figure(figsize=(12, 6)), plt.gca()
            ax.axis('off')
            ax.table(
                cellText=df.values,
                colLabels=df.columns,
                cellLoc='center',
                loc='center'
            )
            plt.title(f"í‘œ {table_data['table_id']}", fontsize=14)
            plt.tight_layout()
            plt.show()
            
            # í…Œì´ë¸” ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“Š í‘œ {table_data['table_id']} ì •ë³´:")
            print(f"  - í–‰ ìˆ˜: {df.shape[0]}")
            print(f"  - ì—´ ìˆ˜: {df.shape[1]}")
            print(f"  - ì—´ ì´ë¦„: {', '.join(df.columns)}")
            
        except Exception as e:
            print(f"âŒ í‘œ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

class DocumentSplitter:
    def __init__(self, chunk_size=500, chunk_overlap=150):  # ì²­í¬ ì‚¬ì´ì¦ˆ ì¶•ì†Œ (800 â†’ 500), ê²¹ì¹¨ ë¹„ìœ¨ 30% ìœ ì§€
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            # êµ¬ë¶„ì ìµœì í™”: ë¬¸ë‹¨ > ë¬¸ì¥ > êµ¬ë‘ì  > ê³µë°± ìˆœì„œë¡œ ì‹œë„
            separators=[
                "\n\n",  # ë¬¸ë‹¨ êµ¬ë¶„
                "\n",    # ì¤„ë°”ê¿ˆ
                ".",     # ë¬¸ì¥ ë
                "!",     # ê°íƒ„ë¬¸
                "?",     # ì˜ë¬¸ë¬¸
                ";",     # ì„¸ë¯¸ì½œë¡ 
                ":",     # ì½œë¡ 
                ",",     # ì‰¼í‘œ
                " ",     # ê³µë°±
                ""       # ë§ˆì§€ë§‰ ìˆ˜ë‹¨
            ]
        )
        logger.info(f"DocumentSplitter ì´ˆê¸°í™”: ì²­í¬ í¬ê¸°={chunk_size}, ê²¹ì¹¨={chunk_overlap}")
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        logger.info(f"ğŸ”ª ë¬¸ì„œ ë¶„í•  ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
        print("ğŸ”ª ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
        try:
            chunks = self.text_splitter.split_documents(documents)
            for i, chunk in enumerate(chunks):
                chunk.metadata['chunk_id'] = f"chunk_{i}"
            logger.info(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")
            return chunks
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜: {e}")
            return documents

# --- Query í™•ì¥ (Ollama API ì´ìš©) ---
class QueryExpander:
    def __init__(self, models: List[str] = AVAILABLE_MODELS):
        self.models = models
        logger.info(f"QueryExpander ì´ˆê¸°í™”: ëª¨ë¸={', '.join(models)}")
        self.prompt_template = """ë‹¹ì‹ ì€ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ PDF ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì›ë˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ 3ê°œì˜ ë³€í˜• ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ê° ë³€í˜• ì¿¼ë¦¬ëŠ” ë‹¤ìŒ íŠ¹ì„±ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤:
1. ê¸ˆìœµ/ë³´í—˜ ìš©ì–´ ì¤‘ì‹¬: ì›ë˜ ì§ˆë¬¸ì—ì„œ ê¸ˆìœµ, ë³´í—˜, ì¬ë¬´ì™€ ê´€ë ¨ëœ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ êµ¬ì„±
2. ì‚¬ì—…ë³´ê³ ì„œ ë§¥ë½: ì‚¬ì—…ë³´ê³ ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´(ì¬ë¬´ìƒíƒœ, ê²½ì˜ì‹¤ì , ì‚¬ì—…ì „ëµ, ë¦¬ìŠ¤í¬, ì§€ë°°êµ¬ì¡° ë“±)ì— ë§ê²Œ ë³€í˜•
3. êµ¬ì²´ì ì¸ ì •ë³´ ì§€í–¥: ìˆ«ì, ë¹„ìœ¨, ê¸ˆì•¡, ë‚ ì§œ ë“± êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•œ í‘œí˜„ í¬í•¨

ì˜ˆì‹œ:
- ì›ë˜ ì§ˆë¬¸: "í•œí™”ì†í•´ë³´í—˜ì˜ ìˆœì´ìµì€?"
  ë³€í˜•1: "í•œí™”ì†í•´ë³´í—˜ ë‹¹ê¸°ìˆœì´ìµ ê¸ˆì•¡"
  ë³€í˜•2: "í•œí™”ì†í•´ë³´í—˜ ì˜ì—…ì´ìµ ì¬ë¬´ì œí‘œ"
  ë³€í˜•3: "í•œí™”ì†í•´ë³´í—˜ ìˆ˜ìµ ì‹¤ì  ì—°ë„ë³„"

- ì›ë˜ ì§ˆë¬¸: "í•œí™”ì†í•´ë³´í—˜ì˜ ì£¼ìš” ì‚¬ì—…ì€?"
  ë³€í˜•1: "í•œí™”ì†í•´ë³´í—˜ ì£¼ë ¥ ë³´í—˜ìƒí’ˆ ì¢…ë¥˜"
  ë³€í˜•2: "í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë¶„ì•¼ ë§¤ì¶œ ë¹„ì¤‘"
  ë³€í˜•3: "í•œí™”ì†í•´ë³´í—˜ í•µì‹¬ì‚¬ì—… ì „ëµ ë°©í–¥"

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ ìœ íš¨í•œ JSON ë°°ì—´ë¡œë§Œ ì‘ë‹µí•˜ë©°, ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì£¼ì„ì€ ì ˆëŒ€ í¬í•¨ì‹œí‚¤ì§€ ë§ˆì„¸ìš”:
["ë³€í˜•1", "ë³€í˜•2", "ë³€í˜•3"]

ì›ë˜ ì§ˆë¬¸: {query}"""
    
    def _generate_with_ollama(self, prompt: str, model: str) -> str:
        if DEBUG_MODE:
            print(f"\nğŸ“ ì¿¼ë¦¬ í™•ì¥ - Ollama API í˜¸ì¶œ ì¤‘ ({model})")
        start_time = time.time()
        try:
            response = requests.post(
                f"{OLLAMA_API_BASE}/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.5, "top_p": 0.95, "num_predict": 500}  # ì˜¨ë„ ë‚®ì¶¤ (0.7 â†’ 0.5)
                }
            )
            response.raise_for_status()
            result = response.json().get("response", "")
            elapsed_time = time.time() - start_time
            if DEBUG_MODE:
                print(f"âœ“ ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
            return result
        except Exception as e:
            logger.error(f"âŒ Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            if DEBUG_MODE:
                print(f"âŒ Ollama API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def _generate_expansion_prompt(self, query: str) -> str:
        """ì¿¼ë¦¬ í™•ì¥ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return f"""ë‹¤ìŒ ì§ˆë¬¸ì„ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•œ 3-4ê°œì˜ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ í™•ì¥í•´ì£¼ì„¸ìš”.
ì›ë˜ ì§ˆë¬¸: {query}

ì£¼ì–´ì§„ ì§ˆë¬¸ì€ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì— ê´€í•œ ê²ƒì…ë‹ˆë‹¤. ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë…ì„ íŒŒì•…í•˜ê³ , ê²€ìƒ‰ì— ìœ ìš©í•œ ìœ ì‚¬ í‘œí˜„ê³¼ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš” (ë²ˆí˜¸ ë§¤ê¸°ê¸°):
1. [ì²« ë²ˆì§¸ ê²€ìƒ‰ ì¿¼ë¦¬]
2. [ë‘ ë²ˆì§¸ ê²€ìƒ‰ ì¿¼ë¦¬]
3. [ì„¸ ë²ˆì§¸ ê²€ìƒ‰ ì¿¼ë¦¬]
4. [ë„¤ ë²ˆì§¸ ê²€ìƒ‰ ì¿¼ë¦¬]

ê° ì¿¼ë¦¬ëŠ” ì›ë˜ ì§ˆë¬¸ì˜ í•µì‹¬ì„ ìœ ì§€í•˜ë˜, ë‹¤ë¥¸ í‘œí˜„ì´ë‚˜ ì¶”ê°€ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í™•ì¥í•´ì•¼ í•©ë‹ˆë‹¤."""
    
    def expand_query(self, query: str) -> List[str]:
        if not query or query.strip() == "":
            return [query]
            
        print(f"\n{'â”€'*60}")
        print(f"ğŸ“Œ 1ë‹¨ê³„: ì¿¼ë¦¬ í™•ì¥ ë° ë¶„ì„")
        print(f"{'â”€'*60}")
        print(f"â–¶ ì›ë³¸ ì¿¼ë¦¬: '{query}'")
        
        # ì¿¼ë¦¬ ê¸¸ì´ì— ë”°ë¼ í™•ì¥ ì „ëµ ì¡°ì •
        if len(query) < 5:  # ë§¤ìš° ì§§ì€ ì¿¼ë¦¬ì˜ ê²½ìš°
            print(f"âš ï¸ ì¿¼ë¦¬ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. í™•ì¥ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return [query]

        # ëª¨ë“  ì¿¼ë¦¬ì— ì›ë³¸ í¬í•¨ (í™•ì¥ ì•ˆ ë˜ë”ë¼ë„)
        all_queries = [query]
        unique_queries = set([query])
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê±´ë„ˆë›¸ ìˆ˜ ìˆë„ë¡ ê° ëª¨ë¸ ê°œë³„ ì‹œë„
        for model in self.models:
            try:
                print(f"\nğŸ¤– {model} ëª¨ë¸ë¡œ ì¿¼ë¦¬ í™•ì¥ ì¤‘...")
                sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
                
                prompt = self._generate_expansion_prompt(query)
                print(f"\nğŸ“ ì¿¼ë¦¬ í™•ì¥ - Ollama API í˜¸ì¶œ ì¤‘ ({model})")
                sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
                
                start_time = time.time()
                responses = self._generate_with_ollama(prompt, model)
                elapsed_time = time.time() - start_time
                print(f"âœ“ ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
                
                # ì‘ë‹µ íŒŒì‹±
                new_queries = []
                for line in responses.split('\n'):
                    line = line.strip()
                    if line and not line.startswith('#') and not line.startswith('=='):
                        # ë²ˆí˜¸ ë˜ëŠ” ëŒ€ì‹œë¡œ ì‹œì‘í•˜ëŠ” í•­ëª© ì œê±°
                        clean_line = re.sub(r'^[\d\-\.\s]+', '', line).strip()
                        if clean_line and len(clean_line) > 5:
                            new_queries.append(clean_line)
                
                # ì¤‘ë³µ ì œê±°
                old_count = len(unique_queries)
                unique_queries.update(new_queries)
                new_count = len(unique_queries)
                
                print(f"âœ“ {new_count - old_count}ê°œì˜ ê³ ìœ  ì¿¼ë¦¬ ìƒì„±ë¨")
                sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
                
                # ì¿¼ë¦¬ê°€ ì¶©ë¶„íˆ ìƒì„±ëœ ê²½ìš° ì¶”ê°€ í™•ì¥ ì¤‘ë‹¨
                if new_count >= 5:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ {model} ëª¨ë¸ ì¿¼ë¦¬ í™•ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ìµœì¢… ì¿¼ë¦¬ ëª©ë¡ ìƒì„± (ì›ë³¸ í¬í•¨)
        all_queries = list(unique_queries)
        
        # ë„ˆë¬´ ë§ì€ ì¿¼ë¦¬ëŠ” í•„í„°ë§ (ìµœëŒ€ 5ê°œ)
        if len(all_queries) > 5:
            # ì›ë³¸ ì¿¼ë¦¬ëŠ” í•­ìƒ í¬í•¨
            filtered_queries = [query]
            # ë‚˜ë¨¸ì§€ ì¤‘ ê°€ì¥ ê¸´ ì¿¼ë¦¬ 4ê°œ ì„ íƒ (ë³´í†µ ë” êµ¬ì²´ì )
            other_queries = [q for q in all_queries if q != query]
            other_queries.sort(key=len, reverse=True)
            filtered_queries.extend(other_queries[:4])
            all_queries = filtered_queries
        
        print(f"\nâœ… ì „ì²´ í™•ì¥ ì™„ë£Œ: {len(all_queries)}ê°œì˜ ê³ ìœ  ì¿¼ë¦¬ ìƒì„±ë¨")
        sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
        return all_queries

# --- RAG ì‹œìŠ¤í…œ (ë²¡í„° ê²€ìƒ‰) ---
class RAGSystem:
    def __init__(self, embedding_type: str = "bge-m3", use_hnsw: bool = True, ef_search: int = 200, ef_construction: int = 200, m: int = 64):
        print("ğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ì‘ë‹µ ìºì‹œ ì´ˆê¸°í™”
        self._cache = {}
        self.cache_file = os.path.join(SCRIPT_DIR, "cache.json")
        
        # í•­ìƒ BGE-M3 ì„ë² ë”© ì‚¬ìš©
        self.embedding_type = "bge-m3"
        self.model_config = EMBEDDING_MODELS["bge-m3"]
        print(f"  - ì„ë² ë”© ëª¨ë¸: {self.model_config['name']} (bge-m3)")
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = BGEM3Embeddings(model_name=self.model_config["name"])
        
        # QueryExpander ì´ˆê¸°í™” ì¶”ê°€
        self.query_expander = QueryExpander(models=AVAILABLE_MODELS)
        
        # FAISS ì¸ë±ìŠ¤ ì„¤ì •
        self.use_hnsw = use_hnsw
        self.ef_search = ef_search
        self.ef_construction = ef_construction
        self.m = m
        
        self.vector_store = None
        self.index_dir = os.path.join(SCRIPT_DIR, "faiss_index")
        self.metadata_file = os.path.join(SCRIPT_DIR, "faiss_metadata.json")
        
        # ìºì‹œ ì´ˆê¸°í™” ì‹œë„
        try:
            self.cache = self._load_cache()
            # ê¸°ì¡´ ìºì‹œë¥¼ _cacheì—ë„ ë³µì‚¬
            self._cache = self.cache.copy()
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œìš´ ìºì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤")
            self.cache = {}
            self._cache = {}
            self._save_cache()  # ìƒˆë¡œìš´ ë¹ˆ ìºì‹œ íŒŒì¼ ìƒì„±
        
        self.qa_prompt = """ë‹¹ì‹ ì€ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ëª…í™•í•˜ê³  ì‚¬ì‹¤ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ì§€ì¹¨]
1. ë¬¸ì„œì—ì„œ ì°¾ì€ í•µì‹¬ ì •ë³´ë¥¼ ë¨¼ì € ë‚˜ì—´í•˜ê³ , ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
2. ìˆ«ì, ë‚ ì§œ, ê¸ˆì•¡ ë“± êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ëŠ” ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.
3. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
4. ë‹µë³€ì€ 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
5. ì „ë¬¸ ìš©ì–´ëŠ” ê°€ëŠ¥í•œ ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ê´€ë ¨ ë¬¸ì„œ:
{context}

ë‹µë³€ í˜•ì‹:
[í•µì‹¬ ì •ë³´]
â€¢ (ì°¾ì€ í•µì‹¬ ì •ë³´ë“¤ì„ ë¶ˆë¦¿ìœ¼ë¡œ ë‚˜ì—´)

[ë‹µë³€]
(ìœ„ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ 3-4ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€)"""
        print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_cache(self) -> Dict[str, str]:
        """ìºì‹œëœ ì‘ë‹µì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            logger.error(f"ìºì‹œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def _save_cache(self):
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                # self._cacheë¡œ ì €ì¥í•˜ë„ë¡ ìˆ˜ì •
                json.dump(self._cache, f, ensure_ascii=False, indent=2)
            # ê·¸ë¦¬ê³  ë™ê¸°í™”ë¥¼ ìœ„í•´ self.cacheë„ ì—…ë°ì´íŠ¸
            self.cache = self._cache.copy()
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _save_document_metadata(self, documents: List[Document], metadata_file: str):
        metadata = {
            'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'document_count': len(documents),
            'hashes': [doc.metadata.get('hash') for doc in documents if 'hash' in doc.metadata]
        }
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    def _generate_with_ollama(self, prompt: str, model: str, stream=False, **params) -> str:
        """Ollama APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        start_time = time.time()
        default_params = {"temperature": 0.7, "top_p": 0.9, "num_predict": 2048}
        default_params.update(params)  # ì‚¬ìš©ì ì œê³µ ë§¤ê°œë³€ìˆ˜ë¡œ ê¸°ë³¸ê°’ ì—…ë°ì´íŠ¸
        
        # 1. ê°€ëŠ¥í•˜ë©´ ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        if OLLAMA_AVAILABLE:
            try:
                if stream:
                    # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                    print("\nì‘ë‹µ: ", end="")
                    sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                    full_result = ""
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
                    for chunk in ollama.generate(
                        model=model,
                        prompt=prompt,
                        options=default_params,
                        stream=True
                    ):
                        chunk_content = chunk.get("response", "")
                        full_result += chunk_content
                        print(chunk_content, end="")
                        sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                    
                    print()  # ì¤„ë°”ê¿ˆ
                    sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                    elapsed_time = time.time() - start_time
                    print(f"âœ“ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì™„ë£Œ (ollama ë¼ì´ë¸ŒëŸ¬ë¦¬): {model} ({elapsed_time:.2f}ì´ˆ)")
                    return full_result
                else:
                    # ì¼ë°˜ ëª¨ë“œ
                    response = ollama.generate(
                        model=model,
                        prompt=prompt,
                        options=default_params
                    )
                    result = response.get("response", "")
                    elapsed_time = time.time() - start_time
                    print(f"âœ“ ë‹µë³€ ìƒì„± ì™„ë£Œ (ollama ë¼ì´ë¸ŒëŸ¬ë¦¬): {model} ({elapsed_time:.2f}ì´ˆ)")
                    return result
            except Exception as e:
                print(f"âš ï¸ ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ ì‹¤íŒ¨: {e}, REST APIë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        
        # 2. ì‹¤íŒ¨í•˜ë©´ REST API ì‚¬ìš©
        try:
            if stream:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                print("\nì‘ë‹µ: ", end="")
                sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                full_result = ""
                
                response = requests.post(
                    f"{OLLAMA_API_BASE}/generate",
                    json={
                        "model": model,
                        "prompt": prompt,
                        "stream": True,
                        "options": default_params
                    },
                    stream=True
                )
                response.raise_for_status()
                
                for line in response.iter_lines():
                    if line:
                        try:
                            line_text = line.decode('utf-8')
                            json_data = json.loads(line_text)
                            chunk_content = json_data.get("response", "")
                            if chunk_content:
                                full_result += chunk_content
                                print(chunk_content, end="")
                                sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                        except json.JSONDecodeError:
                            continue
                
                print()  # ì¤„ë°”ê¿ˆ
                sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                elapsed_time = time.time() - start_time
                print(f"âœ“ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì™„ë£Œ (REST API): {model} ({elapsed_time:.2f}ì´ˆ)")
                return full_result
            else:
                # ì¼ë°˜ ëª¨ë“œ
                response = requests.post(
                    f"{OLLAMA_API_BASE}/generate",
                    json={
                        "model": model,
                        "prompt": prompt,
                        "stream": False,
                        "options": default_params
                    }
                )
                response.raise_for_status()
                result = response.json().get("response", "")
                elapsed_time = time.time() - start_time
                print(f"âœ“ ë‹µë³€ ìƒì„± ì™„ë£Œ (REST API): {model} ({elapsed_time:.2f}ì´ˆ)")
                return result
        except Exception as e:
            logger.error(f"âŒ Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ (ëª¨ë¸: {model}): {e}")
            print(f"âŒ Ollama API í˜¸ì¶œ ì‹¤íŒ¨ (ëª¨ë¸: {model}): {e}")
            return f"[{model} ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨] ì˜¤ë¥˜: {str(e)}"
    
    def load_or_create_vector_store(self, documents: List[Document], force_update: bool = False) -> bool:
        if not force_update and os.path.exists(self.index_dir) and os.path.exists(self.metadata_file):
            try:
                self.vector_store = FAISS.load_local(self.index_dir, self.embeddings, allow_dangerous_deserialization=True)
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    existing_metadata = json.load(f)
                existing_hashes = set(existing_metadata.get('hashes', []))
                new_docs = [doc for doc in documents if doc.metadata.get('hash') not in existing_hashes]
                if new_docs:
                    self.vector_store.add_documents(new_docs)
                    self.vector_store.save_local(self.index_dir)
                    self._save_document_metadata(documents, self.metadata_file)
            except Exception as e:
                logger.error(f"âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
                return self._create_new_vector_store(documents)
        else:
            return self._create_new_vector_store(documents)
        return True
    
    def _create_new_vector_store(self, documents, hnsw_space='l2'):
        """ë¬¸ì„œ ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
        print(f"\n{'â”€'*60}")
        print(f"ğŸ“Š ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘... ")
        print(f"{'â”€'*60}")
        
        # ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if not documents or len(documents) == 0:
            print("âš ï¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            try:
                # ì„ë² ë”© ì°¨ì› ê²°ì •
                embedding_dim = 768  # ê¸°ë³¸ ì°¨ì›
                try:
                    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ìœ¼ë¡œ ì„ë² ë”© ì°¨ì› í™•ì¸
                    embedding_dim = len(self.embeddings.embed_query("í…ŒìŠ¤íŠ¸"))
                    print(f"âœ“ ì„ë² ë”© ì°¨ì› í™•ì¸ë¨: {embedding_dim}")
                except Exception as e:
                    print(f"âš ï¸ ì„ë² ë”© ì°¨ì› í™•ì¸ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ {embedding_dim} ì‚¬ìš©")
                
                # ë¹ˆ ì¸ë±ìŠ¤ êµ¬ì„± ìš”ì†Œ ìƒì„±
                empty_index, docstore, index_to_docstore_id = create_empty_faiss_index(embedding_dim)
                
                if empty_index is None:
                    print("âŒ ë¹ˆ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
                    return None
                
                # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                from langchain.vectorstores import FAISS
                vector_store = FAISS(
                    embedding_function=self.embeddings,
                    index=empty_index,
                    docstore=docstore,
                    index_to_docstore_id=index_to_docstore_id
                )
                
                print("âœ“ ë¹ˆ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ")
                # ì €ì¥ ì •ë³´ ì„¤ì •
                self.document_count = 0
                
                # ì €ì¥ì†Œ ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„±
                if self.index_dir:
                    if not os.path.exists(self.index_dir):
                        os.makedirs(self.index_dir, exist_ok=True)
                    try:
                        vector_store.save_local(self.index_dir)
                        self._save_document_metadata([], self.metadata_file)
                        print(f"âœ“ ë¹ˆ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì™„ë£Œ: {self.index_dir}")
                    except Exception as save_err:
                        print(f"âš ï¸ ë¹ˆ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {save_err}")
                
                return vector_store
                
            except Exception as e:
                print(f"âŒ ë¹ˆ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {e}")
                return None
        
        print(f"ğŸ”¢ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        
        # ìµœëŒ€ ë¬¸ì„œ ìˆ˜ ì œí•œ (8000ê°œ)
        if len(documents) > 8000:
            print(f"âš ï¸ ë¬¸ì„œê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ ({len(documents)}ê°œ). ì²˜ìŒ 8000ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            documents = documents[:8000]
        
        # ë¬¸ì„œ ì„ë² ë”© ì „ ì•ˆì „ ê²€ì‚¬
        try:
            if not self.embeddings:
                print("âŒ ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
                
            print("ğŸ”„ ë¬¸ì„œ ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
            start_time = time.time()
            
            # ë¨¼ì € ì„ë² ë”© ì°¨ì› í™•ì¸
            embedding_dim = 768  # ê¸°ë³¸ ì°¨ì›
            try:
                embedding_dim = len(self.embeddings.embed_query("í…ŒìŠ¤íŠ¸"))
                print(f"âœ“ ì„ë² ë”© ì°¨ì›: {embedding_dim}")
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© ì°¨ì› í™•ì¸ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ {embedding_dim} ì‚¬ìš©")
            
            # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹œë„
            try:
                vector_store = None
                # HNSW ì¸ë±ìŠ¤ë¡œ ìƒì„± ì‹œë„
                try:
                    if hnsw_space == 'cosine':
                        print("ğŸ”„ HNSW ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
                        vector_store = FAISS.from_documents(
                            documents, 
                            self.embeddings,
                            normalize_L2=True,  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•œ ì •ê·œí™”
                            space='inner_product',  # ë‚´ì  ì‚¬ìš©
                            m=64,  # HNSW ê·¸ë˜í”„ì˜ ì´ì›ƒ ìˆ˜
                            ef_construction=128  # êµ¬ì¶• ì‹œ ê³ ë ¤í•  ì´ì›ƒ ìˆ˜
                        )
                    else:  # 'l2' ë˜ëŠ” ê¸°íƒ€
                        print("ğŸ”„ HNSW L2 ê±°ë¦¬ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
                        vector_store = FAISS.from_documents(
                            documents, 
                            self.embeddings,
                            normalize_L2=False,  # L2 ê±°ë¦¬ëŠ” ì •ê·œí™” í•„ìš” ì—†ìŒ
                            space='l2',  # L2 ê±°ë¦¬ ì‚¬ìš©
                            m=64,
                            ef_construction=128
                        )
                    print("âœ“ HNSW ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
                except Exception as hnsw_error:
                    print(f"âŒ HNSW ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {hnsw_error}")
                    print("âš ï¸ ê¸°ë³¸ ì¸ë±ìŠ¤ë¡œ ì¬ì‹œë„ ì¤‘...")
                    vector_store = None
                
                # ê¸°ë³¸ ë°©ë²•ìœ¼ë¡œ ì¬ì‹œë„
                if vector_store is None:
                    print("ğŸ”„ ê¸°ë³¸ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
                    try:
                        vector_store = FAISS.from_documents(documents, self.embeddings)
                        print("âœ“ ê¸°ë³¸ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
                    except Exception as basic_error:
                        print(f"âŒ ê¸°ë³¸ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {basic_error}")
                        print("âš ï¸ ì•ˆì „ ëª¨ë“œë¡œ ì¬ì‹œë„ ì¤‘...")
                        
                        # ì•ˆì „ ëª¨ë“œë¡œ ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ì§ì ‘ ì„ë² ë”© ìƒì„±)
                        try:
                            # ë¬¸ì„œ ë‚´ìš© ë¦¬ìŠ¤íŠ¸ ìƒì„±
                            texts = [doc.page_content for doc in documents]
                            
                            # ì„ë² ë”© ìƒì„±
                            embeddings_list = self.embeddings.embed_documents(texts)
                            
                            # ë¹ˆ ì¸ë±ìŠ¤ ìƒì„±
                            import numpy as np
                            import faiss
                            import uuid
                            
                            # ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ì¸ë±ìŠ¤ ìƒì„±
                            index = faiss.IndexFlatL2(embedding_dim)
                            if len(embeddings_list) > 0:
                                index.add(np.array(embeddings_list, dtype=np.float32))
                            
                            # ë¬¸ì„œ ì €ì¥ì†Œ ìƒì„±
                            docstore = {}
                            index_to_docstore_id = {}
                            
                            # ë¬¸ì„œì™€ ì¸ë±ìŠ¤ ë§¤í•‘
                            for i, doc in enumerate(documents):
                                id = str(uuid.uuid4())
                                docstore[id] = doc
                                index_to_docstore_id[i] = id
                            
                            # FAISS ê°ì²´ ìƒì„±
                            vector_store = FAISS(
                                embedding_function=self.embeddings,
                                index=index,
                                docstore=docstore,
                                index_to_docstore_id=index_to_docstore_id
                            )
                            print("âœ“ ì•ˆì „ ëª¨ë“œë¡œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
                        except Exception as safe_error:
                            print(f"âŒ ì•ˆì „ ëª¨ë“œë¡œë„ ìƒì„± ì‹¤íŒ¨: {safe_error}")
                            return None
                
                elapsed_time = time.time() - start_time
                print(f"âœ“ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                self.document_count = len(documents)
                
                # ì €ì¥ì†Œ ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„±
                if self.index_dir and vector_store is not None:
                    if not os.path.exists(self.index_dir):
                        os.makedirs(self.index_dir, exist_ok=True)
                    try:
                        vector_store.save_local(self.index_dir)
                        self._save_document_metadata(documents, self.metadata_file)
                        print(f"âœ“ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì™„ë£Œ: {self.index_dir}")
                    except Exception as save_err:
                        print(f"âš ï¸ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {save_err}")
                
                return vector_store
                
            except Exception as e:
                print(f"âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                print(f"âš ï¸ ë‚¨ì€ ë¬¸ì„œ ìˆ˜ë¡œ ì¬ì‹œë„: {len(documents)//2}ê°œ")
                
                if len(documents) > 1:
                    # ë¬¸ì„œ ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì—¬ì„œ ì¬ì‹œë„
                    half_docs = documents[:len(documents)//2]
                    return self._create_new_vector_store(half_docs, hnsw_space)
                else:
                    print("âŒ ì¬ì‹œë„ ì‹¤íŒ¨: ë¬¸ì„œê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤")
                    return None
                    
        except Exception as outer_e:
            print(f"âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {outer_e}")
            return None
    
    def search(self, query: str, top_k: int = 12) -> List[Document]:
        """ì¿¼ë¦¬ì— ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        print("\n" + "â”€"*60)
        print(f"ğŸ“Œ 3ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰ ({self.model_config['name']})")
        print("â”€"*60)
        
        try:
            # ë©€í‹° ì¿¼ë¦¬ í™•ì¥ ìˆ˜í–‰
            logger.info(f"ì¿¼ë¦¬ í™•ì¥ ì‹œì‘: '{query}'")
            expanded_queries = []
            expanded_queries.append(query)  # ì›ë³¸ ì¿¼ë¦¬ë„ í¬í•¨
            
            # QueryExpander ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ í™•ì¥
            logger.info(f"ğŸ” ì¿¼ë¦¬ í™•ì¥ ì¤‘...")
            print("\n" + "â”€"*60)
            print(f"ğŸ“Œ 2ë‹¨ê³„: ì¿¼ë¦¬ í™•ì¥ (í•µì‹¬ ëª¨ë¸)")
            print("â”€"*60)
            print(f"â–¶ ì›ë³¸ ì¿¼ë¦¬: '{query}'")
            
            expanded = self.query_expander.expand_query(query)
            expanded_queries.extend(expanded)
            
            # ì¤‘ë³µ ì œê±° ë° ë¹ˆ ë¬¸ìì—´ ì œê±°
            expanded_queries = [q.strip() for q in expanded_queries if q.strip()]
            expanded_queries = list(dict.fromkeys(expanded_queries))  # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
            
            print("âœ… ì „ì²´ í™•ì¥ ì™„ë£Œ: {}ê°œì˜ ê³ ìœ  ì¿¼ë¦¬ ìƒì„±ë¨".format(len(expanded_queries)))
            print("â”€"*60)
            for i, q in enumerate(expanded_queries, 1):
                print(f"  {i}. {q}")
            
            print("\nğŸ“Š í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì¤‘...")
            
            # ê° í™•ì¥ ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
            all_docs = []
            
            for i, exp_query in enumerate(expanded_queries):
                t_start = time.time()
                if not self.vector_store:
                    logger.error("ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    print("âŒ ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return []
                
                # ê° í™•ì¥ ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰
                docs = self.vector_store.similarity_search_with_score(
                    exp_query, k=top_k
                )
                
                t_end = time.time()
                elapsed = t_end - t_start
                print(f"ğŸ” ì¿¼ë¦¬ #{i+1}: \"{exp_query}\"")
                print(f"  âœ“ {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨ ({elapsed:.2f}ì´ˆ)")
                
                # ê²°ê³¼ ë³‘í•©
                all_docs.extend(docs)
            
            # ìŠ¤ì½”ì–´ ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°
            doc_dict = {}
            for doc, score in all_docs:
                doc_id = doc.metadata.get('chunk_id', doc.page_content[:50])
                if doc_id not in doc_dict or score < doc_dict[doc_id][1]:
                    doc_dict[doc_id] = (doc, score)
            
            # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
            min_score = min([score for _, score in doc_dict.values()], default=0)
            max_score = max([score for _, score in doc_dict.values()], default=1)
            score_range = max_score - min_score if max_score > min_score else 1
            
            # ì •ê·œí™”ëœ ì ìˆ˜ë¡œ ì •ë ¬ (ë‚®ì€ ê±°ë¦¬ = ë†’ì€ ìœ ì‚¬ë„)
            sorted_docs = []
            for doc_id, (doc, score) in doc_dict.items():
                normalized_score = 1 - ((score - min_score) / score_range) if score_range > 0 else 0
                sorted_docs.append((doc, normalized_score))
            
            # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            sorted_docs = sorted(sorted_docs, key=lambda x: x[1], reverse=True)
            
            # ìµœì¢… ê²°ê³¼ ë¬¸ì„œ ì¶”ì¶œ
            final_docs = [doc for doc, _ in sorted_docs[:top_k]]
            
            print(f"\nâœ… ê²€ìƒ‰ ì™„ë£Œ: {len(final_docs)}ê°œ ë¬¸ì„œ ì„ íƒë¨")
            
            print("\nğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ:")
            
            for i, (doc, score) in enumerate(sorted_docs[:top_k], 1):
                print(f"\n{'='*80}")
                source_info = f"ë¬¸ì„œ #{i} | í˜ì´ì§€ {doc.metadata.get('page', 'ë¶ˆëª…')}"
                if 'source' in doc.metadata:
                    source_info += f" | ìœ í˜•: {doc.metadata['source']}"
                print(f"ğŸ“‘ {source_info} | ì •ê·œí™” ì ìˆ˜: {score:.4f}")
                print(f"{'â”€'*80}")
                
                # ê²€ìƒ‰ì–´ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬
                content = doc.page_content
                for search_term in expanded_queries:
                    pattern = re.compile(re.escape(search_term), re.IGNORECASE)
                    content = pattern.sub(f"\033[93m{search_term}\033[0m", content)
                print(content)
            
            print(f"\n{'='*80}")
            return final_docs
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def format_context_for_model(self, docs: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ëª¨ë¸ì— ì „ë‹¬í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        formatted_docs = []
        for doc in docs:
            page_info = f"[í˜ì´ì§€ {doc.metadata.get('page', 'ë¶ˆëª…')}]"
            formatted_docs.append(f"{page_info} {doc.page_content}")
        return "\n\n".join(formatted_docs)
    
    def answer(self, query: str, model: str, context: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        logger.debug(f"ë‹µë³€ ìƒì„± ìš”ì²­ - ì¿¼ë¦¬: '{query}', ëª¨ë¸: {model}")
        
        # 1. ìºì‹œì—ì„œ ì‘ë‹µ í™•ì¸
        cache_key = f"{model}:{hashlib.md5((query + context[:100]).encode()).hexdigest()}"
        cached_answer = self._load_cache().get(cache_key)
        
        if cached_answer and not os.environ.get('DISABLE_CACHE'):
            print(f"ğŸ’¾ ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©: {model}")
            return {"answer": cached_answer, "model": model, "cached": True}
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        try:
            # ì˜¤ëŠ˜ ë‚ ì§œ ì •ë³´ ì¶”ê°€
            today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
            
            prompt_template = f"""ë‹¹ì‹ ì€ í•œí™”ì†í•´ë³´í—˜ì˜ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì˜¤ëŠ˜ì€ {today}ì…ë‹ˆë‹¤.

[ì •ë³´]
{context}

[ì§ˆë¬¸]
{query}

[ë‹µë³€]"""
            
            # 3. Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
            try:
                # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
                result = self._generate_with_ollama(prompt_template, model, stream=True)
                
                # ì‘ë‹µ ìºì‹±
                answer_content = result if isinstance(result, str) else result.get('answer', '')
                self._cache[cache_key] = answer_content
                self._save_cache()
                
                # ê²°ê³¼ ë°˜í™˜
                return {
                    "answer": answer_content,
                    "model": model,
                    "cached": False
                }
                
            except Exception as e:
                logger.error(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                return {"answer": f"ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "model": model, "error": True}
                
        except Exception as e:
            logger.error(f"âŒ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {"answer": f"ë‹µë³€ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "model": model, "error": True}

def main():
    # global ì„ ì–¸ì„ í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ìœ¼ë¡œ ì´ë™
    global AVAILABLE_MODELS
    
    print("\n" + "="*60)
    print("ğŸ“Š í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ RAG ì‹œìŠ¤í…œ")
    print("(Ollama ëª¨ë¸ ë¹„êµ: Llama3.1 vs Gemma3)")
    print("="*60)
    print("ğŸ”„ Ollama ì„œë²„ ì—°ê²° í™•ì¸ ì¤‘...")
    
    # ì˜¬ë¼ë§ˆ ëª¨ë¸ í™•ì¸ ë° ì„¤ì •
    available_models = check_ollama_models()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ AVAILABLE_MODELS ì—…ë°ì´íŠ¸
    if available_models:
        AVAILABLE_MODELS = available_models
    
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    args = parser.parse_args()
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” - í•œ ë²ˆë§Œ ìƒì„±
    print(f"\nğŸ’¾ ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì¤‘ (ì„ë² ë”© ëª¨ë¸: BGE-M3)...")
    sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
    
    rag = RAGSystem(
        use_hnsw=True,
        ef_search=200,
        ef_construction=200,
        m=64
    )
    
    # PDF íŒŒì¼ ì²˜ë¦¬
    sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
    print("\nğŸ” PDF ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘...")
    sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
    
    pdf_path = args.pdf
    if not os.path.exists(pdf_path):
        print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return
    
    pdf_processor = PDFProcessor(pdf_path)
    documents = pdf_processor.process()
    
    # ë¬¸ì„œ ë¶„í• 
    splitter = DocumentSplitter(chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap)
    chunks = splitter.split_documents(documents)
    
    # ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ë¡œë“œ
    success = rag.load_or_create_vector_store(chunks, force_update=args.force_update)
    
    if not success:
        print("âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
        return
    
    # ìë™ í‰ê°€ ì„¤ì •
    auto_evaluator = None
    if args.auto_eval:
        try:
            auto_evaluator = AutoEvaluator()
        except Exception as e:
            print(f"âš ï¸ ìë™ í‰ê°€ ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("âš ï¸ ìë™ í‰ê°€ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    
    # ëª¨ë¸ í‰ê°€ì ì´ˆê¸°í™”
    evaluator = ModelEvaluator()
    
    # ìƒí˜¸ì‘ìš© ëª¨ë“œ ë˜ëŠ” ìë™ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰
    print("\n" + "="*80)
    print("ğŸš€ RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"  - ì„ë² ë”© ëª¨ë¸: {rag.model_config['name']}")
    print(f"  - ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬: {rag.index_dir}")
    
    if args.auto_eval and auto_evaluator:
        print(f"  - ìë™ í‰ê°€: í™œì„±í™” (gemma3:12b)")
    else:
        print(f"  - ìë™ í‰ê°€: ë¹„í™œì„±í™”")
    
    # ìë™ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if args.auto_test:
        print("\nğŸ§ª ìë™ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œì‘...")
        auto_question_generator = AutoQuestionGenerator()
        auto_test_manager = AutoTestManager(rag, auto_question_generator, auto_evaluator, evaluator, AVAILABLE_MODELS)
        auto_test_manager.run_auto_test(num_questions=args.num_questions, top_k=args.top_k)
        return
        
    # ìƒí˜¸ì‘ìš© ëª¨ë“œ ì•ˆë‚´
    print("\nğŸ’¬ ëª…ë ¹ì–´ ëª©ë¡:")
    print("  - 'ìë™ í…ŒìŠ¤íŠ¸' ë˜ëŠ” 'auto': ìë™ ì§ˆë¬¸ ìƒì„± ë° í‰ê°€ ëª¨ë“œ ì‹¤í–‰")
    print("  - 'ì¢…ë£Œ' ë˜ëŠ” 'quit': í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("  - ê·¸ ì™¸: ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬")
    
    print("\nì§ˆë¬¸ì´ë‚˜ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    print("="*80)
    
    while True:
        print("\nğŸ’¡ í‘œì¤€ ì…ë ¥ìœ¼ë¡œë¶€í„° ì¿¼ë¦¬ë¥¼ ì½ëŠ” ì¤‘...")
        sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
        
        try:
            query = input().strip()
            print(f"\nğŸ’¬ ì…ë ¥: ğŸ’¬ ì¿¼ë¦¬ ì…ë ¥ ì™„ë£Œ: '{query}'")
            sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
            
            # ì¢…ë£Œ ëª…ë ¹
            if query.lower() in ('ì¢…ë£Œ', 'quit', 'exit'):
                print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
            # ìë™ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            if query.lower() in ('ìë™ í…ŒìŠ¤íŠ¸', 'auto', 'auto test'):
                print("\nğŸ§ª ìë™ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œì‘...")
                sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
                
                try:
                    num_questions = int(input("ìƒì„±í•  ì§ˆë¬¸ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”(1-10): ").strip())
                    num_questions = max(1, min(10, num_questions))
                except (ValueError, EOFError):
                    print("âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥, ê¸°ë³¸ê°’ 5ê°œ ì§ˆë¬¸ìœ¼ë¡œ ì„¤ì •")
                    num_questions = 5
                
                print(f"âœ“ {num_questions}ê°œ ì§ˆë¬¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸ ì‹œì‘")
                sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
                
                auto_question_generator = AutoQuestionGenerator()
                auto_test_manager = AutoTestManager(rag, auto_question_generator, auto_evaluator, evaluator, AVAILABLE_MODELS)
                auto_test_manager.run_auto_test(num_questions=num_questions, top_k=args.top_k)
                continue
            
            # ë¹ˆ ì…ë ¥ ë¬´ì‹œ
            if not query:
                continue
                
            # PDF ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
            print(f"\nğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            sys.stdout.flush()  # ë²„í¼ ë¹„ìš°ê¸°
            
            docs = rag.search(query, top_k=args.top_k)
            print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œì˜ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if not docs:
                print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            context = rag.format_context_for_model(docs)
            print("\nğŸ’¡ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
            
            results = {}
            auto_evaluations = {}
            
            for i, model in enumerate(AVAILABLE_MODELS):
                print(f"\nğŸ“Œ [{i+1}/{len(AVAILABLE_MODELS)}] {model} ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
                result = rag.answer(query, model, context)
                answer = result["answer"]
                results[model] = answer
                print(f"âœ… {model} ëª¨ë¸ ë‹µë³€ ìƒì„± ì™„ë£Œ.")
                
                # ìë™ í‰ê°€ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
                if args.auto_eval:
                    print(f"\nğŸ“Š {model} ëª¨ë¸ ë‹µë³€ ìë™ í‰ê°€ ì¤‘...")
                    evaluation = auto_evaluator.evaluate_answer(query, context, answer)
                    auto_evaluations[model] = evaluation
                    print(f"âœ… {model} ëª¨ë¸ ìë™ í‰ê°€ ì™„ë£Œ.")
            
            print("\nğŸ’¡ ëª¨ë“  ëª¨ë¸ ì²˜ë¦¬ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ ì¤‘...")
            # ê²°ê³¼ ì €ì¥ ë° í‰ê°€ ID ìƒì„±
            evaluation_id = evaluator.save_evaluation(
                query=query,
                context=context,
                results=results,
                metadata={
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "auto_evaluations": auto_evaluations if args.auto_eval else None
                }
            )
            print(f"âœ… í‰ê°€ ID '{evaluation_id}' ìƒì„± ì™„ë£Œ.")
            
            print("\n" + "="*80)
            print(f"ğŸ’¬ ì§ˆë¬¸: {query}")
            print(f"ğŸ“ í‰ê°€ ID: {evaluation_id}")
            print("="*80)
            
            for model, answer in results.items():
                print(f"\n{'â”€'*80}")
                print(f"ğŸ“ ëª¨ë¸: {model}")
                if args.auto_eval and model in auto_evaluations:
                    eval_data = auto_evaluations[model]
                    score = eval_data.get("score")
                    score_text = f"ì ìˆ˜: {score}/5" if score is not None else "ì ìˆ˜: í‰ê°€ ë¶ˆê°€"
                    print(f"ğŸ¤– ìë™ í‰ê°€: {score_text}")
                print(f"{'â”€'*80}")
                print(answer)
                
                # ìë™ í‰ê°€ ì„¸ë¶€ ê²°ê³¼ í‘œì‹œ
                if args.auto_eval and model in auto_evaluations:
                    eval_data = auto_evaluations[model]
                    print(f"\nğŸ“Š ìë™ í‰ê°€ ì„¸ë¶€ ê²°ê³¼:")
                    print(f"{'â”€'*40}")
                    print(f"í‰ê°€ ì´ìœ : {eval_data.get('reason', 'í‰ê°€ ì´ìœ  ì¶”ì¶œ ì‹¤íŒ¨')}")
            
            # ìë™ í‰ê°€ ê²°ê³¼ë¥¼ ModelEvaluatorì— ì €ì¥ (ìˆ˜ë™ í‰ê°€ ì…ë ¥ ì œê±°)
            if args.auto_eval:
                print("\nğŸ’¡ ìë™ í‰ê°€ ê²°ê³¼ ì €ì¥ ì¤‘...")
                for model, eval_data in auto_evaluations.items():
                    score = eval_data.get("score")
                    if score is not None:
                        evaluator.add_evaluation_score(
                            evaluation_id=evaluation_id,
                            model=model,
                            score=score,
                            comments=eval_data.get("reason", "")[:200]  # ì´ìœ  ìš”ì•½
                        )
                print("âœ… ìë™ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ.")
            
            # ë©€í‹°ì¿¼ë¦¬ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ streamlitì—ì„œ ì¡°íšŒ ê°€ëŠ¥í•˜ê²Œ í•¨
            print("\nğŸ’¡ ì¿¼ë¦¬ ê²°ê³¼ JSON íŒŒì¼ ì €ì¥ ì¤‘...")
            query_results_file = os.path.join("query_results", f"query_{evaluation_id}.json")
            os.makedirs("query_results", exist_ok=True)
            
            # ê²°ê³¼ ì €ì¥
            query_result_data = {
                "query": query,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "evaluation_id": evaluation_id,
                "results": results,
                "auto_evaluations": auto_evaluations if args.auto_eval else None,
                "context": context
            }
            
            with open(query_results_file, 'w', encoding='utf-8') as f:
                json.dump(query_result_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nâœ… ì¿¼ë¦¬ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {query_results_file}")
            print(f"Streamlitì—ì„œ ì´ ê²°ê³¼ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
        except EOFError:
            print("\nğŸ‘‹ EOF ê°ì§€, í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
            break
        except Exception as e:
            logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(traceback.format_exc())
            print(f"âŒ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ì„¸ë¶€ ì˜¤ë¥˜ ë‚´ìš©:", traceback.format_exc())

def streamlit_main():
    import streamlit as st
    
    # Streamlit í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ RAG ì‹œìŠ¤í…œ",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    # ì œëª© ë° ì„¤ëª…
    st.title("ğŸ“Š í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ RAG ì‹œìŠ¤í…œ")
    st.subheader("(Ollama ëª¨ë¸ ë¹„êµ: Llama3.1 vs Gemma3)")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("âš™ï¸ í™˜ê²½ ì„¤ì •")
    
    # PDF íŒŒì¼ ì„ íƒ
    pdf_files = glob.glob(os.path.join(SCRIPT_DIR, "*.pdf"))
    if not pdf_files:
        st.sidebar.warning("PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        selected_pdf = PDF_PATH  # ê¸°ë³¸ PDF ê²½ë¡œ ì‚¬ìš©
    else:
        selected_pdf = st.sidebar.selectbox(
            "PDF íŒŒì¼ ì„ íƒ",
            options=pdf_files,
            index=0
        )
    
    # ì²­í¬ í¬ê¸° ë° ê²¹ì¹¨ ì„¤ì •
    chunk_size = st.sidebar.slider("ì²­í¬ í¬ê¸°", min_value=100, max_value=1000, value=500, step=50)
    chunk_overlap = st.sidebar.slider("ì²­í¬ ê²¹ì¹¨", min_value=50, max_value=300, value=150, step=25)
    
    # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì„¤ì •
    top_k = st.sidebar.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (Top-K)", min_value=3, max_value=20, value=10)
    
    # ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸ ì˜µì…˜
    force_update = st.sidebar.checkbox("ë²¡í„° ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸", value=False)
    
    # HNSW ì¸ë±ìŠ¤ ì‚¬ìš© ì˜µì…˜
    use_hnsw = st.sidebar.checkbox("HNSW ì¸ë±ìŠ¤ ì‚¬ìš© (ì •í™•ë„ í–¥ìƒ)", value=True)
    
    # ìë™ í‰ê°€ ì˜µì…˜
    auto_eval = st.sidebar.checkbox("ìë™ í‰ê°€ í™œì„±í™” (gemma3:12b í•„ìš”)", value=True)
    
    # íƒ­ ì„¤ì •
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ ì§ˆë¬¸ ì‘ë‹µ", "ğŸ”„ ìë™ í…ŒìŠ¤íŠ¸", "ğŸ” ë””ë²„ê·¸"])
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    logger = setup_logging()
    evaluator = ModelEvaluator()
    auto_evaluator = AutoEvaluator(model_name="gemma3:12b")
    auto_question_generator = AutoQuestionGenerator(model_name="gemma3:12b")
    
    # Ollama ì„œë²„ ì—°ê²° ë° ëª¨ë¸ í™•ì¸
    print("ğŸ”„ Ollama ëª¨ë¸ í™•ì¸ ì¤‘...")
    available_models = check_ollama_models()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ìŠ¤íŠ¸ë¦¼ë¦¿ì— í‘œì‹œ
    if available_models:
        st.sidebar.success(f"âœ“ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(available_models)}")
    else:
        st.sidebar.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ì§ˆë¬¸ ì‘ë‹µ íƒ­
    with tab1:
        st.header("ì§ˆë¬¸ ì‘ë‹µ")
        
        # ì €ì¥ëœ ì¿¼ë¦¬ ê²°ê³¼ í™•ì¸
        query_results_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "query_results")
        os.makedirs(query_results_dir, exist_ok=True)
        query_results_files = glob.glob(os.path.join(query_results_dir, "query_*.json"))
        query_results_files = sorted(query_results_files, key=os.path.getmtime, reverse=True)
        
        if query_results_files:
            st.subheader("ğŸ“ ì €ì¥ëœ ì¿¼ë¦¬ ê²°ê³¼")
            selected_result_file = st.selectbox(
                "ì´ì „ ì¿¼ë¦¬ ê²°ê³¼ ì„ íƒ",
                options=["ìƒˆ ì¿¼ë¦¬ ì…ë ¥"] + query_results_files,
                format_func=lambda x: f"ìƒˆ ì¿¼ë¦¬ ì…ë ¥" if x == "ìƒˆ ì¿¼ë¦¬ ì…ë ¥" else f"{os.path.basename(x)} - {time.ctime(os.path.getmtime(x))}"
            )
            
            # ì´ì „ ì¿¼ë¦¬ ê²°ê³¼ ë¡œë“œ
            if selected_result_file != "ìƒˆ ì¿¼ë¦¬ ì…ë ¥":
                try:
                    with open(selected_result_file, 'r', encoding='utf-8') as f:
                        query_data = json.load(f)
                    
                    # ë°ì´í„° í‘œì‹œ
                    st.subheader(f"ì§ˆë¬¸: {query_data['query']}")
                    st.caption(f"ìƒì„± ì‹œê°„: {query_data['timestamp']}")
                    
                    # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ
                    if 'context' in query_data:
                        with st.expander("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸", expanded=False):
                            st.markdown(query_data['context'])
                    
                    # ê° ëª¨ë¸ì˜ ì‘ë‹µ í‘œì‹œ
                    if 'results' in query_data:
                        for model, answer in query_data['results'].items():
                            with st.expander(f"ğŸ“ ëª¨ë¸: {model}", expanded=True):
                                st.markdown(answer)
                                
                                # ìë™ í‰ê°€ ê²°ê³¼ í‘œì‹œ
                                if query_data.get('auto_evaluations') and model in query_data['auto_evaluations']:
                                    eval_data = query_data['auto_evaluations'][model]
                                    score = eval_data.get('score')
                                    reason = eval_data.get('reason', 'í‰ê°€ ì •ë³´ ì—†ìŒ')
                                    
                                    st.divider()
                                    st.markdown("**ğŸ¤– ìë™ í‰ê°€ ê²°ê³¼:**")
                                    if score is not None:
                                        st.markdown(f"**ì ìˆ˜**: {score}/5")
                                    st.markdown(f"**í‰ê°€ ì´ìœ **: {reason}")
                except Exception as e:
                    st.error(f"ì¿¼ë¦¬ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ìƒˆ ì¿¼ë¦¬ ì…ë ¥
        if not query_results_files or selected_result_file == "ìƒˆ ì¿¼ë¦¬ ì…ë ¥":
            # ì§ˆë¬¸ ì…ë ¥
            if 'last_question' not in st.session_state:
                st.session_state.last_question = ""
                
            question = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", height=100, value=st.session_state.last_question)
            
            if st.button("ì§ˆë¬¸ ì œì¶œ", type="primary", disabled=(not selected_pdf)):
                if question:
                    st.session_state.last_question = question
                    
                    with st.spinner("ì§ˆë¬¸ì— ë‹µë³€ ìƒì„± ì¤‘..."):
                        try:
                            # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
                            rag = RAGSystem(
                                use_hnsw=use_hnsw,
                                ef_search=200,
                                ef_construction=200,
                                m=64
                            )
                            
                            # PDF ì²˜ë¦¬
                            processor = PDFProcessor(selected_pdf)
                            documents = processor.process()
                            
                            # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆê³  PDFê°€ ë³€ê²½ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¸ë±ìŠ¤ ë¡œë“œë§Œ ì‹¤í–‰
                            if not documents and not force_update:
                                success = rag.load_or_create_vector_store([], force_update=False)
                                if not success:
                                    st.error("âŒ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨")
                                    return
                            else:
                                # ìƒˆ ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° ë¶„í•  í›„ ì¸ë±ì‹±
                                splitter = DocumentSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                                chunks = splitter.split_documents(documents)
                                
                                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                                import gc
                                gc.collect()
                                time.sleep(1)
                                
                                # ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
                                success = rag.load_or_create_vector_store(chunks, force_update=force_update)
                                if not success:
                                    st.error("âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                                    return
                            
                            # ì¿¼ë¦¬ ì²˜ë¦¬
                            retrieved_docs = rag.search(question, top_k=top_k)
                            context = rag.format_context_for_model(retrieved_docs)
                            
                            # ê²°ê³¼ ë³€ìˆ˜
                            results = {}
                            auto_evaluations = {}
                            evaluation_id = None
                            
                            # ëª¨ë¸ í‰ê°€ ìƒì„±
                            if auto_eval:
                                evaluation_id = evaluator.save_evaluation(question, context, {}, {
                                    "top_k": top_k,
                                    "has_auto_eval": True
                                })
                            
                            # ê° ëª¨ë¸ì— ëŒ€í•´ ë‹µë³€ ìƒì„±
                            progress_bar = st.progress(0)
                            for i, model in enumerate(AVAILABLE_MODELS):
                                progress = (i / len(AVAILABLE_MODELS)) * 100
                                progress_bar.progress(int(progress))
                                
                                # ëª¨ë¸ ì‘ë‹µ ìƒì„±
                                result = rag.answer(question, model, context)
                                answer = result["answer"]
                                results[model] = answer
                                
                                # ìë™ í‰ê°€ ì‹¤í–‰
                                if auto_eval:
                                    auto_evaluation = auto_evaluator.evaluate_answer(question, context, answer)
                                    auto_evaluations[model] = auto_evaluation
                                    
                                    # í‰ê°€ ê²°ê³¼ ì €ì¥
                                    if evaluation_id:
                                        score = auto_evaluation.get("score", 0)
                                        comments = auto_evaluation.get("reason", "")
                                        evaluator.add_evaluation_score(evaluation_id, model, score, comments)
                            
                            progress_bar.progress(100)
                            
                            # ê²°ê³¼ ì €ì¥
                            timestamp = time.strftime("%Y%m%d_%H%M%S")
                            query_id = re.sub(r'\W+', '_', question)[:30]
                            query_results_file = os.path.join(query_results_dir, f"query_{timestamp}_{query_id}.json")
                            
                            query_result_data = {
                                "query": question,
                                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                                "evaluation_id": evaluation_id,
                                "results": results,
                                "auto_evaluations": auto_evaluations if auto_eval else None,
                                "context": context
                            }
                            
                            with open(query_results_file, 'w', encoding='utf-8') as f:
                                json.dump(query_result_data, f, ensure_ascii=False, indent=2)
                            
                            # ê²°ê³¼ í‘œì‹œ
                            st.success("âœ… ì‘ë‹µì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤")
                            
                            # ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ
                            with st.expander("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸", expanded=False):
                                st.markdown(context)
                            
                            # ê° ëª¨ë¸ì˜ ì‘ë‹µ í‘œì‹œ
                            for model, answer in results.items():
                                with st.expander(f"ğŸ“ ëª¨ë¸: {model}", expanded=True):
                                    st.markdown(answer)
                                    
                                    # ìë™ í‰ê°€ ê²°ê³¼ í‘œì‹œ
                                    if auto_eval and model in auto_evaluations:
                                        eval_data = auto_evaluations[model]
                                        score = eval_data.get("score")
                                        reason = eval_data.get("reason", "í‰ê°€ ì •ë³´ ì—†ìŒ")
                                        
                                        st.divider()
                                        st.markdown("**ğŸ¤– ìë™ í‰ê°€ ê²°ê³¼:**")
                                        if score is not None:
                                            st.markdown(f"**ì ìˆ˜**: {score}/5")
                                        st.markdown(f"**í‰ê°€ ì´ìœ **: {reason}")
                            
                        except Exception as e:
                            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
                            logger.error(traceback.format_exc())
    
    # ìë™ í…ŒìŠ¤íŠ¸ íƒ­
    with tab2:
        st.header("ìë™ í…ŒìŠ¤íŠ¸")
        st.write("RAG ì‹œìŠ¤í…œì˜ ìë™ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        
        # ìë™ í…ŒìŠ¤íŠ¸ ì„¤ì •
        num_questions = st.slider("ìƒì„±í•  ì§ˆë¬¸ ìˆ˜", min_value=1, max_value=20, value=5)
        
        if st.button("ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", type="primary"):
            with st.spinner("ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘..."):
                try:
                    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
                    rag = RAGSystem(
                        use_hnsw=use_hnsw,
                        ef_search=200,
                        ef_construction=200,
                        m=64
                    )
                    
                    # PDF ì²˜ë¦¬
                    processor = PDFProcessor(selected_pdf)
                    documents = processor.process()
                    
                    # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆê³  PDFê°€ ë³€ê²½ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¸ë±ìŠ¤ ë¡œë“œë§Œ ì‹¤í–‰
                    if not documents and not force_update:
                        success = rag.load_or_create_vector_store([], force_update=False)
                        if not success:
                            st.error("âŒ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨")
                            return
                    else:
                        # ìƒˆ ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° ë¶„í•  í›„ ì¸ë±ì‹±
                        splitter = DocumentSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                        chunks = splitter.split_documents(documents)
                        
                        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                        import gc
                        gc.collect()
                        time.sleep(1)
                        
                        # ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸
                        success = rag.load_or_create_vector_store(chunks, force_update=force_update)
                        if not success:
                            st.error("âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                            return
                    
                    # ìë™ í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™”
                    auto_test_manager = AutoTestManager(
                        rag_system=rag,
                        auto_question_generator=auto_question_generator,
                        auto_evaluator=auto_evaluator,
                        evaluator=evaluator,
                        available_models=available_models
                    )
                    
                    # ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                    st.session_state.auto_test_results = []
                    
                    # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì¶”ê°€
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # ì„ì˜ì˜ ë¬¸ì„œ ì¶”ì¶œ
                    random_doc_indices = random.sample(range(len(chunks)), min(5, len(chunks)))
                    random_docs = [chunks[i] for i in random_doc_indices]
                    
                    # ìƒì„±í•  ì§ˆë¬¸ ìˆ˜ ì¡°ì •
                    status_text.text("ë¬¸ì„œì—ì„œ ì§ˆë¬¸ ìƒì„± ì¤‘...")
                    for i, doc in enumerate(random_docs):
                        # ë¬¸ì„œì—ì„œ ì§ˆë¬¸ ìƒì„±
                        context = doc.page_content
                        doc_questions = auto_question_generator.generate_questions(context)
                        
                        # ì§ˆë¬¸ë§ˆë‹¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                        for j, question in enumerate(doc_questions[:max(1, num_questions // len(random_docs))]):
                            current_progress = (i * max(1, num_questions // len(random_docs)) + j) / num_questions * 100
                            progress_bar.progress(int(current_progress))
                            status_text.text(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ({i * max(1, num_questions // len(random_docs)) + j + 1}/{num_questions}): {question}")
                            
                            # ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ìˆ˜í–‰
                            retrieved_docs = rag.search(question, top_k=top_k)
                            context = rag.format_context_for_model(retrieved_docs)
                            
                            # ëª¨ë¸ë³„ ë‹µë³€ ìƒì„±
                            results = {}
                            auto_evaluations = {}
                            
                            for model in available_models:
                                # ëª¨ë¸ ì‘ë‹µ ìƒì„±
                                result = rag.answer(question, model, context)
                                answer = result["answer"]
                                results[model] = answer
                                
                                # ìë™ í‰ê°€ ì‹¤í–‰
                                auto_evaluation = auto_evaluator.evaluate_answer(question, context, answer)
                                auto_evaluations[model] = auto_evaluation
                            
                            # ê²°ê³¼ ì €ì¥
                            test_result = {
                                "question": question,
                                "context": context,
                                "results": results,
                                "auto_evaluations": auto_evaluations
                            }
                            st.session_state.auto_test_results.append(test_result)
                    
                    progress_bar.progress(100)
                    status_text.text("ìë™ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.success(f"âœ… {len(st.session_state.auto_test_results)}ê°œì˜ ì§ˆë¬¸ì— ëŒ€í•œ ìë™ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
                    
                    # ì„±ëŠ¥ ìš”ì•½
                    st.subheader("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
                    model_scores = {model: [] for model in available_models}
                    
                    for result in st.session_state.auto_test_results:
                        for model, eval_data in result["auto_evaluations"].items():
                            if "score" in eval_data:
                                model_scores[model].append(eval_data["score"])
                    
                    # í‰ê·  ì ìˆ˜ ê³„ì‚° ë° í‘œì‹œ
                    summary_data = []
                    for model, scores in model_scores.items():
                        if scores:
                            avg_score = sum(scores) / len(scores)
                            summary_data.append({
                                "ëª¨ë¸": model,
                                "í‰ê·  ì ìˆ˜": f"{avg_score:.2f}",
                                "í…ŒìŠ¤íŠ¸ ìˆ˜": len(scores)
                            })
                    
                    if summary_data:
                        st.table(summary_data)
                    
                    # ê°œë³„ ê²°ê³¼ í‘œì‹œ
                    st.subheader("ğŸ” ê°œë³„ ì§ˆë¬¸ ê²°ê³¼")
                    for i, result in enumerate(st.session_state.auto_test_results):
                        with st.expander(f"ì§ˆë¬¸ {i+1}: {result['question']}", expanded=i==0):
                            # ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ
                            with st.expander("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸", expanded=False):
                                st.markdown(result["context"])
                            
                            # ê° ëª¨ë¸ì˜ ì‘ë‹µ í‘œì‹œ
                            for model, answer in result["results"].items():
                                with st.expander(f"ğŸ“ ëª¨ë¸: {model}", expanded=True):
                                    st.markdown(answer)
                                    
                                    # ìë™ í‰ê°€ ê²°ê³¼ í‘œì‹œ
                                    if model in result["auto_evaluations"]:
                                        eval_data = result["auto_evaluations"][model]
                                        score = eval_data.get("score")
                                        reason = eval_data.get("reason", "í‰ê°€ ì •ë³´ ì—†ìŒ")
                                        
                                        st.divider()
                                        st.markdown("**ğŸ¤– ìë™ í‰ê°€ ê²°ê³¼:**")
                                        if score is not None:
                                            st.markdown(f"**ì ìˆ˜**: {score}/5")
                                        st.markdown(f"**í‰ê°€ ì´ìœ **: {reason}")
                    
                except Exception as e:
                    st.error(f"âŒ ìë™ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
                    logger.error(traceback.format_exc())
    
    # ë””ë²„ê·¸ íƒ­
    with tab3:
        st.header("ë””ë²„ê·¸ ì •ë³´")
        
        # ì‹œìŠ¤í…œ ì •ë³´
        st.subheader("ì‹œìŠ¤í…œ ì •ë³´")
        st.json({
            "Python ë²„ì „": sys.version,
            "ì‹¤í–‰ ê²½ë¡œ": sys.executable,
            "ìš´ì˜ì²´ì œ": platform.platform(),
            "ë©”ëª¨ë¦¬ ì •ë³´": psutil.virtual_memory()._asdict() if "psutil" in sys.modules else "psutil ëª¨ë“ˆ ì—†ìŒ"
        })
        
        # Ollama ëª¨ë¸ ì •ë³´
        st.subheader("Ollama ëª¨ë¸ ì •ë³´")
        try:
            response = requests.get(f"{OLLAMA_API_BASE}/tags")
            response.raise_for_status()
            models = response.json().get("models", [])
            st.table([{
                "ëª¨ë¸ëª…": model.get("name"),
                "í¬ê¸°": f"{model.get('size') / 1024 / 1024 / 1024:.2f} GB" if model.get('size') else "ì•Œ ìˆ˜ ì—†ìŒ",
                "ìˆ˜ì •ì¼": datetime.fromtimestamp(model.get('modified_at', 0)).strftime('%Y-%m-%d %H:%M:%S') if model.get('modified_at') else "ì•Œ ìˆ˜ ì—†ìŒ"
            } for model in models])
        except Exception as e:
            st.error(f"Ollama ì„œë²„ ì—°ê²° ì˜¤ë¥˜: {e}")
        
        # í‰ê°€ í†µê³„
        st.subheader("ëª¨ë¸ í‰ê°€ í†µê³„")
        summary = evaluator.get_evaluation_summary()
        if summary:
            summary_data = []
            for model, stats in summary.items():
                summary_data.append({
                    "ëª¨ë¸": model,
                    "í‰ê°€ íšŸìˆ˜": stats.get('total_evaluations', 0),
                    "í‰ê·  ì ìˆ˜": f"{stats.get('avg_score', 0):.2f}",
                    "í‰ê·  ì†ë„": f"{stats.get('avg_speed', 0):.2f} í† í°/ì´ˆ" if stats.get('avg_speed') else "ì•Œ ìˆ˜ ì—†ìŒ"
                })
            st.table(summary_data)
        else:
            st.info("ì•„ì§ í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def create_empty_faiss_index(dimension=768):
    """ë¹ˆ FAISS ì¸ë±ìŠ¤ ìƒì„± ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"""
    try:
        import faiss
        import numpy as np
        from langchain.docstore.in_memory import InMemoryDocstore
        
        # ë¹ˆ FAISS ì¸ë±ìŠ¤ ìƒì„±
        empty_index = faiss.IndexFlatL2(dimension)
        
        # ë¹ˆ ë¬¸ì„œ ì €ì¥ì†Œ ìƒì„±
        docstore = InMemoryDocstore({})
        
        # ë¹ˆ ë§¤í•‘ ìƒì„±
        index_to_docstore_id = {}
        
        return empty_index, docstore, index_to_docstore_id
        
    except Exception as e:
        print(f"âŒ ë¹ˆ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        # ìµœì†Œí•œì˜ ê¸°ë³¸ ê°ì²´ ë°˜í™˜
        return None, None, None

# ëª¨ë¸ í‰ê°€ ëª¨ë“ˆ
class ModelEvaluator:
    def __init__(self, evaluation_file: str = None):
        if evaluation_file is None:
            evaluation_file = os.path.join(SCRIPT_DIR, "model_evaluations.json")
        self.evaluation_file = evaluation_file
        self.evaluations = self._load_evaluations()
    
    def _load_evaluations(self) -> Dict:
        """ê¸°ì¡´ í‰ê°€ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            if os.path.exists(self.evaluation_file):
                with open(self.evaluation_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {"evaluations": [], "statistics": {}}
        except Exception as e:
            logger.error(f"í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"evaluations": [], "statistics": {}}
    
    def save_evaluation(self, query: str, context: str, results: Dict[str, str], 
                       metadata: Dict = None) -> str:
        """ëª¨ë¸ ì‘ë‹µì„ ì €ì¥í•˜ê³  í‰ê°€ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            evaluation_id = hashlib.md5(f"{query}_{time.time()}".encode()).hexdigest()[:8]
            
            evaluation_entry = {
                "id": evaluation_id,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "query": query,
                "context_summary": context[:200] + "..." if len(context) > 200 else context,
                "results": results,
                "metadata": metadata or {},
                "evaluations": {model: {
                    "score": None,  # 1-5: ì „ë°˜ì ì¸ ì‘ë‹µ í’ˆì§ˆ
                    "comments": ""  # í‰ê°€ ì½”ë©˜íŠ¸
                } for model in results.keys()}
            }
            
            self.evaluations["evaluations"].append(evaluation_entry)
            self._save_evaluations()
            
            return evaluation_id
        except Exception as e:
            logger.error(f"í‰ê°€ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def add_evaluation_score(self, evaluation_id: str, model: str, 
                           score: int, comments: str = "") -> bool:
        """íŠ¹ì • í‰ê°€ í•­ëª©ì— ì ìˆ˜ì™€ ì½”ë©˜íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            for entry in self.evaluations["evaluations"]:
                if entry["id"] == evaluation_id:
                    if model in entry["evaluations"]:
                        entry["evaluations"][model]["score"] = score
                        entry["evaluations"][model]["comments"] = comments
                        self._save_evaluations()
                        return True
            return False
        except Exception as e:
            logger.error(f"í‰ê°€ ì ìˆ˜ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def get_evaluation_summary(self) -> Dict:
        """ëª¨ë“  í‰ê°€ì˜ í†µê³„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ëª¨ë“  ëª¨ë¸ì˜ í‰ê°€ ë°ì´í„°ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
            model_stats = {}
            
            # ëª¨ë“  í‰ê°€ ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ê° ëª¨ë¸ì˜ í‰ê°€ ì •ë³´ ìˆ˜ì§‘
            for entry in self.evaluations["evaluations"]:
                for model, eval_data in entry["evaluations"].items():
                    if model not in model_stats:
                        model_stats[model] = {
                            "total_evaluations": 0,
                            "total_score": 0.0,
                            "avg_score": 0.0
                        }
                    
                    if eval_data.get("score") is not None:  # scoreê°€ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
                        model_stats[model]["total_evaluations"] += 1
                        model_stats[model]["total_score"] += eval_data["score"]
            
            # í‰ê·  ì ìˆ˜ ê³„ì‚°
            for model_data in model_stats.values():
                if model_data["total_evaluations"] > 0:
                    model_data["avg_score"] = round(
                        model_data["total_score"] / model_data["total_evaluations"], 
                        2
                    )
                del model_data["total_score"]  # ì¤‘ê°„ ê³„ì‚°ì— ì‚¬ìš©ëœ í•„ë“œ ì œê±°
            
            # í†µê³„ ì €ì¥ ë° ë°˜í™˜
            self.evaluations["statistics"] = model_stats
            self._save_evaluations()
            return model_stats
            
        except Exception as e:
            logger.error(f"í‰ê°€ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def _save_evaluations(self):
        """í‰ê°€ ë°ì´í„°ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            with open(self.evaluation_file, 'w', encoding='utf-8') as f:
                json.dump(self.evaluations, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"í‰ê°€ ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

class AutoEvaluator:
    def __init__(self, model_name: str = "gemma3:12b"):
        self.model_name = model_name
        self.eval_prompt_template = """ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€ ìŒì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì—„ê²©í•œ í‰ê°€ìì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ˆë¬¸, ê´€ë ¨ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸, ëª¨ë¸ ë‹µë³€ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€ì´ ì •í™•í•˜ê³  ê´€ë ¨ì„±ì´ ë†’ì€ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 1-5ì  ì²™ë„ë¡œ í‰ê°€í•˜ì„¸ìš”:
1: ì™„ì „íˆ ì˜ëª»ëœ ì •ë³´ë¥¼ ì œê³µí•˜ê±°ë‚˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‹µë³€
2: ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ì€ ìˆì§€ë§Œ ë¶€ì •í™•í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•œ ë‹µë³€
3: ê¸°ë³¸ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í–ˆì§€ë§Œ ì„¸ë¶€ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì•½ê°„ì˜ ì˜¤ë¥˜ê°€ ìˆìŒ
4: ì •í™•í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ë‹µë³€ì´ì§€ë§Œ ì™„ë²½í•˜ì§€ ì•ŠìŒ
5: ì™„ë²½í•˜ê²Œ ì •í™•í•˜ê³  í¬ê´„ì ì´ë©° ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œ ë‹µë³€

[ì§ˆë¬¸]
{question}

[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]
{context}

[ëª¨ë¸ ë‹µë³€]
{answer}

[í‰ê°€]
ìœ„ ë‹µë³€ì— ëŒ€í•œ í‰ê°€ë¥¼ 1-5ì  ì²™ë„ë¡œ ìˆ˜í–‰í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë‹µë³€ í˜•ì‹:
ì ìˆ˜: (1-5 ì‚¬ì´ì˜ ì •ìˆ˜ë§Œ ì…ë ¥)
ì´ìœ : (í‰ê°€ ì´ìœ  ì„¤ëª…)"""
        
    def _generate_with_ollama(self, prompt: str, stream=False) -> str:
        """Ollama APIë¥¼ í†µí•´ í‰ê°€ ìƒì„±"""
        print(f"\nğŸ¤– ìë™ í‰ê°€ ìƒì„± ì¤‘... ({self.model_name})")
        start_time = time.time()
        
        # Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if OLLAMA_AVAILABLE:
            try:
                if stream:
                    # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                    print("\ní‰ê°€ ì‘ë‹µ: ", end="")
                    sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                    full_result = ""
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
                    for chunk in ollama.generate(
                        model=self.model_name,
                        prompt=prompt,
                        options={"temperature": 0.2, "top_p": 0.95, "num_predict": 1000},
                        stream=True
                    ):
                        chunk_content = chunk.get("response", "")
                        full_result += chunk_content
                        print(chunk_content, end="")
                        sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                    
                    print()  # ì¤„ë°”ê¿ˆ
                    sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                    elapsed_time = time.time() - start_time
                    print(f"âœ“ í‰ê°€ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                    return full_result
                else:
                    # ì¼ë°˜ ëª¨ë“œ
                    response = ollama.generate(
                        model=self.model_name,
                        prompt=prompt,
                        options={"temperature": 0.2, "top_p": 0.95, "num_predict": 1000}
                    )
                    result = response.get("response", "")
                    elapsed_time = time.time() - start_time
                    print(f"âœ“ í‰ê°€ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                    return result
            except Exception as e:
                print(f"âš ï¸ ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ ì‹¤íŒ¨: {e}, REST APIë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        
        # REST API ì‚¬ìš©
        try:
            if stream:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                print("\ní‰ê°€ ì‘ë‹µ: ", end="")
                full_result = ""
                
                response = requests.post(
                    f"{OLLAMA_API_BASE}/generate",
                    json={
                        "model": self.model_name,
                        "prompt": prompt,
                        "stream": True,
                        "options": {"temperature": 0.2, "top_p": 0.95, "num_predict": 1000}
                    },
                    stream=True
                )
                response.raise_for_status()
                
                for line in response.iter_lines():
                    if line:
                        try:
                            line_text = line.decode('utf-8')
                            json_data = json.loads(line_text)
                            chunk_content = json_data.get("response", "")
                            if chunk_content:
                                full_result += chunk_content
                                print(chunk_content, end="")
                                sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                        except json.JSONDecodeError:
                            continue
                
                print()  # ì¤„ë°”ê¿ˆ
                sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                elapsed_time = time.time() - start_time
                print(f"âœ“ ìŠ¤íŠ¸ë¦¬ë° í‰ê°€ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                return full_result
            else:
                # ì¼ë°˜ ëª¨ë“œ
                response = requests.post(
                    f"{OLLAMA_API_BASE}/generate",
                    json={
                        "model": self.model_name,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.2, "top_p": 0.95, "num_predict": 1000}
                    }
                )
                response.raise_for_status()
                result = response.json().get("response", "")
                elapsed_time = time.time() - start_time
                print(f"âœ“ í‰ê°€ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                return result
        except Exception as e:
            logger.error(f"âŒ ìë™ í‰ê°€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ ìë™ í‰ê°€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "í‰ê°€ ì‹¤íŒ¨"
    
    def evaluate_answer(self, question: str, context: str, answer: str, stream: bool = True) -> Dict[str, Any]:
        """ë‹µë³€ì˜ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ í‰ê°€"""
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í‰ê°€ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê³ ë ¤)
        max_context_length = 1500
        if len(context) > max_context_length:
            context_parts = context.split("\n\n")
            optimized_context = []
            current_length = 0
            
            for part in context_parts:
                if current_length + len(part) <= max_context_length:
                    optimized_context.append(part)
                    current_length += len(part)
                else:
                    break
            
            context = "\n\n".join(optimized_context)
        
        prompt = self.eval_prompt_template.format(
            question=question,
            context=context,
            answer=answer
        )
        
        evaluation_result = self._generate_with_ollama(prompt, stream=stream)
        
        # ì ìˆ˜ ì¶”ì¶œ
        score_pattern = r"ì ìˆ˜:\s*(\d+)"
        score_match = re.search(score_pattern, evaluation_result)
        
        score = None
        reason = evaluation_result
        
        if score_match:
            try:
                score = int(score_match.group(1))
                # ì ìˆ˜ ë²”ìœ„ í™•ì¸ (1-5)
                if score < 1 or score > 5:
                    score = None
            except ValueError:
                score = None
            
            # ì´ìœ  ì¶”ì¶œ
            reason_pattern = r"ì´ìœ :(.*?)(?=$|ì ìˆ˜:)"
            reason_match = re.search(reason_pattern, evaluation_result, re.DOTALL)
            if reason_match:
                reason = reason_match.group(1).strip()
        
        return {
            "score": score,
            "reason": reason,
            "raw_evaluation": evaluation_result
        }

class AutoQuestionGenerator:
    def __init__(self, model_name: str = "gemma3:12b"):
        self.model_name = model_name
        self.question_prompt_template = """ë‹¹ì‹ ì€ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì— ê´€í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”:
1. ì¬ë¬´/ì‹¤ì  ê´€ë ¨ ì§ˆë¬¸ (ìˆ˜ìµ, ì†ì‹¤, ì„±ì¥ë¥  ë“±)
2. ì‚¬ì—… ì „ëµ ê´€ë ¨ ì§ˆë¬¸
3. ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê´€ë ¨ ì§ˆë¬¸
4. ì§€ë°°êµ¬ì¡° ê´€ë ¨ ì§ˆë¬¸
5. ìƒí’ˆ/ì„œë¹„ìŠ¤ ê´€ë ¨ ì§ˆë¬¸

[ë¬¸ì„œ ë‚´ìš©]
{context}

ìœ„ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§ˆë¬¸ 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”:
["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3", "ì§ˆë¬¸4", "ì§ˆë¬¸5"]"""

    def _generate_with_ollama(self, prompt: str, stream=False) -> str:
        """Ollama APIë¥¼ í†µí•´ ì§ˆë¬¸ ìƒì„±"""
        print(f"\nğŸ¤– ìë™ ì§ˆë¬¸ ìƒì„± ì¤‘... ({self.model_name})")
        start_time = time.time()
        
        # Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if OLLAMA_AVAILABLE:
            try:
                if stream:
                    # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                    print("\nì§ˆë¬¸ ìƒì„± ì‘ë‹µ: ", end="")
                    sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                    full_result = ""
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
                    for chunk in ollama.generate(
                        model=self.model_name,
                        prompt=prompt,
                        options={"temperature": 0.7, "top_p": 0.95, "num_predict": 1000},
                        stream=True
                    ):
                        chunk_content = chunk.get("response", "")
                        full_result += chunk_content
                        print(chunk_content, end="")
                        sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                    
                    print()  # ì¤„ë°”ê¿ˆ
                    sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                    elapsed_time = time.time() - start_time
                    print(f"âœ“ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                    return full_result
                else:
                    # ì¼ë°˜ ëª¨ë“œ
                    response = ollama.generate(
                        model=self.model_name,
                        prompt=prompt,
                        options={"temperature": 0.7, "top_p": 0.95, "num_predict": 1000}
                    )
                    result = response.get("response", "")
                    elapsed_time = time.time() - start_time
                    print(f"âœ“ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                    return result
            except Exception as e:
                print(f"âš ï¸ ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ ì‹¤íŒ¨: {e}, REST APIë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        
        # REST API ì‚¬ìš©
        try:
            if stream:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                print("\nì§ˆë¬¸ ìƒì„± ì‘ë‹µ: ", end="")
                full_result = ""
                
                response = requests.post(
                    f"{OLLAMA_API_BASE}/generate",
                    json={
                        "model": self.model_name,
                        "prompt": prompt,
                        "stream": True,
                        "options": {"temperature": 0.7, "top_p": 0.95, "num_predict": 1000}
                    },
                    stream=True
                )
                response.raise_for_status()
                
                for line in response.iter_lines():
                    if line:
                        try:
                            line_text = line.decode('utf-8')
                            json_data = json.loads(line_text)
                            chunk_content = json_data.get("response", "")
                            if chunk_content:
                                full_result += chunk_content
                                print(chunk_content, end="")
                                sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                        except json.JSONDecodeError:
                            continue
                
                print()  # ì¤„ë°”ê¿ˆ
                sys.stdout.flush()  # ì¶œë ¥ ë²„í¼ ì¦‰ì‹œ ë¹„ìš°ê¸°
                elapsed_time = time.time() - start_time
                print(f"âœ“ ìŠ¤íŠ¸ë¦¬ë° ì§ˆë¬¸ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                return full_result
            else:
                # ì¼ë°˜ ëª¨ë“œ
                response = requests.post(
                    f"{OLLAMA_API_BASE}/generate",
                    json={
                        "model": self.model_name,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.7, "top_p": 0.95, "num_predict": 1000}
                    }
                )
                response.raise_for_status()
                result = response.json().get("response", "")
                elapsed_time = time.time() - start_time
                print(f"âœ“ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                return result
        except Exception as e:
            logger.error(f"âŒ ìë™ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ ìë™ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "[]"

    def generate_questions(self, context: str, stream: bool = True) -> List[str]:
        """ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        max_context_length = 2000
        if len(context) > max_context_length:
            context_parts = context.split("\n\n")
            optimized_context = []
            current_length = 0
            
            for part in context_parts:
                if current_length + len(part) <= max_context_length:
                    optimized_context.append(part)
                    current_length += len(part)
                else:
                    break
            
            context = "\n\n".join(optimized_context)
        
        prompt = self.question_prompt_template.format(context=context)
        response = self._generate_with_ollama(prompt, stream=stream)
        
        try:
            # JSON ì¶”ì¶œ
            json_pattern = r'\[.*\]'
            json_match = re.search(json_pattern, response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                questions = json.loads(json_str)
                return questions[:5]  # ìµœëŒ€ 5ê°œ ì§ˆë¬¸ ë°˜í™˜
            else:
                # JSON í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
                lines = [line.strip() for line in response.split('\n') 
                         if line.strip() and not line.strip().startswith('[') and not line.strip().endswith(']')]
                questions = []
                for line in lines:
                    # ë²ˆí˜¸ íŒ¨í„´ ì œê±° (ì˜ˆ: "1. ", "2) ", "ì§ˆë¬¸ 1: " ë“±)
                    clean_line = re.sub(r'^(\d+[\.\):]|\*)\s*', '', line).strip()
                    if clean_line and len(clean_line) > 10:  # ìµœì†Œ ê¸¸ì´ ì œí•œ
                        questions.append(clean_line)
                return questions[:5]  # ìµœëŒ€ 5ê°œ ì§ˆë¬¸ ë°˜í™˜
        except json.JSONDecodeError:
            logger.error("âŒ JSON ë””ì½”ë”© ì˜¤ë¥˜")
            # ë²ˆí˜¸ë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ ëª©ë¡ì„ ì¶”ì¶œ
            lines = [line.strip() for line in response.split('\n') if line.strip()]
            questions = []
            for line in lines:
                # ë²ˆí˜¸ íŒ¨í„´ ì œê±° (ì˜ˆ: "1. ", "2) ", "ì§ˆë¬¸ 1: " ë“±)
                if re.match(r'^\d+[\.\):]|^\*', line):
                    clean_line = re.sub(r'^(\d+[\.\):]|\*)\s*', '', line).strip()
                    if clean_line and len(clean_line) > 10:  # ìµœì†Œ ê¸¸ì´ ì œí•œ
                        questions.append(clean_line)
            return questions[:5]  # ìµœëŒ€ 5ê°œ ì§ˆë¬¸ ë°˜í™˜
        
        return []  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ëª©ë¡ ë°˜í™˜

class AutoTestManager:
    def __init__(self, rag_system, auto_question_generator, auto_evaluator, evaluator, available_models):
        self.rag = rag_system
        self.question_generator = auto_question_generator
        self.auto_evaluator = auto_evaluator
        self.evaluator = evaluator
        self.available_models = available_models
        self.results = {
            "tests": [],
            "summary": {}
        }
    
    def run_auto_test(self, num_questions: int = 5, top_k: int = 10, stream: bool = True):
        """ìë™ ì§ˆë¬¸ ìƒì„± ë° í‰ê°€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ ìë™ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì§ˆë¬¸ ìˆ˜: {num_questions})")
        print(f"{'='*80}")
        
        # ë¬¸ì„œì—ì„œ ëª‡ ê°œì˜ í˜ì´ì§€ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        try:
            # ë²¡í„° ì €ì¥ì†Œì—ì„œ ë¬´ì‘ìœ„ ë¬¸ì„œ ìƒ˜í”Œë§
            docs = self.rag.vector_store.similarity_search("í•œí™”ì†í•´ë³´í—˜", k=20)
            
            # ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
            all_test_results = []
            model_scores = {model: [] for model in self.available_models}
            
            # ê° ë¬¸ì„œ ì…‹ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            for i in range(min(num_questions, 3)):  # ìµœëŒ€ 3ë²ˆì˜ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ìƒì„±
                print(f"\nğŸ“š í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ #{i+1} ì¤€ë¹„ ì¤‘...")
                
                # ìƒ˜í”Œ ë¬¸ì„œì—ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                sample_docs = random.sample(docs, min(5, len(docs)))
                context = "\n\n".join([f"[í˜ì´ì§€ {doc.metadata.get('page', 'ë¶ˆëª…')}] {doc.page_content}" for doc in sample_docs])
                
                # ìë™ ì§ˆë¬¸ ìƒì„±
                print("\nğŸ“ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ ìƒì„± ì¤‘...")
                questions = self.question_generator.generate_questions(context, stream=stream)
                
                if not questions:
                    print("âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨, ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                    continue
                
                print(f"\nâœ… {len(questions)}ê°œ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ:")
                for j, q in enumerate(questions):
                    print(f"  {j+1}. {q}")
                
                # ê° ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì‘ë‹µ ìƒì„± ë° í‰ê°€
                for j, question in enumerate(questions[:min(5, len(questions))]):
                    print(f"\n{'='*50}")
                    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ #{i+1}-{j+1}: '{question}'")
                    
                    test_results = {
                        "question": question,
                        "context_summary": context[:200] + "..." if len(context) > 200 else context,
                        "models": {}
                    }
                    
                    # ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ë‹µë³€ ìƒì„± ë° í‰ê°€
                    for model in self.available_models:
                        print(f"\nğŸ”„ ëª¨ë¸ '{model}' í…ŒìŠ¤íŠ¸ ì¤‘...")
                        
                        # ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
                        try:
                            sources, answer = self.rag.query(
                                question, 
                                model_name=model, 
                                top_k=top_k
                            )
                            
                            # ìë™ í‰ê°€ ìˆ˜í–‰
                            evaluation = self.auto_evaluator.evaluate_answer(
                                question, 
                                context, 
                                answer,
                                stream=stream
                            )
                            
                            # í‰ê°€ ê²°ê³¼ ì €ì¥
                            test_results["models"][model] = {
                                "answer": answer,
                                "score": evaluation.get("score"),
                                "reason": evaluation.get("reason"),
                                "raw_evaluation": evaluation.get("raw_evaluation")
                            }
                            
                            # ëª¨ë¸ ì ìˆ˜ ì§‘ê³„
                            if evaluation.get("score") is not None:
                                model_scores[model].append(evaluation.get("score"))
                                
                            print(f"ğŸ“Š í‰ê°€ ì ìˆ˜: {evaluation.get('score', 'í‰ê°€ ë¶ˆê°€')}/5")
                            
                        except Exception as e:
                            logger.error(f"ëª¨ë¸ {model} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                            print(f"âŒ ëª¨ë¸ {model} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                            test_results["models"][model] = {
                                "error": str(e)
                            }
                    
                    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
                    all_test_results.append(test_results)
                    
                    print(f"{'='*50}")
            
            # ìš”ì•½ í†µê³„ ê³„ì‚°
            summary = {}
            for model in self.available_models:
                scores = model_scores[model]
                if scores:
                    avg_score = sum(scores) / len(scores)
                    summary[model] = {
                        "avg_score": round(avg_score, 2),
                        "num_evaluations": len(scores)
                    }
                else:
                    summary[model] = {
                        "avg_score": 0,
                        "num_evaluations": 0
                    }
            
            # ê²°ê³¼ ì €ì¥
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            results_file = f"auto_test_results_{timestamp}.json"
            final_results = {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "num_questions": sum(1 for _ in all_test_results),
                "tests": all_test_results,
                "summary": summary
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(final_results, f, ensure_ascii=False, indent=2)
            
            # ìš”ì•½ ì¶œë ¥
            print(f"\n{'='*80}")
            print(f"ğŸ“Š ìë™ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
            print(f"{'='*80}")
            print(f"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìˆ˜: {sum(1 for _ in all_test_results)}")
            
            for model, stats in summary.items():
                print(f"\n{model}:")
                if stats["num_evaluations"] > 0:
                    print(f"  - í‰ê·  ì ìˆ˜: {stats['avg_score']:.2f}/5.00 ({stats['num_evaluations']}ê°œ í‰ê°€)")
                else:
                    print(f"  - í‰ê°€ ì—†ìŒ")
            
            print(f"\nê²°ê³¼ íŒŒì¼ ì €ì¥ë¨: {results_file}")
            print(f"{'='*80}")
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ ìë™ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            print(f"âŒ ìë™ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            print(traceback.format_exc())
            return None

def check_ollama_models():
    """
    ì˜¬ë¼ë§ˆ ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ í™•ì¸í•©ë‹ˆë‹¤.
    ì—¬ëŸ¬ ë°©ë²•ì„ ì‹œë„í•˜ê³  ì„±ê³µ ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global AVAILABLE_MODELS
    
    # ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ (ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
    default_models = ["gemma3:1b", "gemma3:4b", "gemma3:12b"]
    supported_models = []
    
    # ë°©ë²• 1: Ollama Python í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°©ë²•)
    if OLLAMA_AVAILABLE:
        try:
            print("ğŸ”„ Ollama Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëª¨ë¸ ì¡°íšŒ ì‹œë„ ì¤‘...")
            ollama_models = ollama.list()
            available_models = [model['name'] for model in ollama_models.get('models', [])]
            print(f"âœ“ Ollama Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—°ê²° ì„±ê³µ: {len(available_models)}ê°œ ëª¨ë¸ ë°œê²¬")
            
            if available_models:
                print(f"âœ“ ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡: {', '.join(available_models)}")
                
                # ìš°ì„  ìš°ë¦¬ê°€ ì›í•˜ëŠ” ëª¨ë¸(AVAILABLE_MODELS)ì´ ìˆëŠ”ì§€ í™•ì¸
                for model_name in AVAILABLE_MODELS:
                    if any(model_name in model for model in available_models):
                        supported_models.append(model_name)
                
                # ì›í•˜ëŠ” ëª¨ë¸ì´ ì—†ìœ¼ë©´ gemma ë˜ëŠ” llama ëª¨ë¸ì„ ì°¾ì•„ë´„
                if not supported_models:
                    for model in available_models:
                        if "gemma" in model.lower() or "llama" in model.lower():
                            supported_models.append(model)
                
                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì„¤ì¹˜ëœ ëª¨ë“  ëª¨ë¸ ì‚¬ìš©
                if not supported_models:
                    supported_models = available_models
                
                print(f"âœ“ ì‚¬ìš©í•  ëª¨ë¸: {', '.join(supported_models)}")
                return supported_models
            else:
                print("âš ï¸ Python ë¼ì´ë¸ŒëŸ¬ë¦¬: ì„¤ì¹˜ëœ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ Ollama Python ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 2: Ollama REST API ì‚¬ìš©
    try:
        print("ğŸ”„ Ollama REST APIë¡œ ëª¨ë¸ ì¡°íšŒ ì‹œë„ ì¤‘...")
        response = requests.get(f"{OLLAMA_API_BASE}/tags", timeout=5)
        response.raise_for_status()
        models = response.json().get("models", [])
        installed_models = [model.get("name") for model in models]
        print(f"âœ“ Ollama REST API ì—°ê²° ì„±ê³µ: {len(installed_models)}ê°œ ëª¨ë¸ ë°œê²¬")
        
        if installed_models:
            # ìš°ì„  ìš°ë¦¬ê°€ ì›í•˜ëŠ” ëª¨ë¸(AVAILABLE_MODELS)ì´ ìˆëŠ”ì§€ í™•ì¸
            for model_name in AVAILABLE_MODELS:
                if any(model_name in model for model in installed_models):
                    supported_models.append(model_name)
            
            # ì›í•˜ëŠ” ëª¨ë¸ì´ ì—†ìœ¼ë©´ gemma ë˜ëŠ” llama ëª¨ë¸ì„ ì°¾ì•„ë´„
            if not supported_models:
                for model in installed_models:
                    if "gemma" in model.lower() or "llama" in model.lower():
                        supported_models.append(model)
            
            # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì„¤ì¹˜ëœ ëª¨ë“  ëª¨ë¸ ì‚¬ìš©
            if not supported_models:
                supported_models = installed_models
            
            print(f"âœ“ ì‚¬ìš©í•  ëª¨ë¸: {', '.join(supported_models)}")
            return supported_models
        else:
            print("âš ï¸ REST API: ì„¤ì¹˜ëœ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ Ollama REST API ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 3: ëª…ë ¹ì¤„ ë„êµ¬ ì‚¬ìš©
    try:
        print("ğŸ”„ ëª…ë ¹ì¤„ 'ollama list' ì‹¤í–‰ ì‹œë„ ì¤‘...")
        import subprocess
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True, check=True, timeout=5)
        output_lines = result.stdout.strip().split('\n')
        
        # í—¤ë” ì œê±° í›„ ëª¨ë¸ ì´ë¦„ë§Œ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì»¬ëŸ¼)
        if len(output_lines) > 1:
            cmd_models = [line.split()[0] for line in output_lines[1:]]
            print(f"âœ“ ëª…ë ¹ì¤„ 'ollama list' ì‹¤í–‰ ì„±ê³µ: {len(cmd_models)}ê°œ ëª¨ë¸ ë°œê²¬")
            
            if cmd_models:
                # ìš°ì„  ìš°ë¦¬ê°€ ì›í•˜ëŠ” ëª¨ë¸(AVAILABLE_MODELS)ì´ ìˆëŠ”ì§€ í™•ì¸
                for model_name in AVAILABLE_MODELS:
                    if any(model_name in model for model in cmd_models):
                        supported_models.append(model_name)
                
                # ì›í•˜ëŠ” ëª¨ë¸ì´ ì—†ìœ¼ë©´ gemma ë˜ëŠ” llama ëª¨ë¸ì„ ì°¾ì•„ë´„
                if not supported_models:
                    for model in cmd_models:
                        if "gemma" in model.lower() or "llama" in model.lower():
                            supported_models.append(model)
                
                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì„¤ì¹˜ëœ ëª¨ë“  ëª¨ë¸ ì‚¬ìš©
                if not supported_models:
                    supported_models = cmd_models
                
                print(f"âœ“ ì‚¬ìš©í•  ëª¨ë¸: {', '.join(supported_models)}")
                return supported_models
            else:
                print("âš ï¸ ëª…ë ¹ì¤„: ì„¤ì¹˜ëœ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ëª…ë ¹ì¤„: ì„¤ì¹˜ëœ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ëª…ë ¹ì¤„ 'ollama list' ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
    print(f"âš ï¸ ëª¨ë“  ëª¨ë¸ ì¡°íšŒ ë°©ë²• ì‹¤íŒ¨. ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {', '.join(default_models)}")
    return default_models

if __name__ == "__main__":
    # ëª…ë ¹ì¤„ ì¸ì ì„¤ì •
    parser = argparse.ArgumentParser(description="í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ RAG ì‹œìŠ¤í…œ (Streamlit)")
    
    # ê¸°íƒ€ ì„¤ì •
    parser.add_argument("--pdf", type=str, default="./[í•œí™”ì†í•´ë³´í—˜]ì‚¬ì—…ë³´ê³ ì„œ(2025.03.11).pdf", help="PDF íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--embeddings", type=str, default="bge-m3", choices=["e5", "openai", "bge-m3"], help="ì„ë² ë”© ëª¨ë¸")
    parser.add_argument("--chunk-size", type=int, default=500, help="ë¬¸ì„œ ì²­í¬ í¬ê¸°")
    parser.add_argument("--chunk-overlap", type=int, default=150, help="ë¬¸ì„œ ì²­í¬ ê²¹ì¹¨")
    parser.add_argument("--top-k", type=int, default=12, help="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜")
    parser.add_argument("--force-update", action="store_true", help="ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸")
    parser.add_argument("--use-multi-query", action="store_true", help="ë‹¤ì¤‘ ì¿¼ë¦¬ í™•ì¥ ì‚¬ìš©")
    parser.add_argument("--hybrid-weight", type=float, default=0.5, help="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (0-1)")
    parser.add_argument("--auto-eval", action="store_true", help="ìë™ í‰ê°€ í™œì„±í™”")
    parser.add_argument("--auto-test", action="store_true", help="ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--num-questions", type=int, default=5, help="ìë™ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìˆ˜")

    args = parser.parse_args()
    
    # í˜„ì¬ ì‹¤í–‰ í™˜ê²½ì´ Streamlitì¸ì§€ í™•ì¸
    if 'STREAMLIT_RUNTIME' in os.environ:
        # Streamlit í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘
        streamlit_main()
    else:
        # ì¼ë°˜ Python í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘
        try:
            main()
        except Exception as e:
            logger = setup_logging()
            logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(traceback.format_exc())
            print(f"âŒ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ì„¸ë¶€ ì˜¤ë¥˜ ë‚´ìš©:", traceback.format_exc())
import os
import re
import time
import json
import hashlib
import logging
import traceback
import requests
import torch
import argparse
from typing import List, Dict, Any, Optional
import numpy as np
from dotenv import load_dotenv
import pypdf
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from sentence_transformers import SentenceTransformer
import faiss
from langchain_community.vectorstores import FAISS

# ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
DEBUG_MODE = False

# OLLAMA_AVAILABLE ë³€ìˆ˜ ì •ì˜
import ollama
OLLAMA_AVAILABLE = True

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
SCRIPT_DIR = os.getcwd()
BASE_DIR = os.path.dirname(SCRIPT_DIR)
print(f"í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ê²½ë¡œ: {SCRIPT_DIR}")
print(f"ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ: {BASE_DIR}")

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenMP ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ (FAISSì™€ Java ì¶©ëŒ ë°©ì§€)
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

# í™˜ê²½ ì„¤ì •
PDF_PATH = os.path.join(SCRIPT_DIR, "[í•œí™”ì†í•´ë³´í—˜]ì‚¬ì—…ë³´ê³ ì„œ(2025.03.11).pdf")
INDEX_DIR = os.path.join(SCRIPT_DIR, "Index")
METADATA_FILE = os.path.join(SCRIPT_DIR, "Index/document_metadata_bge.json")
LOG_FILE = os.path.join(SCRIPT_DIR, "Log/hwgi_rag_streamlit.log")
CACHE_FILE = os.path.join(SCRIPT_DIR, "cache.json")

# Ollama API ê¸°ë³¸ URL ì„¤ì •
OLLAMA_API_BASE = "http://localhost:11434/api"

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„¤ì •
AVAILABLE_MODELS = ["gemma3:12b"]

# ëª¨ë¸ ì„¤ì •
EMBEDDING_MODELS = {
    "bge-m3": {
        "name": "BAAI/bge-m3",
        "index_dir": INDEX_DIR,
        "metadata_file": METADATA_FILE
    }
}

# ë¡œê¹… ì„¤ì •
def setup_logging(log_level=logging.DEBUG):
    logger = logging.getLogger('hwgi_rag')
    logger.setLevel(log_level)
    if not logger.handlers:
        file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
        file_handler.setLevel(log_level)
        console_handler = logging.StreamHandler()
        console_handler.setLevel(log_level)
        log_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(log_format)
        console_handler.setFormatter(log_format)
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
    return logger

logger = setup_logging()

# ### BGE-M3 ì„ë² ë”© í´ë˜ìŠ¤
class BGEM3Embeddings(Embeddings):
    def __init__(self, model_name: str = "BAAI/bge-m3"):
        print(f"âœ“ BGE-M3 ì„ë² ë”© ëª¨ë¸ '{model_name}' ì´ˆê¸°í™” ì¤‘...")
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("âœ“ Apple Silicon GPU (MPS) ì‚¬ìš© ê°€ëŠ¥")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print("âœ“ NVIDIA GPU (CUDA) ì‚¬ìš© ê°€ëŠ¥")
        else:
            self.device = torch.device("cpu")
            print("âœ“ CPU ì‚¬ìš©")
        
        self.model.to(self.device)
        print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜"""
        try:
            batch_size = 64
            all_embeddings = []
            for i in range(0, len(texts), batch_size):
                batch = [self._preprocess_text(text) for text in texts[i:i + batch_size]]
                with torch.inference_mode():
                    embeddings = self.model.encode(
                        batch,
                        convert_to_tensor=True,
                        device=self.device,
                        normalize_embeddings=True
                    )
                    if self.device.type == "mps":
                        embeddings = embeddings.to("cpu")
                    all_embeddings.extend(embeddings.cpu().numpy().tolist())
            return all_embeddings
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜"""
        try:
            query_text = f"query: {text}"
            with torch.inference_mode():
                embedding = self.model.encode(
                    [query_text],
                    convert_to_tensor=True,
                    device=self.device,
                    normalize_embeddings=True
                )
                if self.device.type == "mps":
                    embedding = embedding.to("cpu")
                return embedding.cpu().numpy().tolist()[0]
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

# ### PDF ì²˜ë¦¬ í´ë˜ìŠ¤
class PDFProcessor:
    def __init__(self, pdf_path: str):
        if not os.path.isabs(pdf_path):
            self.pdf_path = os.path.join(SCRIPT_DIR, pdf_path)
        else:
            self.pdf_path = pdf_path
        self.text_content = []
        self.tables = []
        self.page_count = 0
        self.pdf_hash = self._calculate_pdf_hash()
        self.hash_file = os.path.join(SCRIPT_DIR, "pdf_hash.json")
        logger.info(f"PDFProcessor ì´ˆê¸°í™”: '{self.pdf_path}' íŒŒì¼ ì²˜ë¦¬ ì¤€ë¹„")
    
    def _calculate_pdf_hash(self) -> str:
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_hash = hashlib.md5(file.read()).hexdigest()
            return pdf_hash
        except Exception as e:
            logger.error(f"PDF í•´ì‹œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return ""
    
    def _load_previous_hash(self) -> str:
        try:
            if os.path.exists(self.hash_file):
                with open(self.hash_file, 'r') as f:
                    data = json.load(f)
                    return data.get('pdf_hash', '')
            return ''
        except Exception as e:
            logger.error(f"ì´ì „ í•´ì‹œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return ''
    
    def _save_current_hash(self):
        try:
            os.makedirs(os.path.dirname(self.hash_file), exist_ok=True)
            data = {
                'pdf_hash': self.pdf_hash,
                'pdf_path': self.pdf_path,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            with open(self.hash_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ í˜„ì¬ PDF í•´ì‹œ ì €ì¥ ì™„ë£Œ: {self.hash_file}")
        except Exception as e:
            print(f"âš ï¸ í˜„ì¬ í•´ì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"í˜„ì¬ í•´ì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def needs_processing(self) -> bool:
        previous_hash = self._load_previous_hash()
        needs_processing = previous_hash != self.pdf_hash
        if not needs_processing:
            logger.info("ì´ì „ì— ì²˜ë¦¬ëœ ë™ì¼í•œ PDF íŒŒì¼ ê°ì§€")
            print("âœ“ ì´ë¯¸ ì²˜ë¦¬ëœ PDF íŒŒì¼ì…ë‹ˆë‹¤")
        return needs_processing
    
    def extract_text(self) -> List[Document]:
        logger.info("ğŸ“„ PDF í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ ì‹œì‘")
        print("ğŸ“„ PDF í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
        documents = []
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)
                self.page_count = len(pdf_reader.pages)
                for page_num, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    if text.strip():
                        doc_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                        self.text_content.append({
                            "page": page_num + 1,
                            "content": text,
                            "hash": doc_hash
                        })
                        documents.append(
                            Document(
                                page_content=text,
                                metadata={"page": page_num + 1, "source": "text", "hash": doc_hash}
                            )
                        )
            logger.info(f"âœ… ì´ {self.page_count}í˜ì´ì§€ì—ì„œ {len(documents)}ê°œì˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ì¶œ ì™„ë£Œ")
            return documents
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def extract_tables(self) -> List[Document]:
        logger.info("ğŸ“Š PDF í‘œ ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
        print("ğŸ“Š PDF í‘œ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        try:
            table_documents = []
            with pdfplumber.open(self.pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    tables = page.extract_tables()
                    if not tables:
                        continue
                    for table_idx, table in enumerate(tables):
                        table_content = []
                        for row in table:
                            if any(cell for cell in row):
                                cleaned_row = [str(cell).strip() if cell else "" for cell in row]
                                table_content.append(cleaned_row)
                        if not table_content:
                            continue
                        table_text = '\n'.join([','.join(row) for row in table_content])
                        table_hash = hashlib.md5(table_text.encode('utf-8')).hexdigest()
                        metadata = {
                            'table_id': f'table_p{page_num}_t{table_idx + 1}',
                            'page': page_num,
                            'source': 'table',
                            'hash': table_hash,
                            'row_count': len(table_content),
                            'col_count': len(table_content[0]) if table_content else 0
                        }
                        self.tables.append({
                            'table_id': metadata['table_id'],
                            'content': table_text,
                            'raw_data': table_content,
                            'hash': table_hash,
                            'metadata': metadata
                        })
                        table_documents.append(
                            Document(
                                page_content=f"í‘œ {metadata['table_id']}:\n{table_text}",
                                metadata=metadata
                            )
                        )
            logger.info(f"âœ… {len(table_documents)}ê°œì˜ í‘œ ì²˜ë¦¬ ì™„ë£Œ")
            return table_documents
        except Exception as e:
            logger.error(f"âŒ í‘œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def process(self) -> List[Document]:
        print(f"\n{'â”€'*60}")
        print("ğŸ“Œ 1ë‹¨ê³„: PDF ë¬¸ì„œ ì²˜ë¦¬")
        print(f"{'â”€'*60}")
        if not self.needs_processing():
            logger.info("ì´ì „ì— ì²˜ë¦¬ëœ ë™ì¼í•œ PDF íŒŒì¼ ê°ì§€")
            print("âœ“ ì´ë¯¸ ì²˜ë¦¬ëœ PDF íŒŒì¼ì…ë‹ˆë‹¤")
            return []
        logger.info("===== PDF ì²˜ë¦¬ ì‹œì‘ =====")
        text_docs = self.extract_text()
        table_docs = self.extract_tables()
        all_docs = text_docs + table_docs
        if not all_docs:
            print("âš ï¸ PDFì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return []
        self._save_current_hash()
        logger.info(f"ğŸ“š {len(all_docs)}ê°œì˜ ë¬¸ì„œ ì¡°ê° ìƒì„±ë¨")
        print(f"ğŸ“š PDF ì²˜ë¦¬ ì™„ë£Œ: {len(text_docs)}ê°œ í…ìŠ¤íŠ¸ ë¬¸ì„œ, {len(table_docs)}ê°œ í‘œ ë¬¸ì„œ ìƒì„±")
        return all_docs

# ### ë¬¸ì„œ ë¶„í•  í´ë˜ìŠ¤
class DocumentSplitter:
    def __init__(self, chunk_size=800, chunk_overlap=200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ";", ":", ",", " ", ""]
        )
        logger.info(f"DocumentSplitter ì´ˆê¸°í™”: ì²­í¬ í¬ê¸°={chunk_size}, ê²¹ì¹¨={chunk_overlap}")
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        logger.info(f"ğŸ”ª ë¬¸ì„œ ë¶„í•  ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
        print("ğŸ”ª ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
        try:
            chunks = self.text_splitter.split_documents(documents)
            for i, chunk in enumerate(chunks):
                chunk.metadata['chunk_id'] = f"chunk_{i}"
            logger.info(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")
            return chunks
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜: {e}")
            return documents

# ### RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤
class RAGSystem:
    def __init__(self, embedding_type: str = "bge-m3", use_hnsw: bool = True, ef_search: int = 200, ef_construction: int = 200, m: int = 64):
        print("ğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        print(f"  - ì„ë² ë”© ëª¨ë¸: {embedding_type}")
        
        if hasattr(RAGSystem, '_initialized'):
            logger.info("RAG ì‹œìŠ¤í…œì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            return
        
        RAGSystem._initialized = True
        self.embedding_type = embedding_type
        self.embedding_name = None
        
        if embedding_type == "bge-m3":
            self.embeddings = BGEM3Embeddings(model_name="BAAI/bge-m3")
            self.embedding_name = "bge-m3"
        
        self._cache = self._load_cache()
        self.use_hnsw = use_hnsw
        self.ef_search = ef_search
        self.ef_construction = ef_construction
        self.m = m
        self.vector_store = None
        self.index_dir = INDEX_DIR
        os.makedirs(self.index_dir, exist_ok=True)
        self.ollama_base_url = "http://localhost:11434/api"
        self.available_models = self._check_ollama_models()
        
        if self.available_models:
            print(f"âœ“ ì‚¬ìš©í•  ëª¨ë¸: {self.available_models[0]}")
        print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_cache(self) -> Dict[str, str]:
        cache_file = os.path.join(SCRIPT_DIR, "cache.json")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {}
    
    def _save_cache(self):
        cache_file = os.path.join(SCRIPT_DIR, "cache.json")
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self._cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _check_ollama_models(self) -> List[str]:
        try:
            response = requests.get(f"{self.ollama_base_url}/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [model["name"] for model in models if model["name"] in AVAILABLE_MODELS]
            return []
        except Exception as e:
            logger.error(f"Ollama ëª¨ë¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _generate_with_ollama(self, prompt: str, model: str, stream: bool = True) -> str:
        logger.debug(f"ë‹µë³€ ìƒì„± ìš”ì²­ - ì¿¼ë¦¬: '{prompt[:30]}...', ëª¨ë¸: {model}")
        api_json = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {"temperature": 0.7, "top_p": 0.9}
        }
        response_text = ""
        try:
            if stream:
                print(f"\n================================================================================")
                print(f"ğŸ“ ëª¨ë¸: {model}")
                print(f"================================================================================")
                with requests.post(f"{self.ollama_base_url}/generate", json=api_json, stream=True) as response:
                    for line in response.iter_lines():
                        if line:
                            data = json.loads(line)
                            chunk = data.get("response", "")
                            response_text += chunk
                            print(chunk, end="", flush=True)
                            if data.get("done", False):
                                print()
            else:
                response = requests.post(f"{self.ollama_base_url}/generate", json=api_json)
                if response.status_code == 200:
                    response_text = response.json().get("response", "")
                    print(response_text)
            return response_text
        except Exception as e:
            error_msg = f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            logger.error(error_msg)
            print(f"\nâŒ {error_msg}")
            return f"âš ï¸ {error_msg}"
    
    def answer(self, query: str, model: str, context: str) -> Dict[str, Any]:
        cache_key = f"{model}:{hashlib.md5((query + context[:200]).encode()).hexdigest()}"
        cached_answer = self._cache.get(cache_key)
        if cached_answer and not os.environ.get('DISABLE_CACHE'):
            logger.info(f"ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©: ëª¨ë¸={model}")
            print(f"ğŸ’¾ ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©: {model}")
            return {"answer": cached_answer, "model": model, "cached": True}
        
        from datetime import datetime
        today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        qa_template = f"""ë‹¹ì‹ ì€ í˜„ì¬ ì‹œê°„ {today} ê¸°ì¤€ìœ¼ë¡œ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ì² ì €í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ AI ë¹„ì„œì…ë‹ˆë‹¤.

ë‹¤ìŒ ì§€ì¹¨ì„ ì² ì €íˆ ë”°ë¼ ë‹µë³€í•´ ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ìˆ«ì, ë‚ ì§œ, ê¸ˆì•¡ ë“± ì‚¬ì‹¤ì  ì •ë³´ëŠ” ë¬¸ì„œ ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ëª…í™•í•˜ê³  êµ¬ì¡°ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì°¸ê³  ë¬¸ì„œ ë‚´ìš©:
{context}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€:"""
        try:
            result = self._generate_with_ollama(qa_template, model, stream=True)
            self._cache[cache_key] = result
            self._save_cache()
            return {"answer": result, "model": model, "cached": False}
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {"answer": f"ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "model": model, "error": True}
    
    def search(self, query: str, top_k: int = 5) -> List[Document]:
        logger.info(f"ê²€ìƒ‰ ìš”ì²­: '{query}' (top_k={top_k})")
        print(f"\nğŸ” ê²€ìƒ‰ ì¤‘: '{query}'\n")
        if self.vector_store is None:
            print("âŒ ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return []
        try:
            docs = self.vector_store.similarity_search(query, k=top_k)
            logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ")
            print(f"âœ“ ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ")
            return docs
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def load_or_create_vector_store(self, documents: List[Document], force_update: bool = False) -> bool:
        index_folder = os.path.join(INDEX_DIR, "faiss_index_bge-m3")
        metadata_file = os.path.join(INDEX_DIR, "document_metadata_bge-m3.json")
        
        if not force_update and os.path.exists(os.path.join(index_folder, "index.faiss")) and os.path.exists(os.path.join(index_folder, "index.pkl")):
            try:
                self.vector_store = FAISS.load_local(
                    index_folder,
                    self.embeddings,
                    normalize_L2=True,
                    allow_dangerous_deserialization=True
                )
                if self.use_hnsw and hasattr(self.vector_store.index, 'hnsw'):
                    self.vector_store.index.hnsw.efSearch = self.ef_search
                logger.info(f"ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ: {index_folder}")
                print(f"âœ“ ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ: {index_folder}")
                return True
            except Exception as e:
                logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print(f"âŒ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return self._create_new_vector_store(documents, index_folder, metadata_file)
    
    def _create_new_vector_store(self, documents: List[Document], index_folder: str, metadata_file: str) -> bool:
        logger.info("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹œì‘")
        print("ğŸ”„ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
        if not documents:
            logger.error("ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            print("âŒ ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        for i, doc in enumerate(documents):
            doc.metadata["doc_id"] = f"doc_{i}"
        
        document_metadata = [{"content_preview": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content, **doc.metadata} for doc in documents]
        os.makedirs(os.path.dirname(metadata_file), exist_ok=True)
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(document_metadata, f, ensure_ascii=False, indent=2)
        
        try:
            vector_store = FAISS.from_documents(documents, self.embeddings, normalize_L2=True, distance_strategy="COSINE")
            if self.use_hnsw and hasattr(vector_store.index, 'hnsw'):
                vector_store.index.hnsw.efConstruction = self.ef_construction
                vector_store.index.hnsw.efSearch = self.ef_search
                vector_store.index.hnsw.M = self.m
            vector_store.save_local(index_folder)
            self.vector_store = vector_store
            logger.info(f"âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
            print(f"âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
            return True
        except Exception as e:
            logger.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def format_context_for_model(self, documents: List[Document]) -> str:
        if not documents:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        formatted_docs = [f"[ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}, í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}]\n{doc.page_content}" for doc in documents]
        return "ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n" + "\n\n---\n\n".join(formatted_docs)

# ### ë©”ì¸ í•¨ìˆ˜
def main():
    parser = argparse.ArgumentParser(description='í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ RAG ì‹œìŠ¤í…œ')
    parser.add_argument('--pdf', type=str, default=PDF_PATH, help='PDF íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--chunk-size', type=int, default=800, help='ì²­í¬ í¬ê¸°')
    parser.add_argument('--chunk-overlap', type=int, default=200, help='ì²­í¬ ê²¹ì¹¨')
    parser.add_argument('--top-k', type=int, default=5, help='ê²€ìƒ‰ ê²°ê³¼ ìˆ˜')
    parser.add_argument('--force-update', action='store_true', help='ë²¡í„° ì¸ë±ìŠ¤ ê°•ì œ ì—…ë°ì´íŠ¸')
    args = parser.parse_args()
    
    rag = RAGSystem(use_hnsw=True, ef_search=200, ef_construction=200, m=64)
    processor = PDFProcessor(args.pdf)
    documents = processor.process()
    splitter = DocumentSplitter(chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap)
    chunks = splitter.split_documents(documents)
    rag.load_or_create_vector_store(chunks, force_update=args.force_update)
    
    print("\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+D):")
    import sys
    lines = []
    try:
        for line in sys.stdin:
            if not line.strip():
                break
            lines.append(line)
        question = ''.join(lines).strip()
    except EOFError:
        question = ''.join(lines).strip()
    
    if not question:
        print("âŒ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return 1
    
    retrieved_docs = rag.search(question, top_k=args.top_k)
    context = rag.format_context_for_model(retrieved_docs)
    for model in AVAILABLE_MODELS:
        print(f"\n{'='*80}")
        print(f"ğŸ“ ëª¨ë¸: {model}")
        print(f"{'='*80}")
        result = rag.answer(question, model, context)
        print(result["answer"])
    
    return 0

if __name__ == "__main__":
    main()
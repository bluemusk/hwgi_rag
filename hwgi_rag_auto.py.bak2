import os
import re
import time
import json
import hashlib
import logging
import traceback
import requests
import torch
import argparse
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import streamlit as st
from dotenv import load_dotenv
import asyncio
import time
import io
import base64
import aiohttp
from io import StringIO
from datetime import datetime
import glob
import sys
import tabula
import random
import shutil
import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€
import math
from uuid import uuid4
from concurrent.futures import ThreadPoolExecutor

# ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
DEBUG_MODE = False

# OLLAMA_AVAILABLE ë³€ìˆ˜ ì •ì˜
import ollama
OLLAMA_AVAILABLE = True
   
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
SCRIPT_DIR = os.getcwd()
BASE_DIR = os.path.dirname(SCRIPT_DIR)  # ìƒìœ„ ë””ë ‰í† ë¦¬
print(f"í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ê²½ë¡œ: {SCRIPT_DIR}")
print(f"ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ: {BASE_DIR}")


# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenMP ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ (FAISSì™€ Java ì¶©ëŒ ë°©ì§€)
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pypdf
import tabula
import pdfplumber

# RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from sentence_transformers import SentenceTransformer
import faiss

try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from rank_bm25 import BM25Okapi
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸ langchain_community.embeddings ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# RAG í‰ê°€ ê´€ë ¨ ë©”íŠ¸ë¦­ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from rank_bm25 import BM25Okapi
    EVAL_LIBS_AVAILABLE = True
except ImportError:
    EVAL_LIBS_AVAILABLE = False

# í™˜ê²½ ì„¤ì •
PDF_PATH = os.path.join(SCRIPT_DIR, "[í•œí™”ì†í•´ë³´í—˜]ì‚¬ì—…ë³´ê³ ì„œ(2025.03.11).pdf")
INDEX_DIR = os.path.join(SCRIPT_DIR, "Index")  # ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ê¸°ë³¸ ê²½ë¡œ
METADATA_FILE = os.path.join(SCRIPT_DIR, "Index/document_metadata_bge.json")  # ë©”íƒ€ë°ì´í„° íŒŒì¼
LOG_FILE = os.path.join(SCRIPT_DIR, "Log/hwgi_rag_streamlit.log")
CACHE_FILE = os.path.join(SCRIPT_DIR, "cache.json")
EVALUATION_FILE = os.path.join(SCRIPT_DIR, "Log/model_evaluations.json")  # ëª¨ë¸ í‰ê°€ ê²°ê³¼ ì €ì¥ íŒŒì¼

# Ollama API ê¸°ë³¸ URL ì„¤ì •
OLLAMA_API_BASE = "http://localhost:11434/api"

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„¤ì •
AVAILABLE_MODELS = ["gemma3:12b"]

# ëª¨ë¸ ì„¤ì •
EMBEDDING_MODELS = {
    "bge-m3": {
        "name": "BAAI/bge-m3",
        "index_dir": INDEX_DIR,
        "metadata_file": METADATA_FILE
    }
}

# ë¡œê¹… ì„¤ì •
def setup_logging(log_level=logging.DEBUG):
    logger = logging.getLogger('hwgi_rag')
    logger.setLevel(log_level)
    if not logger.handlers:
        file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
        file_handler.setLevel(log_level)
        console_handler = logging.StreamHandler()
        console_handler.setLevel(log_level)
        log_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(log_format)
        console_handler.setFormatter(log_format)
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
    return logger

logger = setup_logging()

# E5Embeddings í´ë˜ìŠ¤ë¥¼ BGE-M3 ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
class BGEM3Embeddings(Embeddings):
    def __init__(self, model_name: str = "BAAI/bge-m3"):
        print(f"âœ“ BGE-M3 ì„ë² ë”© ëª¨ë¸ '{model_name}' ì´ˆê¸°í™” ì¤‘...")
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("âœ“ Apple Silicon GPU (MPS) ì‚¬ìš© ê°€ëŠ¥")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print("âœ“ NVIDIA GPU (CUDA) ì‚¬ìš© ê°€ëŠ¥")
        else:
            self.device = torch.device("cpu")
            print("âœ“ CPU ì‚¬ìš©")
        
        self.model.to(self.device)
        print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (32 â†’ 64)
            batch_size = 64
            all_embeddings = []
            
            # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¡œ ë°°ì¹˜ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = [self._preprocess_text(text) for text in texts[i:i + batch_size]]
                with torch.inference_mode():
                    embeddings = self.model.encode(
                        batch,
                        convert_to_tensor=True,
                        device=self.device,
                        normalize_embeddings=True  # L2 ì •ê·œí™” ì ìš©
                    )
                    if self.device.type == "mps":
                        embeddings = embeddings.to("cpu")
                    all_embeddings.extend(embeddings.cpu().numpy().tolist())
            
            return all_embeddings
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ì¿¼ë¦¬ìš© ì ‘ë‘ì‚¬ ì¶”ê°€
            query_text = f"query: {text}"
            with torch.inference_mode():
                embedding = self.model.encode(
                    [query_text],
                    convert_to_tensor=True,
                    device=self.device,
                    normalize_embeddings=True
                )
                # MPS ë””ë°”ì´ìŠ¤ì—ì„œ CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜
                if self.device.type == "mps":
                    embedding = embedding.to("cpu")
                return embedding.cpu().numpy().tolist()[0]
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

# OpenAI ì„ë² ë”© í´ë˜ìŠ¤ ì •ì˜
class OpenAIEmbeddings(Embeddings):
    def __init__(self, model_name: str = "text-embedding-3-small"):
        print(f"âœ“ OpenAI ì„ë² ë”© ëª¨ë¸ '{model_name}' ì´ˆê¸°í™” ì¤‘...")
        load_dotenv()  # .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ë¡œë“œ
        self.model_name = model_name
        self.client = OpenAI()
        print("âœ“ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë°°ì¹˜ í¬ê¸° ì„¤ì • (OpenAI API ì œí•œ ê³ ë ¤)
            batch_size = 100
            all_embeddings = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=batch,
                    encoding_format="float"
                )
                batch_embeddings = [data.embedding for data in response.data]
                all_embeddings.extend(batch_embeddings)
            
            return all_embeddings
        except Exception as e:
            print(f"âŒ OpenAI ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            response = self.client.embeddings.create(
                model=self.model_name,
                input=[text],
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ OpenAI ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

# --- PDF ì²˜ë¦¬ ë° ë¬¸ì„œ ë¶„í•  ---
class PDFProcessor:
    def __init__(self, pdf_path: str):
        # ê²½ë¡œê°€ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
        if not os.path.isabs(pdf_path):
            self.pdf_path = os.path.join(SCRIPT_DIR, pdf_path)
        else:
            self.pdf_path = pdf_path
        self.text_content = []  # í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥
        self.tables = []  # í‘œ ë°ì´í„° ì €ì¥
        self.page_count = 0  # ì´ í˜ì´ì§€ ìˆ˜
        self.pdf_hash = self._calculate_pdf_hash()  # PDF íŒŒì¼ í•´ì‹œ
        self.hash_file = os.path.join(SCRIPT_DIR, "pdf_hash.json")  # í•´ì‹œ ì €ì¥ íŒŒì¼
        logger.info(f"PDFProcessor ì´ˆê¸°í™”: '{self.pdf_path}' íŒŒì¼ ì²˜ë¦¬ ì¤€ë¹„")
    
    def _calculate_pdf_hash(self) -> str:
        """PDF íŒŒì¼ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_hash = hashlib.md5(file.read()).hexdigest()
            return pdf_hash
        except Exception as e:
            logger.error(f"PDF í•´ì‹œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return ""
    
    def _load_previous_hash(self) -> str:
        """ì´ì „ì— ì²˜ë¦¬í•œ PDFì˜ í•´ì‹œê°’ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            if os.path.exists(self.hash_file):
                with open(self.hash_file, 'r') as f:
                    data = json.load(f)
                    return data.get('pdf_hash', '')
            return ''
        except Exception as e:
            logger.error(f"ì´ì „ í•´ì‹œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return ''
    
    def _save_current_hash(self):
        """í˜„ì¬ PDF í•´ì‹œê°’ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            os.makedirs(os.path.dirname(self.hash_file), exist_ok=True)
            data = {
                'pdf_hash': self.pdf_hash,
                'pdf_path': self.pdf_path,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            with open(self.hash_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ“ í˜„ì¬ PDF í•´ì‹œ ì €ì¥ ì™„ë£Œ: {self.hash_file}")
        except Exception as e:
            print(f"âš ï¸ í˜„ì¬ í•´ì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(f"í˜„ì¬ í•´ì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def needs_processing(self) -> bool:
        """PDF íŒŒì¼ì˜ ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. í•´ì‹œê°’ì´ ë‹¤ë¥´ë©´ ì¬ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
        previous_hash = self._load_previous_hash()
        needs_processing = previous_hash != self.pdf_hash
        
        if not needs_processing:
            logger.info("ì´ì „ì— ì²˜ë¦¬ëœ ë™ì¼í•œ PDF íŒŒì¼ ê°ì§€. ë³€ê²½ ì—†ìŒìœ¼ë¡œ íŒë‹¨.")
        
        return needs_processing
    
    def force_processing(self) -> bool:
        """PDF íŒŒì¼ì˜ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë„ë¡ ê°•ì œ ì„¤ì •í•©ë‹ˆë‹¤."""
        # í•´ì‹œ íŒŒì¼ ì‚­ì œë¥¼ í†µí•´ ê°•ì œ ì²˜ë¦¬
        if os.path.exists(self.hash_file):
            try:
                os.remove(self.hash_file)
                logger.info(f"PDF í•´ì‹œ íŒŒì¼ ì‚­ì œ: {self.hash_file}")
                print(f"âœ“ PDF í•´ì‹œ íŒŒì¼ ì‚­ì œë¨ - ê°•ì œ ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™”")
                return True
            except Exception as e:
                logger.error(f"PDF í•´ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                print(f"âš ï¸ PDF í•´ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
                return False
        return True
    
    def extract_text(self) -> List[Document]:
        logger.info("ğŸ“„ PDF í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ ì‹œì‘")
        print("ğŸ“„ PDF í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
        documents = []
        try:
            with open(self.pdf_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)
                self.page_count = len(pdf_reader.pages)
                
                for page_num, page in enumerate(pdf_reader.pages):
                    text = page.extract_text()
                    if text.strip():
                        doc_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                        
                        # í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥
                        self.text_content.append({
                            "page": page_num + 1,
                            "content": text,
                            "hash": doc_hash
                        })
                        
                        documents.append(
                            Document(
                                page_content=text,
                                metadata={"page": page_num + 1, "source": "text", "hash": doc_hash}
                            )
                        )
            logger.info(f"âœ… ì´ {self.page_count}í˜ì´ì§€ì—ì„œ {len(documents)}ê°œì˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ì¶œ ì™„ë£Œ")
            return documents
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def table_id_generation(self, element):
        """í…Œì´ë¸” ID ìƒì„± í•¨ìˆ˜"""
        if "Table" not in element:
            return {}
        else:
            values = element['Table']  # list of tables
            keys = [f"element{element['id']}-table{i}" for i in range(len(values))]
            return dict(zip(keys, values))

    def extract_cell_color(self, table):
        """í…Œì´ë¸”ì˜ ì²« ì…€ê³¼ ë§ˆì§€ë§‰ ì…€ ìƒ‰ìƒ ì¶”ì¶œ í•¨ìˆ˜"""
        page_image = table.page.to_image()
        cell_image_first = page_image.original.crop(table.cells[0])
        cell_image_last = page_image.original.crop(table.cells[-1])
        
        res_list = []
        for cell_image in [cell_image_first, cell_image_last]:
            width, height = cell_image.size
            background_pixel = cell_image.getpixel((width/5, height/5))
            res_list.append(background_pixel)
        
        return res_list

    def extract_image(self, table, resolution=300):
        """í…Œì´ë¸” ì´ë¯¸ì§€ ì¶”ì¶œ í•¨ìˆ˜"""
        scale_factor = resolution / 72
        x0, y0, x1, y1 = table.bbox
        
        x0 *= scale_factor
        y0 *= scale_factor
        x1 *= scale_factor
        y1 *= scale_factor
        
        img = table.page.to_image(resolution=resolution)
        table_img = img.original.crop((x0, y0, x1, y1))
        
        return table_img

    def extract_table_info(self, table):
        """í…Œì´ë¸” ë©”íƒ€ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜"""
        table_dict = {
            'page': self.extract_page_number(str(table.page)),
            'bbox': table.bbox,
            'ncol': len(table.columns),
            'nrow': len(table.rows),
            'content': table.extract(),
            'cell_color': self.extract_cell_color(table),
            'img': self.extract_image(table)
        }
        
        return table_dict

    def compare_tables(self, table_A, table_B):
        """ë‘ í…Œì´ë¸”ì´ ê°™ì€ í…Œì´ë¸”ì¸ì§€ ë¹„êµ"""
        prev_info = self.extract_table_info(table_A)
        curr_info = self.extract_table_info(table_B)
        
        counter = 0
        # ë‘ í…Œì´ë¸”ì˜ í˜ì´ì§€ê°€ ì¸ì ‘í•´ ìˆëŠ”ê°€?
        if curr_info['page'] - prev_info['page'] == 1:
            counter += 1
        
        # í…Œì´ë¸” ìœ„ì¹˜ê°€ ì´ì–´ì§€ëŠ”ê°€?
        if (np.round(prev_info['bbox'][3], 0) > 780) and (np.round(curr_info['bbox'][1], 0) == 50):
            counter += 1
        
        # ì…€ ìƒ‰ìƒì´ ê°™ì€ê°€?
        if prev_info['cell_color'][1] == curr_info['cell_color'][0]:
            counter += 1
        
        # ì»¬ëŸ¼ ìˆ˜ê°€ ê°™ì€ê°€?
        if prev_info['ncol'] == curr_info['ncol']:
            counter += 1
        
        decision = 'same table' if counter == 4 else 'different table'
        return [(counter, decision)]

    def find_table_location_in_text(self, element_content):
        """ì½˜í…ì¸  ë‚´ í…Œì´ë¸” ìœ„ì¹˜ ì°¾ê¸°"""
        start_pattern = '<table>'
        table_start_position = re.finditer(start_pattern, element_content)
        start_positions = [(match.start(), match.end()) for match in table_start_position]
        
        end_pattern = '</table>'
        table_end_position = re.finditer(end_pattern, element_content)
        end_positions = [(match.start(), match.end()) for match in table_end_position]
        
        table_location_in_text = [(start[0], end[1]) 
                                for start, end in zip(start_positions, end_positions)]
        
        return table_location_in_text

    def group_table_position(self, element_table):
        """ì—°ì†ëœ í…Œì´ë¸”ì˜ í¬ì§€ì…˜ì„ ë¬¶ì–´ì£¼ëŠ” í•¨ìˆ˜"""
        pos = 0
        counter = 0
        result = []
        
        for i in range(1, len(element_table)):
            counter += 1
            table_comparison_result = self.compare_tables(element_table[i-1], element_table[i])[0][1]
            
            if table_comparison_result != 'same table':
                result.append([pos, pos+counter])
                pos += counter
                counter = 0
        
        # ë§ˆì§€ë§‰ ê·¸ë£¹ ì¶”ê°€
        result.append([pos, pos + counter + 1])
        return result

    def merge_dicts(self, dict_list):
        """ì—¬ëŸ¬ ê°œì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜"""
        merged_dict = {
            'page': [],
            'bbox': [],
            'ncol': 0,
            'nrow': 0,
            'content': [],
            'cell_color': [],
            'img': [],
            'obj_type': ['table']
        }
        
        for d in dict_list:
            merged_dict['page'].append(d['page'])
            merged_dict['bbox'].append(d['bbox'])
            merged_dict['ncol'] = max(merged_dict['ncol'], d['ncol'])
            merged_dict['nrow'] += d['nrow']
            merged_dict['content'] += (d['content'])
            merged_dict['cell_color'].append(d['cell_color'])
            merged_dict['img'].append(d['img'])
        
        return merged_dict

    def extract_page_number(self, text):
        """í…Œì´ë¸”ì´ ìœ„ì¹˜í•œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ í•¨ìˆ˜"""
        match = re.search(r"<Page:(\d+)>", text)
        return int(match.group(1)) if match else None

    def extract_tables(self) -> List[Document]:
        """PDFì—ì„œ í‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  Documentë¡œ ë³€í™˜"""
        logger.info("ğŸ“Š PDF í‘œ ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
        print("ğŸ“Š PDF í‘œ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        try:
            table_documents = []
            
            with pdfplumber.open(self.pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    tables = page.extract_tables()
                    if not tables:
                        continue
                    
                    for table_idx, table in enumerate(tables):
                        # ë¹ˆ í–‰/ì—´ ì œê±° ë° ë¬¸ìì—´ ë³€í™˜
                        table_content = []
                        for row in table:
                            if any(cell for cell in row):  # ë¹ˆ í–‰ ì œì™¸
                                cleaned_row = [str(cell).strip() if cell else "" for cell in row]
                                table_content.append(cleaned_row)
                        
                        if not table_content:
                            continue
                        
                        # CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        table_text = '\n'.join([','.join(row) for row in table_content])
                        table_hash = hashlib.md5(table_text.encode('utf-8')).hexdigest()
                        
                        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        metadata = {
                            'table_id': f'table_p{page_num}_t{table_idx + 1}',
                            'page': page_num,
                            'source': 'table',
                            'hash': table_hash,
                            'row_count': len(table_content),
                            'col_count': len(table_content[0]) if table_content else 0
                        }
                        
                        # í‘œ ì •ë³´ ì €ì¥
                        self.tables.append({
                            'table_id': metadata['table_id'],
                            'content': table_text,
                            'raw_data': table_content,
                            'hash': table_hash,
                            'metadata': metadata
                        })
                        
                        # Document ê°ì²´ ìƒì„±
                        table_documents.append(
                            Document(
                                page_content=f"í‘œ {metadata['table_id']}:\n{table_text}",
                                metadata=metadata
                            )
                        )
            
            logger.info(f"âœ… {len(table_documents)}ê°œì˜ í‘œ ì²˜ë¦¬ ì™„ë£Œ")
            return table_documents
            
        except Exception as e:
            logger.error(f"âŒ í‘œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def process(self) -> List[Document]:
        """PDFë¥¼ ì²˜ë¦¬í•˜ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        print(f"\n{'â”€'*60}")
        print("ğŸ“Œ 1ë‹¨ê³„: PDF ë¬¸ì„œ ì²˜ë¦¬")
        print(f"{'â”€'*60}")
        
        if not self.needs_processing():
            logger.info("ì´ì „ì— ì²˜ë¦¬ëœ ë™ì¼í•œ PDF íŒŒì¼ ê°ì§€. ë³€ê²½ ì—†ìŒìœ¼ë¡œ íŒë‹¨.")
            print("âœ“ ì´ë¯¸ ì²˜ë¦¬ëœ PDF íŒŒì¼ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ì²˜ë¦¬ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.")
            # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš©í•˜ë„ë¡ í•¨
            return []
        
        logger.info("===== PDF ì²˜ë¦¬ ì‹œì‘ =====")
        text_docs = self.extract_text()
        table_docs = self.extract_tables()
        all_docs = text_docs + table_docs
        
        if not all_docs:
            print("âš ï¸ PDFì—ì„œ ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []
        
        # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ë©´ í˜„ì¬ í•´ì‹œ ì €ì¥
        self._save_current_hash()
        logger.info(f"ğŸ“š {len(all_docs)}ê°œì˜ ë¬¸ì„œ ì¡°ê° ìƒì„±ë¨")
        print(f"ğŸ“š PDF ì²˜ë¦¬ ì™„ë£Œ: {len(text_docs)}ê°œ í…ìŠ¤íŠ¸ ë¬¸ì„œ, {len(table_docs)}ê°œ í‘œ ë¬¸ì„œ ìƒì„±")
        return all_docs
    
    def visualize_table(self, table_id: int):
        """íŠ¹ì • í‘œë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤ (matplotlib ì‚¬ìš©)"""
        if not self.tables:
            print("âš ï¸ í‘œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìœ íš¨í•œ table_id í™•ì¸
        table_index = table_id - 1
        if table_index < 0 or table_index >= len(self.tables):
            print(f"âš ï¸ í‘œ #{table_id}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        try:
            table_data = self.tables[table_index]
            df = pd.DataFrame(table_data["raw_data"])
            
            # í‘œ ì‹œê°í™”
            fig, ax = plt.figure(figsize=(12, 6)), plt.gca()
            ax.axis('off')
            ax.table(
                cellText=df.values,
                colLabels=df.columns,
                cellLoc='center',
                loc='center'
            )
            plt.title(f"í‘œ {table_data['table_id']}", fontsize=14)
            plt.tight_layout()
            plt.show()
            
            # í…Œì´ë¸” ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“Š í‘œ {table_data['table_id']} ì •ë³´:")
            print(f"  - í–‰ ìˆ˜: {df.shape[0]}")
            print(f"  - ì—´ ìˆ˜: {df.shape[1]}")
            print(f"  - ì—´ ì´ë¦„: {', '.join(df.columns)}")
            
        except Exception as e:
            print(f"âŒ í‘œ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

class DocumentSplitter:
    def __init__(self, chunk_size=1000, chunk_overlap=300):  # ì²­í¬ ì‚¬ì´ì¦ˆ ì¦ê°€ (500 â†’ 800), ê²¹ì¹¨ í¬ê¸° ì¦ê°€ (150 â†’ 200)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            # êµ¬ë¶„ì ìµœì í™”: ë¬¸ë‹¨ > ë¬¸ì¥ > êµ¬ë‘ì  > ê³µë°± ìˆœì„œë¡œ ì‹œë„
            separators=[
                "\n\n",  # ë¬¸ë‹¨ êµ¬ë¶„
                "\n",    # ì¤„ë°”ê¿ˆ
                ".",     # ë¬¸ì¥ ë
                "!",     # ê°íƒ„ë¬¸
                "?",     # ì˜ë¬¸ë¬¸
                ";",     # ì„¸ë¯¸ì½œë¡ 
                ":",     # ì½œë¡ 
                ",",     # ì‰¼í‘œ
                " ",     # ê³µë°±
                ""       # ë§ˆì§€ë§‰ ìˆ˜ë‹¨
            ]
        )
        logger.info(f"DocumentSplitter ì´ˆê¸°í™”: ì²­í¬ í¬ê¸°={chunk_size}, ê²¹ì¹¨={chunk_overlap}")
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        logger.info(f"ğŸ”ª ë¬¸ì„œ ë¶„í•  ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
        print("ğŸ”ª ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  ì¤‘...")
        try:
            chunks = self.text_splitter.split_documents(documents)
            for i, chunk in enumerate(chunks):
                chunk.metadata['chunk_id'] = f"chunk_{i}"
            logger.info(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")
            return chunks
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜: {e}")
            return documents

# --- ì¿¼ë¦¬ í™•ì¥ê¸° í´ë˜ìŠ¤ ---
class QueryExpander:
    """ì¿¼ë¦¬ í™•ì¥ í´ë˜ìŠ¤"""
    def __init__(self, model_name: str = None):
        """QueryExpander ì´ˆê¸°í™”"""
        # ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€
        if hasattr(QueryExpander, '_instance'):
            self.model = QueryExpander._instance.model
            self.models = QueryExpander._instance.models
            logger.info(f"QueryExpander ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©: {self.model}")
            return
        
        # ëª¨ë¸ ì„¤ì •
        self.models = check_ollama_models()
        
        # ëª¨ë¸ ìë™ ì„ íƒ ë˜ëŠ” ì§€ì •ëœ ëª¨ë¸ ì‚¬ìš©
        if model_name and model_name in self.models:
            self.model = model_name
            logger.info(f"QueryExpander ëª¨ë¸ ì§€ì •: {model_name}")
        elif self.models:
            self.model = self.models[0]
            logger.info(f"QueryExpander ëª¨ë¸ ìë™ ì„ íƒ: {self.model}")
        else:
            self.model = "gemma3:7b" # ê¸°ë³¸ ëª¨ë¸
            logger.warning(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ì–´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {self.model}")
        
        # ì¸ìŠ¤í„´ìŠ¤ ì €ì¥
        QueryExpander._instance = self
    
    def expand_query(self, query: str, max_queries: int = 5) -> List[str]:
        """
        ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ì–‘í•œ ê²€ìƒ‰ìš© ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        - ì›ë³¸ ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¬êµ¬ì„±í•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ í™•ì¥
        - ë°˜í™˜ê°’ì€ ì›ë³¸ ì¿¼ë¦¬ë¥¼ í¬í•¨í•œ í™•ì¥ ì¿¼ë¦¬ ëª©ë¡
        """
        logger.info(f"ğŸ” ì¿¼ë¦¬ í™•ì¥ ì‹œì‘: '{query}'")
        print(f"ğŸ” ì¿¼ë¦¬ í™•ì¥ ìƒì„± ì¤‘: '{query}'")
        
        try:
            if not query.strip():
                logger.warning("ë¹ˆ ì¿¼ë¦¬ëŠ” í™•ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return [query]
            
            # ì—”í‹°í‹° íƒ€ì… ê°ì§€ (ì¸ë¬¼, ìƒí’ˆ, ê¸°ìˆ  ë“±)
            entity_type, entity_name = self._detect_entity(query)
            
            # íŠ¹ìˆ˜ ì—”í‹°í‹°ê°€ ê°ì§€ëœ ê²½ìš°
            if entity_type and entity_name:
                logger.info(f"íŠ¹ìˆ˜ ì—”í‹°í‹° ê°ì§€: ìœ í˜•={entity_type}, ì´ë¦„='{entity_name}'")
                print(f"ğŸ” {entity_type} ê²€ìƒ‰ ê°ì§€: '{entity_name}'")
                
                # ì—”í‹°í‹° ìœ í˜•ë³„ íŠ¹í™” ì¿¼ë¦¬ ìƒì„±
                return self._generate_entity_specific_queries(entity_type, entity_name, query, max_queries)
            
            # ì¼ë°˜ ì¿¼ë¦¬ í™•ì¥ í”„ë¡¬í”„íŠ¸
            prompt = f"""
ë‹¹ì‹ ì€ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ëŠ” RAG ì‹œìŠ¤í…œì˜ ì¿¼ë¦¬ í™•ì¥ê¸°ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ì—ì„œ ë‹µì„ ì°¾ê¸° ìœ„í•œ ê´€ë ¨ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ìƒì„±í•´ ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: "{query}"

ì¤‘ìš”: ë°˜ë“œì‹œ ì›ë³¸ ì§ˆë¬¸ê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì´ ìˆëŠ” ì¿¼ë¦¬ë§Œ ìƒì„±í•˜ì„¸ìš”. 
ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ì ì¸ ì¿¼ë¦¬ëŠ” ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë³€í˜•í•´ ì£¼ì„¸ìš”:
1. ë™ì˜ì–´ë‚˜ ìœ ì‚¬ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ê¸°
2. êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ ë³€í™˜í•˜ê¸°
3. ê´€ë ¨ëœ ì„¸ë¶€ ê°œë…ìœ¼ë¡œ í™•ì¥í•˜ê¸° (ë‹¨, ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ìœ ì§€í•´ì•¼ í•¨)
4. ì§ˆë¬¸í˜•/í‚¤ì›Œë“œí˜• ë“± ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•˜ê¸°

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ ì£¼ì„¸ìš”:
```json
{{
  "queries": [
    "ì²« ë²ˆì§¸ ì¿¼ë¦¬",
    "ë‘ ë²ˆì§¸ ì¿¼ë¦¬",
    "ì„¸ ë²ˆì§¸ ì¿¼ë¦¬",
    ...
  ]
}}
```
JSON í˜•ì‹ë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
ìµœëŒ€ {max_queries}ê°œì˜ ê´€ë ¨ì„± ë†’ì€ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
"""
            
            # Ollama APIë¥¼ í†µí•´ ì¿¼ë¦¬ í™•ì¥ ìƒì„±
            response_text = self._generate_query_expansion(prompt)
            
            # JSON íŒŒì‹±
            try:
                # JSON í˜•ì‹ ê²€ìƒ‰
                json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(1)
                
                # ì¤‘ê´„í˜¸ ê¸°ë°˜ JSON ì¶”ì¶œ
                json_match = re.search(r'{.*}', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(0)
                
                # JSON íŒŒì‹±
                response = json.loads(response_text)
                
                # ì¿¼ë¦¬ ëª©ë¡ ì¶”ì¶œ
                if "queries" in response:
                    expanded_queries = response["queries"]
                    
                    # ì¤‘ë³µ ì œê±° ë° ë¹ˆ ì¿¼ë¦¬ ì œê±°
                    expanded_queries = [q.strip() for q in expanded_queries if q.strip()]
                    
                    # ê´€ë ¨ì„± í•„í„°ë§ - ì›ë³¸ ì¿¼ë¦¬ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¿¼ë¦¬ë§Œ ìœ ì§€
                    keywords = self._extract_keywords(query)
                    if keywords:
                        # ì ì–´ë„ í•˜ë‚˜ì˜ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¿¼ë¦¬ë§Œ ìœ ì§€
                        filtered_queries = []
                        for q in expanded_queries:
                            if any(keyword.lower() in q.lower() for keyword in keywords):
                                filtered_queries.append(q)
                        expanded_queries = filtered_queries
                    
                    expanded_queries = list(dict.fromkeys(expanded_queries))
                    
                    # ì›ë³¸ ì¿¼ë¦¬ê°€ ëª©ë¡ì— ì—†ìœ¼ë©´ ì¶”ê°€
                    if query not in expanded_queries:
                        expanded_queries.insert(0, query)
                    
                    # ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜ ì œí•œ
                    expanded_queries = expanded_queries[:max_queries]
                    
                    logger.info(f"âœ… ì¿¼ë¦¬ í™•ì¥ ì™„ë£Œ: {len(expanded_queries)}ê°œ ìƒì„±")
                    logger.info(f"í™•ì¥ ì¿¼ë¦¬: {expanded_queries}")
                    print(f"âœ… í™•ì¥ ì¿¼ë¦¬ {len(expanded_queries)}ê°œ ìƒì„± ì™„ë£Œ")
                    return expanded_queries
                else:
                    logger.warning("ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼ì— 'queries' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return [query]
            
            except json.JSONDecodeError as e:
                logger.error(f"ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                logger.error(f"ì›ë³¸ ì‘ë‹µ: {response_text}")
                return [query]
        
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(traceback.format_exc())
            return [query]
    
    def _detect_entity(self, query: str) -> Tuple[Optional[str], Optional[str]]:
        """ì¿¼ë¦¬ì—ì„œ íŠ¹ìˆ˜ ì—”í‹°í‹°(ì¸ë¬¼, ìƒí’ˆ, ê¸°ìˆ  ë“±)ë¥¼ ê°ì§€í•©ë‹ˆë‹¤."""
        # 1. í•œêµ­ ì¸ëª… + ì¡°ì‚¬ + ì˜ë¬¸ì‚¬ íŒ¨í„´ ê°œì„ 
        # í•œêµ­ ì¸ëª…ì€ ë³´í†µ 2-3ê¸€ìì´ë©° ì¡°ì‚¬(ì€,ëŠ”,ì´,ê°€)ê°€ ë°”ë¡œ ë¶™ìŒ
        korean_name_pattern = r'^([ê°€-í£]{2,3})(ì€|ëŠ”|ì´|ê°€)\s+(?:ëˆ„êµ¬|ëˆ„êµ¬ëƒ|ëˆ„êµ°ê°€|ëˆ„êµ¬ì¸ê°€|ì–´ë–¤|ì‚¬ëŒ|ì¸ë¬¼|ì„ì›|ì§ì›)'
        korean_name_match = re.search(korean_name_pattern, query)
        if korean_name_match:
            name = korean_name_match.group(1).strip()  # ì¡°ì‚¬ë¥¼ ì œì™¸í•œ ì´ë¦„ë§Œ ì¶”ì¶œ
            logger.info(f"í•œêµ­ ì¸ëª… ê°ì§€: '{name}' (ì¡°ì‚¬: '{korean_name_match.group(2)}')")
            return "ì¸ë¬¼", name
            
        # 2. í•œêµ­ ì¸ëª… + ì˜ë¬¸ëŒ€ëª…ì‚¬ íŒ¨í„´ (ì¡°ì‚¬ ì—†ì´)
        # ì˜ˆ: "ë¬¸ìˆ˜ì§„ ëˆ„êµ¬ì•¼", "ê¹€ì² ìˆ˜ ëˆ„êµ¬"
        korean_name_simple_pattern = r'^([ê°€-í£]{2,3})\s+(?:ëˆ„êµ¬|ëˆ„êµ¬ì•¼|ëˆ„êµ¬ëƒ|ëˆ„êµ¬ì¸ì§€|ë­|ì–´ë–¤)'
        korean_name_simple_match = re.search(korean_name_simple_pattern, query)
        if korean_name_simple_match:
            name = korean_name_simple_match.group(1).strip()
            logger.info(f"í•œêµ­ ì¸ëª… ê°ì§€ (ì¡°ì‚¬ ì—†ìŒ): '{name}'")
            return "ì¸ë¬¼", name
        
        # 3. ì´ë¦„ + ì¡°ì‚¬ë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: "ë¬¸ìˆ˜ì§„ì€", "ê¹€ì² ìˆ˜ê°€")
        # ì¼ë°˜ì ìœ¼ë¡œ "~ì€" í˜•íƒœë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ì€ í•´ë‹¹ ì¸ë¬¼ì— ëŒ€í•œ ì§ˆë¬¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        korean_name_suffix_pattern = r'^([ê°€-í£]{2,3})(ì€|ëŠ”|ì´|ê°€)(?:\s|$)'
        korean_name_suffix_match = re.search(korean_name_suffix_pattern, query)
        if korean_name_suffix_match:
            name = korean_name_suffix_match.group(1).strip()
            logger.info(f"í•œêµ­ ì¸ëª… ê°ì§€ (ì¡°ì‚¬ë¡œ ì¢…ë£Œ): '{name}'")
            return "ì¸ë¬¼", name
        
        # 4. ì˜ì–´ ì¸ëª… íŒ¨í„´ (ê¸°ì¡´ ì½”ë“œ)
        english_person_pattern = r'^([a-zA-Z\s]{2,20})(?:ì€|ëŠ”|ì´|ê°€)?\s+(?:ëˆ„êµ¬|ì–´ë–¤|ì–´ëŠ|ë¬´ìŠ¨)'
        english_person_match = re.search(english_person_pattern, query)
        if english_person_match:
            name = english_person_match.group(1).strip()
            return "ì¸ë¬¼", name
            
        # 5. ìƒí’ˆ/ì„œë¹„ìŠ¤ ê°ì§€
        product_pattern = r'(?:ìƒí’ˆ|ì„œë¹„ìŠ¤|ë³´í—˜)\s+([ê°€-í£a-zA-Z0-9\s]{2,20})(?:ì´|ê°€|ì€|ëŠ”)?\s+(?:ë¬´ì—‡|ë­|ì–´ë–¤|ì–´ë–»ê²Œ)'
        product_match = re.search(product_pattern, query)
        if product_match:
            product = product_match.group(1).strip()
            return "ìƒí’ˆ", product
            
        # 6. ê¸°ìˆ /ê¸°ëŠ¥ ê°ì§€
        tech_pattern = r'(?:ê¸°ìˆ |ê¸°ëŠ¥|ì‹œìŠ¤í…œ|í”Œë«í¼)\s+([ê°€-í£a-zA-Z0-9\s]{2,20})(?:ì´|ê°€|ì€|ëŠ”)?\s+(?:ë¬´ì—‡|ë­|ì–´ë–¤|ì–´ë–»ê²Œ)'
        tech_match = re.search(tech_pattern, query)
        if tech_match:
            tech = tech_match.group(1).strip()
            return "ê¸°ìˆ ", tech
            
        # 7. ì¸ëª…ë§Œ ìˆëŠ” ê²½ìš° (ì§ì ‘ ì–¸ê¸‰)
        # í•œê¸€ ì´ë¦„ì€ 2-3ê¸€ìë¡œ ì œí•œ
        name_only_pattern = r'^([ê°€-í£]{2,3}|[a-zA-Z\s]{2,20})$'
        name_only_match = re.search(name_only_pattern, query)
        if name_only_match:
            name = name_only_match.group(1).strip()
            return "ì¸ë¬¼", name
            
        # ê°ì§€ëœ ì—”í‹°í‹°ê°€ ì—†ëŠ” ê²½ìš°
        return None, None
            
    def _extract_keywords(self, query: str) -> List[str]:
        """ì›ë³¸ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {"ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ì˜", "ê³¼", "ì™€", "ë¡œ", "ìœ¼ë¡œ", 
                    "ì´ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "í•˜ë‹¤", "ë˜ë‹¤", "í•œë‹¤", "ëœë‹¤", "ë¬´ì—‡", "ëˆ„êµ¬", "ì–´ë””", 
                    "ì–¸ì œ", "ì™œ", "ì–´ë–»ê²Œ", "ì–´ë–¤", "ì–¼ë§ˆë‚˜", "ëª‡", "ë¬´ìŠ¨"}
        
        # ì¡°ì‚¬ì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        cleaned_query = re.sub(r'[^\w\s]', ' ', query)
        words = cleaned_query.split()
        
        # ê¸¸ì´ê°€ 1ì¸ ë‹¨ì–´ì™€ ë¶ˆìš©ì–´ ì œê±°
        keywords = [word for word in words if len(word) > 1 and word not in stopwords]
        return keywords
    
    def _generate_entity_specific_queries(self, entity_type: str, entity_name: str, original_query: str, max_queries: int) -> List[str]:
        """íŠ¹ì • ì—”í‹°í‹° ìœ í˜•ì— íŠ¹í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        logger.info(f"'{entity_name}'({entity_type})ì— ëŒ€í•œ íŠ¹í™” ì¿¼ë¦¬ ìƒì„± ì¤‘")
        
        # ê¸°ë³¸ ì¿¼ë¦¬ëŠ” í•­ìƒ ì›ë³¸ ì¿¼ë¦¬ë¡œ ì‹œì‘
        result_queries = [original_query]
        
        # ì—”í‹°í‹° ìœ í˜•ë³„ íŠ¹í™” ì¿¼ë¦¬ ìƒì„±
        if entity_type == "ì¸ë¬¼":
            # í•œí™”ì†í•´ë³´í—˜ ë‚´ ì§ì±… ê´€ë ¨ ì¿¼ë¦¬ í™•ì¥
            roles = [
                "ëŒ€í‘œì´ì‚¬", "ì‚¬ì¥", "ë¶€ì‚¬ì¥", "ì „ë¬´", "ìƒë¬´", "ì´ì‚¬", "ê°ì‚¬", 
                "ë³¸ë¶€ì¥", "ì‹¤ì¥", "íŒ€ì¥", "ë¶€ì¥", "ì°¨ì¥", "ê³¼ì¥", "ëŒ€ë¦¬", "ì‚¬ì›",
                "ì„ì›", "ì§‘í–‰ì„ì›", "ì‚¬ì™¸ì´ì‚¬", "ê¸°íƒ€ë¹„ìƒë¬´ì´ì‚¬", "íšŒì¥", "ë¶€íšŒì¥", 
                "ìˆ˜ì„ë¶€ì‚¬ì¥", "ìˆ˜ì„ì „ë¬´", "ìƒì„ê°ì‚¬", "CIO", "CFO", "CRO", "CEO", 
                "COO", "CTO", "ìœ„ì›ì¥", "ì‚¬ë‚´ì´ì‚¬", "ê²½ì˜ì§„", "ì±…ì„ì", "ë‹´ë‹¹ì"
            ]
            
            # ì¸ë¬¼ ê²€ìƒ‰ì„ ìœ„í•œ ë™ì‚¬ ë° ìˆ˜ì‹ì–´
            actions = [
                "ë‹´ë‹¹", "ì±…ì„", "ë§¡ì€", "ì—­í• ", "ì§ë¬´", "ì—…ë¬´", "ì§ì±…", "ì†Œê°œ", "ì´ë ¥", 
                "ê²½ë ¥", "í”„ë¡œí•„", "ì•½ë ¥", "ê²½í—˜", "ì´ì „ ê²½ë ¥", "ì „ë¬¸ ë¶„ì•¼", "ì‹¤ì ", 
                "ì„±ê³¼", "ê³µí—Œ", "ê¸°ì—¬", "ë°œí‘œ", "ì—°ì„¤", "ì¸í„°ë·°", "ë‹´í™”", "ë³´ê³ ì„œ",
                "ì´ë ¥ì„œ", "í•™ë ¥", "ì „ê³µ", "ì¶œì‹ ", "ìƒë…„ì›”ì¼", "ë‚˜ì´", "ì´ë©”ì¼", "ì—°ë½ì²˜"
            ]
            
            # ë¶€ì„œ ë˜ëŠ” íŒ€ ê´€ë ¨ í‚¤ì›Œë“œ
            departments = [
                "ê²½ì˜ê¸°íš", "ì¬ë¬´", "íšŒê³„", "ë§ˆì¼€íŒ…", "ì˜ì—…", "íŒë§¤", "ë¦¬ìŠ¤í¬", "ì¤€ë²•", 
                "ê°ì‚¬", "ì „ëµ", "ë””ì§€í„¸", "IT", "ì¸ì‚¬", "ì´ë¬´", "ë²•ë¬´", "í™ë³´", "íˆ¬ì", 
                "ë³´í—˜ê³„ë¦¬", "ì†í•´ì‚¬ì •", "ì–¸ë”ë¼ì´íŒ…", "ë³´ìƒ", "ê³ ê°ì„œë¹„ìŠ¤", "ì—°êµ¬ì†Œ",
                "ìì‚°ìš´ìš©", "ë””ì§€í„¸í˜ì‹ ", "AI", "ë¹…ë°ì´í„°", "ESG", "í•´ì™¸ì‚¬ì—…"
            ]
            
            # ê¸°ë³¸ ì¸ë¬¼ ì¿¼ë¦¬
            basic_queries = [
                f"{entity_name}",
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜",
                f"í•œí™”ì†í•´ë³´í—˜ {entity_name}"
            ]
            
            # ì§ì±… ê¸°ë°˜ ì¿¼ë¦¬
            role_queries = [f"{role} {entity_name}" for role in roles[:10]]
            role_queries.extend([f"{entity_name} {role}" for role in roles[10:20]])
            
            # ë¶€ì„œ ê¸°ë°˜ ì¿¼ë¦¬
            dept_queries = [f"{entity_name} {dept}" for dept in departments[:10]]
            
            # ì§ë¬´/ì—­í•  ê¸°ë°˜ ì¿¼ë¦¬
            action_queries = [f"{entity_name} {action}" for action in actions[:10]]
            
            # ê²°í•© ì¿¼ë¦¬ (ë” êµ¬ì²´ì ì¸ ê²€ìƒ‰)
            combined_queries = [
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜ í”„ë¡œí•„",
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜ ì´ë ¥",
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜ ì§ì±…",
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜ ê²½ë ¥",
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜ ì—…ë¬´",
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜ ë‹´ë‹¹"
            ]
            
            # ëª¨ë“  ì¿¼ë¦¬ ê²°í•©
            result_queries.extend(basic_queries)
            result_queries.extend(role_queries)
            result_queries.extend(dept_queries)
            result_queries.extend(action_queries)
            result_queries.extend(combined_queries)
            
        elif entity_type == "ìƒí’ˆ":
            specific_queries = [
                f"{entity_name}",
                f"{entity_name} ìƒí’ˆ",
                f"{entity_name} ë³´í—˜",
                f"{entity_name} ì„œë¹„ìŠ¤",
                f"{entity_name} íŠ¹ì§•",
                f"{entity_name} ì„¤ëª…",
                f"{entity_name} ì†Œê°œ",
                f"{entity_name} ì¥ì ",
                f"{entity_name} ì¡°ê±´",
                f"{entity_name} ê³„ì•½",
                f"í•œí™”ì†í•´ë³´í—˜ {entity_name}",
                f"{entity_name} ë³´ì¥",
                f"{entity_name} ë³´í—˜ë£Œ",
                f"{entity_name} íŒë§¤",
                f"{entity_name} ê°€ì…"
            ]
            result_queries.extend(specific_queries)
            
        elif entity_type == "ê¸°ìˆ ":
            specific_queries = [
                f"{entity_name}",
                f"{entity_name} ê¸°ìˆ ",
                f"{entity_name} ì‹œìŠ¤í…œ",
                f"{entity_name} í”Œë«í¼",
                f"{entity_name} ì ìš©",
                f"{entity_name} í™œìš©",
                f"{entity_name} êµ¬í˜„",
                f"{entity_name} ë„ì…",
                f"{entity_name} íŠ¹ì§•",
                f"{entity_name} íš¨ê³¼",
                f"í•œí™”ì†í•´ë³´í—˜ {entity_name}",
                f"{entity_name} ê°œë°œ",
                f"{entity_name} í˜ì‹ ",
                f"{entity_name} íˆ¬ì"
            ]
            result_queries.extend(specific_queries)
            
        else:
            # ê¸°íƒ€ ì—”í‹°í‹° ìœ í˜•ì— ëŒ€í•œ ì¼ë°˜ì ì¸ í™•ì¥
            specific_queries = [
                f"{entity_name}",
                f"{entity_name} í•œí™”ì†í•´ë³´í—˜",
                f"í•œí™”ì†í•´ë³´í—˜ {entity_name}",
                f"{entity_name} ì„¤ëª…",
                f"{entity_name} ë‚´ìš©",
                f"{entity_name} ì†Œê°œ",
                f"{entity_name} ì •ë³´"
            ]
            result_queries.extend(specific_queries)
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        result_queries = list(dict.fromkeys(result_queries))[:max_queries]
        
        logger.info(f"{entity_type} íŠ¹í™” ì¿¼ë¦¬ {len(result_queries)}ê°œ ìƒì„± ì™„ë£Œ")
        print(f"âœ… '{entity_name}'({entity_type})ì— ê´€í•œ íŠ¹í™” ì¿¼ë¦¬ {len(result_queries)}ê°œ ìƒì„±")
        
        return result_queries
        
    def _generate_query_expansion(self, prompt: str) -> str:
        """Ollama APIë¥¼ í†µí•´ ì¿¼ë¦¬ í™•ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            url = "http://localhost:11434/api/generate"
            headers = {"Content-Type": "application/json"}
            data = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9
                }
            }
            
            logger.info(f"Ollama API í˜¸ì¶œ: ëª¨ë¸={self.model}")
            response = requests.post(url, headers=headers, json=data)
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "")
            else:
                logger.error(f"Ollama API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                return ""
        
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ í™•ì¥ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return ""

# --- RAG ì‹œìŠ¤í…œ (ë²¡í„° ê²€ìƒ‰) ---
class RAGSystem:
    """ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    def __init__(self, embedding_type: str = "bge-m3", use_hnsw: bool = True, ef_search: int = 200, ef_construction: int = 200, m: int = 64):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        print(f"  - ì„ë² ë”© ëª¨ë¸: {embedding_type}")
        
        # ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ë³€ìˆ˜
        if hasattr(RAGSystem, '_initialized'):
            logger.info("RAG ì‹œìŠ¤í…œì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        # ì´ˆê¸°í™” ì™„ë£Œ í‘œì‹œ
        RAGSystem._initialized = True
        
        # ì„ë² ë”© ëª¨ë¸ ìœ í˜• ì €ì¥
        self.embedding_type = embedding_type
        self.embedding_name = None
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        if embedding_type == "bge-m3":
            self.embeddings = BGEM3Embeddings(model_name="BAAI/bge-m3")
            self.embedding_name = "bge-m3"
        elif LANGCHAIN_AVAILABLE and embedding_type == "bge":
            # BGE-base ì„ë² ë”© ì‚¬ìš© (HuggingFaceEmbeddings í•„ìš”)
            print("âœ“ BGE-base ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            self.embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")
            self.embedding_name = "bge"
        else:
            # ê¸°ë³¸ìœ¼ë¡œ BGE-M3 ì„ë² ë”© ì‚¬ìš© (embedding_typeì´ "bge-m3"ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ê°’ì¸ ê²½ìš°ì—ë§Œ)
            if embedding_type != "bge-m3":
                print("âœ“ BGE-M3 ì„ë² ë”© ëª¨ë¸ 'BAAI/bge-m3' ì´ˆê¸°í™” ì¤‘ (ê¸°ë³¸ê°’)")
                self.embeddings = BGEM3Embeddings(model_name="BAAI/bge-m3")
                self.embedding_name = "bge-m3"
                self.embedding_type = "bge-m3"  # íƒ€ì… ì—…ë°ì´íŠ¸
            else:
                # ì´ë¯¸ "bge-m3"ë¡œ ì´ˆê¸°í™”ëœ ê²½ìš° ê±´ë„ˆëœë‹ˆë‹¤ (ì²« ë²ˆì§¸ if ì¡°ê±´ì—ì„œ ì²˜ë¦¬ë¨)
                pass

        # ì‘ë‹µ ìºì‹œ ì´ˆê¸°í™”
        self._cache = self._load_cache()
        
        # HNSW ì¸ë±ìŠ¤ ì˜µì…˜
        self.use_hnsw = use_hnsw
        self.ef_search = ef_search
        self.ef_construction = ef_construction
        self.m = m
        
        # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        self.vector_store = None
                
        # ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.index_dir = INDEX_DIR
        os.makedirs(self.index_dir, exist_ok=True)
        
        # Ollama REST API ì„¤ì •
        self.ollama_base_url = "http://localhost:11434/api"
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡
        self.available_models = check_ollama_models()
        
        if self.available_models:
            selected_model = self.available_models[0]
            print(f"âœ“ ì‚¬ìš©í•  ëª¨ë¸: {selected_model}")
            
            # ì¿¼ë¦¬ í™•ì¥ê¸° ì´ˆê¸°í™”
            self.query_expander = QueryExpander(model_name=selected_model)
            logger.info(f"QueryExpander ëª¨ë¸ ìë™ ì„ íƒ: {selected_model}")
            print(f"âœ“ ì¿¼ë¦¬ í™•ì¥ì— {selected_model} ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            print(f"  - ì¿¼ë¦¬ í™•ì¥: í™œì„±í™”ë¨")
        else:
            self.query_expander = None
            print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ì–´ ì¿¼ë¦¬ í™•ì¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            print(f"  - ì¿¼ë¦¬ í™•ì¥: ë¹„í™œì„±í™”ë¨")
        
        print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_cache(self) -> Dict[str, str]:
        """ìºì‹œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        cache_file = os.path.join(SCRIPT_DIR, "cache.json")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {}
    
    def _save_cache(self):
        """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        cache_file = os.path.join(SCRIPT_DIR, "cache.json")
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self._cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _generate_with_ollama(self, prompt: str, model: str = "gemma3:1b", stream: bool = True, max_retries: int = 3) -> str:
        """Ollama APIë¥¼ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            api_url = f"{self.ollama_base_url}/generate"
            
            # API ìš”ì²­ JSON
            api_json = {
                "model": model,
                "prompt": prompt,
                "stream": stream,
                "options": {
                    "temperature": 0.1,  # ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •í•˜ì—¬ ë” ê²°ì •ì ì¸ ì‘ë‹µ ìƒì„±
                    "top_p": 0.1,        # top_pë„ ë‚®ê²Œ ì„¤ì •í•˜ì—¬ ë” ê²°ì •ì ì¸ ì‘ë‹µ ìƒì„±
                    "top_k": 10          # top_kë„ ë‚®ê²Œ ì„¤ì •
                }
            }
            
            # ì¶”ê°€ ì˜µì…˜ ì„¤ì •
            if "llama3" in model or "gemma3" in model:
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ (ë¼ë§ˆ3, ì ¬ë§ˆ3 ëª¨ë¸ìš©)
                api_json["system"] = """ë‹¹ì‹ ì€ ì •í™•ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ëŠ” ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•­ìƒ ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ë§Œì•½ ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ ê·¸ ì‚¬ì‹¤ì„ ì•Œë ¤ì•¼ í•©ë‹ˆë‹¤. ì •í™•í•œ ì •ë³´ë§Œ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ í•µì‹¬ ì›ì¹™ì…ë‹ˆë‹¤."""
            
            logger.info(f"Ollama API í˜¸ì¶œ: ëª¨ë¸={model}")
            
            # HTTP ìš”ì²­ ì „ì†¡
            current_retry = 0
            while current_retry < max_retries:
                try:
                    response = requests.post(api_url, json=api_json, timeout=60)
                    break
                except requests.exceptions.RequestException as e:
                    logger.error(f"Ollama API ìš”ì²­ ì‹¤íŒ¨ ({current_retry+1}/{max_retries}): {e}")
                    current_retry += 1
                    if current_retry < max_retries:
                        time.sleep(1)  # ì¬ì‹œë„ ì „ 1ì´ˆ ëŒ€ê¸°
                    else:
                        raise ConnectionError(f"Ollama API ì—°ê²° ì‹¤íŒ¨: {e}")
            
            # ì‘ë‹µ ì²˜ë¦¬
            response_text = ""
            if response.status_code == 200:
                if stream:
                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                    print(f"\n================================================================================")
                    print(f"ğŸ“ ëª¨ë¸: {model}")
                    print(f"================================================================================")
                    
                    for line in response.iter_lines():
                        if line:
                            try:
                                json_response = json.loads(line.decode('utf-8'))
                                chunk = json_response.get("response", "")
                                response_text += chunk
                                # ì¤„ë‹¨ìœ„ ì¶œë ¥
                                print(chunk, end="", flush=True)
                            except json.JSONDecodeError:
                                logger.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜: {line}")
                    
                    print()  # ì¤„ë°”ê¿ˆ ì¶”ê°€
                    logger.debug(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ: ê¸¸ì´={len(response_text)}")
                else:
                    # ë‹¨ì¼ ì‘ë‹µ ì²˜ë¦¬
                    json_response = response.json()
                    response_text = json_response.get("response", "")
                    logger.debug(f"ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ì‘ë‹µ ìƒì„± ì™„ë£Œ: ê¸¸ì´={len(response_text)}")
                
                return response_text
            else:
                error_msg = f"Ollama API ì˜¤ë¥˜: {response.status_code}, {response.text}"
                logger.error(error_msg)
                return f"âš ï¸ {error_msg}"
        
        except Exception as e:
            error_msg = f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            print(f"\nâŒ {error_msg}")
            return f"âš ï¸ {error_msg}"

    def answer(self, query: str, model: str, context: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        try:
            # 1. ìºì‹œ í™•ì¸
            cache_key = f"{model}:{hashlib.md5((query + context[:100]).encode()).hexdigest()}"
            cached_answer = self._cache.get(cache_key)
            
            if cached_answer and not os.environ.get('DISABLE_CACHE'):
                logger.info(f"ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©: ëª¨ë¸={model}")
                print(f"ğŸ’¾ ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©: {model}")
                return {
                    "answer": cached_answer,
                    "model": model,
                    "cached": True
                }
            
            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
            try:
                # ì˜¤ëŠ˜ ë‚ ì§œ ì •ë³´ ì¶”ê°€
                today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
                
                # ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì§€ëŠ¥ì ìœ¼ë¡œ ìë¥´ê¸°
                max_context_length = 10000  # í† í° í•œë„ë¥¼ ê³ ë ¤í•œ ê¸¸ì´ ì œí•œ
                if len(context) > max_context_length:
                    logger.warning(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì´ˆê³¼: {len(context)}ì -> {max_context_length}ìë¡œ ì œí•œ")
                    print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(context)}ì). {max_context_length}ìë¡œ ì œí•œí•©ë‹ˆë‹¤.")
                    
                    # ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
                    sections = context.split("\n\n")
                    
                    # ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì •ë ¬
                    scored_sections = []
                    for section in sections:
                        # ì œëª© ì„¹ì…˜ì€ í•­ìƒ ìœ ì§€
                        if section.startswith("ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ"):
                            score = float('inf')  # ìµœê³  ì ìˆ˜ë¡œ ì„¤ì •
                        elif section.startswith("[ë¬¸ì„œ #"):
                            score = float('inf') - 1  # ë¬¸ì„œ ì œëª©ë„ ë†’ì€ ì ìˆ˜
                        else:
                            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                            query_terms = set(re.findall(r'\w+', query.lower()))
                            section_text = section.lower()
                            score = sum(1 for term in query_terms if term in section_text)
                        
                        scored_sections.append((section, score))
                    
                    # ì ìˆ˜ë³„ ì •ë ¬ (ë†’ì€ ê²ƒë¶€í„°)
                    sorted_sections = sorted(scored_sections, key=lambda x: x[1], reverse=True)
                    
                    # ì»¨í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
                    trimmed_context = ""
                    current_length = 0
                    
                    # ë†’ì€ ì ìˆ˜ì˜ ì„¹ì…˜ë¶€í„° ì¶”ê°€
                    for section, _ in sorted_sections:
                        # ì œëª© ì„¹ì…˜ì€ í•­ìƒ í¬í•¨
                        if section.startswith("ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ") or section.startswith("[ë¬¸ì„œ #"):
                            trimmed_context += section + "\n\n"
                            continue
                            
                        # ê¸¸ì´ ì œí•œ í™•ì¸
                        if current_length + len(section) <= max_context_length:
                            trimmed_context += section + "\n\n"
                            current_length += len(section) + 2  # ì¤„ë°”ê¿ˆ í¬í•¨
                        else:
                            # ì œí•œ ë„ë‹¬ ì‹œ ì¢…ë£Œ
                            break
                    
                    # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
                    context = trimmed_context.strip()
                
                # í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ íŠ¹í™” QA í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
                qa_template = f"""ë‹¹ì‹ ì€ {today} ê¸°ì¤€ìœ¼ë¡œ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì¤‘ìš” ì •ë³´ ë° ì œì•½ì‚¬í•­
ì•„ë˜ ê·œì¹™ì„ ì² ì €íˆ ì¤€ìˆ˜í•˜ì§€ ì•Šìœ¼ë©´ ì‹¬ê°í•œ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤:

1. ì ˆëŒ€ì ìœ¼ë¡œ ì œê³µëœ ë¬¸ì„œ ì½˜í…ì¸ ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ì œê³µëœ ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìœ¼ë©´ "í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë‹µë³€í•˜ì„¸ìš”.

2. ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ë‚´ìš©ì„ ì ˆëŒ€ë¡œ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. íŠ¹íˆ ë‹¤ìŒ ì£¼ì œëŠ” ë¬¸ì„œì— ì§ì ‘ ì–¸ê¸‰ëœ ê²½ìš°ì—ë§Œ í¬í•¨í•˜ì„¸ìš”:
   - ë””ì§€í„¸ ì „í™˜/ë””ì§€í„¸ ê¸°ìˆ 
   - AI/ì¸ê³µì§€ëŠ¥
   - ë¹…ë°ì´í„°
   - ESG ê²½ì˜
   - í•€í…Œí¬

3. ìˆ«ì, ë‚ ì§œ, ê¸ˆì•¡ ë“± ëª¨ë“  ì‚¬ì‹¤ì  ì •ë³´ëŠ” ë¬¸ì„œì— ìˆëŠ” ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”. ì–´ë–¤ ì •ë³´ë„ ë³€í˜•í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¼ë°˜í™”í•˜ì§€ ë§ˆì„¸ìš”.

4. "~ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤", "~í•  ê²ƒì…ë‹ˆë‹¤", "~í•˜ê³  ìˆìŠµë‹ˆë‹¤"ì™€ ê°™ì€ ì¶”ì¸¡ì„± í‘œí˜„ì€ ë¬¸ì„œì— ì§ì ‘ ê·¸ëŸ° í‘œí˜„ì´ ìˆì§€ ì•Šì€ í•œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

5. ë¬¸ì„œì— ëª…í™•í•˜ê²Œ ë‚˜ì™€ìˆì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”. ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ ê·¸ ì‚¬ì‹¤ì„ ì†”ì§í•˜ê²Œ ì¸ì •í•˜ì„¸ìš”.

## ë‹µë³€ í˜•ì‹ ì§€ì¹¨
1. ë‹µë³€ì€ ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°–ì¶”ê³ , í•­ëª©ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.
2. í‘œ, ê·¸ë˜í”„, ìˆ«ì ë°ì´í„°ëŠ” ì›ë³¸ í˜•ì‹ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ì—¬ í‘œí˜„í•˜ì„¸ìš”.
3. íŠ¹ì • ì¸ë¬¼ì— ëŒ€í•œ ì§ˆë¬¸ì€ ë¬¸ì„œì— ì‹¤ì œë¡œ ë“±ì¥í•˜ëŠ” ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©:
{context}

ìœ„ ì‚¬ì—…ë³´ê³ ì„œ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€:"""
                    
                    return self._generate_with_ollama(qa_template, model, stream=True)
                else:
                    # ë‹¨ì¼ ì‘ë‹µ ì²˜ë¦¬
                    json_response = response.json()
                    response_text = json_response.get("response", "")
                    
                    # í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œ íŠ¹í™” QA í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
                    qa_template = f"""ë‹¹ì‹ ì€ {today} ê¸°ì¤€ìœ¼ë¡œ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì¤‘ìš” ì •ë³´ ë° ì œì•½ì‚¬í•­
ì•„ë˜ ê·œì¹™ì„ ì² ì €íˆ ì¤€ìˆ˜í•˜ì§€ ì•Šìœ¼ë©´ ì‹¬ê°í•œ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤:

1. ì ˆëŒ€ì ìœ¼ë¡œ ì œê³µëœ ë¬¸ì„œ ì½˜í…ì¸ ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ì œê³µëœ ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìœ¼ë©´ "í•´ë‹¹ ë‚´ìš©ì€ ì œê³µëœ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë‹µë³€í•˜ì„¸ìš”.

2. ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ë‚´ìš©ì„ ì ˆëŒ€ë¡œ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. íŠ¹íˆ ë‹¤ìŒ ì£¼ì œëŠ” ë¬¸ì„œì— ì§ì ‘ ì–¸ê¸‰ëœ ê²½ìš°ì—ë§Œ í¬í•¨í•˜ì„¸ìš”:
   - ë””ì§€í„¸ ì „í™˜/ë””ì§€í„¸ ê¸°ìˆ 
   - AI/ì¸ê³µì§€ëŠ¥
   - ë¹…ë°ì´í„°
   - ESG ê²½ì˜
   - í•€í…Œí¬

3. ìˆ«ì, ë‚ ì§œ, ê¸ˆì•¡ ë“± ëª¨ë“  ì‚¬ì‹¤ì  ì •ë³´ëŠ” ë¬¸ì„œì— ìˆëŠ” ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”. ì–´ë–¤ ì •ë³´ë„ ë³€í˜•í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¼ë°˜í™”í•˜ì§€ ë§ˆì„¸ìš”.

4. "~ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤", "~í•  ê²ƒì…ë‹ˆë‹¤", "~í•˜ê³  ìˆìŠµë‹ˆë‹¤"ì™€ ê°™ì€ ì¶”ì¸¡ì„± í‘œí˜„ì€ ë¬¸ì„œì— ì§ì ‘ ê·¸ëŸ° í‘œí˜„ì´ ìˆì§€ ì•Šì€ í•œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

                    break
            
            context = "\n\n".join(optimized_context)
        
        prompt = EVAL_PROMPT_TEMPLATE.format(
            question=question,
            context=context,
            answer=answer
        )
        
        # í‰ê°€ ê²°ê³¼ë¥¼ í•˜ë“œì½”ë”©ëœ ì˜ˆì‹œë¡œ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” LLMì„ í˜¸ì¶œí•´ì•¼ í•¨)
        # ì´ ë¶€ë¶„ì€ ì„ì‹œë¡œ ì¶”ê°€í•˜ì—¬ ì½”ë“œê°€ ë™ì‘í•˜ë„ë¡ í•¨
        example_evaluation = {
            "score": 3,
            "reason": "ë‹µë³€ì´ ê¸°ë³¸ì ì¸ ì •ë³´ëŠ” ì œê³µí•˜ì§€ë§Œ, ì„¸ë¶€ ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.",
            "raw_evaluation": "í‰ê°€ ì „ì²´ í…ìŠ¤íŠ¸"
        }
        
        return example_evaluation


# ì§ˆë¬¸ ìƒì„±ê¸° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
QUESTION_GENERATOR_TEMPLATE = """ë‹¹ì‹ ì€ í•œí™”ì†í•´ë³´í—˜ ì‚¬ì—…ë³´ê³ ì„œì— ê´€í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”:
1. ì¬ë¬´/ì‹¤ì  ê´€ë ¨ ì§ˆë¬¸ (ìˆ˜ìµ, ì†ì‹¤, ì„±ì¥ë¥  ë“±)
2. ì‚¬ì—… ì „ëµ ê´€ë ¨ ì§ˆë¬¸
3. ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê´€ë ¨ ì§ˆë¬¸
4. ì§€ë°°êµ¬ì¡° ê´€ë ¨ ì§ˆë¬¸
5. ìƒí’ˆ/ì„œë¹„ìŠ¤ ê´€ë ¨ ì§ˆë¬¸

[ë¬¸ì„œ ë‚´ìš©]
{context}

ìœ„ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§ˆë¬¸ 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, ë¬¸ì„œ ë‚´ìš©ì—ì„œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”:
["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3", "ì§ˆë¬¸4", "ì§ˆë¬¸5"]"""


class AutoQuestionGenerator:
    def __init__(self, model_name: str = "gemma3:12b"):
        self.model_name = model_name
    
    def generate_questions(self, context: str, stream: bool = True) -> List[str]:
        """ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±"""
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
        max_context_length = 2000
        if len(context) > max_context_length:
            context_parts = context.split("\n\n")
            optimized_context = []
            current_length = 0
            
            for part in context_parts:
                if current_length + len(part) <= max_context_length:
                    optimized_context.append(part)
                    current_length += len(part)
                else:
                    break
            
            context = "\n\n".join(optimized_context)
        
        prompt = QUESTION_GENERATOR_TEMPLATE.format(context=context)
        
        # ì„ì‹œë¡œ í•˜ë“œì½”ë”©ëœ ì§ˆë¬¸ ëª©ë¡ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” LLMì„ í˜¸ì¶œí•´ì•¼ í•¨)
        # ì´ ë¶€ë¶„ì€ ì„ì‹œë¡œ ì¶”ê°€í•˜ì—¬ ì½”ë“œê°€ ë™ì‘í•˜ë„ë¡ í•¨
        example_questions = [
            "í•œí™”ì†í•´ë³´í—˜ì˜ 2024ë…„ 1ë¶„ê¸° ë‹¹ê¸°ìˆœì´ìµì€ ì–¼ë§ˆì¸ê°€ìš”?",
            "í•œí™”ì†í•´ë³´í—˜ì˜ ì£¼ìš” ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì „ëµì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "í•œí™”ì†í•´ë³´í—˜ì˜ ë””ì§€í„¸ ì „í™˜ ì „ëµì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "í•œí™”ì†í•´ë³´í—˜ì˜ ì§€ë°°êµ¬ì¡° íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "í•œí™”ì†í•´ë³´í—˜ì˜ ì£¼ìš” ë³´í—˜ ìƒí’ˆ ë¼ì¸ì—…ì€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?"
        ]
        
        return example_questions


class AutoTestManager:
    def __init__(self, rag_system: RAGSystem, test_count: int = 5):
        """ìë™ í…ŒìŠ¤íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.rag_system = rag_system
        self.test_count = test_count
        self.question_generator = AutoQuestionGenerator()
        self.evaluator = AutoEvaluator()
        self.available_models = ["gemma3:12b", "gemma3:7.8b", "claude3:sonnet", "claude3:haiku"]
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self.results = {
            "tests": [],
            "summary": {
                "avg_score": 0,
                "count": 0,
                "score_distribution": {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
            }
        }
    
    def run_auto_test(self, use_random_docs: bool = True, test_count: Optional[int] = None) -> Dict[str, Any]:
        """ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if test_count is not None:
            self.test_count = test_count
        
        test_results = []
        
        if use_random_docs:
            # ëœë¤ ë¬¸ì„œì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            docs = self.rag_system.vectorstore.get_random_documents(min(20, self.test_count * 2))
            for doc in docs[:self.test_count]:
                test_results.extend(self._run_test_on_document(doc))
        else:
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            # êµ¬í˜„ í•„ìš”ì‹œ ì¶”ê°€
            pass
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì—…ë°ì´íŠ¸
        self._update_summary()
        
        return self.results
    
    def _run_test_on_document(self, document) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ë¬¸ì„œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        context = document.page_content
        source = document.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
        
        # ì§ˆë¬¸ ìƒì„±
        try:
            questions = self.question_generator.generate_questions(context)
        except Exception as e:
            logging.error(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return []
        
        test_results = []
        
        # ê° ì§ˆë¬¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for question in questions[:1]:  # ë¬¸ì„œë‹¹ ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ ì‚¬ìš© (ë¶€í•˜ ì œí•œ)
            try:
                # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
                retrieved_docs = self.rag_system.search(question, top_k=5)
                answer = self.rag_system.answer(question, retrieved_docs)
                
                # ë‹µë³€ í‰ê°€
                evaluation = self.evaluator.evaluate_answer(
                    question=question,
                    context=context,
                    answer=answer
                )
                
                # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
                test_result = {
                    "question": question,
                    "source": source,
                    "context": context[:500] + ("..." if len(context) > 500 else ""),
                    "answer": answer,
                    "evaluation": evaluation,
                    "score": evaluation["score"],
                    "timestamp": datetime.now().isoformat()
                }
                
                test_results.append(test_result)
                self.results["tests"].append(test_result)
                
            except Exception as e:
                logging.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        
        return test_results
    
    def _update_summary(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì—…ë°ì´íŠ¸"""
        tests = self.results["tests"]
        
        if not tests:
            return
        
        # ì ìˆ˜ ë¶„í¬ ì´ˆê¸°í™”
        score_distribution = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
        
        # ê° í…ŒìŠ¤íŠ¸ ê²°ê³¼ì˜ ì ìˆ˜ ì¹´ìš´íŠ¸
        total_score = 0
        for test in tests:
            score = test.get("score", 0)
            if 1 <= score <= 5:
                score_distribution[score] += 1
                total_score += score
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = total_score / len(tests) if tests else 0
        
        # ìš”ì•½ ì—…ë°ì´íŠ¸
        self.results["summary"] = {
            "avg_score": round(avg_score, 2),
            "count": len(tests),
            "score_distribution": score_distribution
        }

def check_ollama_models() -> List[str]:
    """Ollama APIë¥¼ í†µí•´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    # ì´ë¯¸ ì‹¤í–‰ëœ ê²½ìš° ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
    if hasattr(check_ollama_models, '_cached_models'):
        return check_ollama_models._cached_models
    
    api_url = "http://localhost:11434/api/tags"
    try:
        logger.info("Ollama REST APIë¡œ ëª¨ë¸ ì¡°íšŒ ì‹œë„ ì¤‘...")
        print("ğŸ”„ Ollama REST APIë¡œ ëª¨ë¸ ì¡°íšŒ ì‹œë„ ì¤‘...")
        
        response = requests.get(api_url, timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            models = [model["name"] for model in data.get("models", [])]
            
            if models:
                logger.info(f"Ollama REST API ì—°ê²° ì„±ê³µ: {len(models)}ê°œ ëª¨ë¸ ë°œê²¬")
                print(f"âœ“ Ollama REST API ì—°ê²° ì„±ê³µ: {len(models)}ê°œ ëª¨ë¸ ë°œê²¬")
                
                # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ìºì‹œ
                check_ollama_models._cached_models = models
                return models
            else:
                logger.warning("Ollama APIì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                print("âš ï¸ Ollama APIì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.error(f"Ollama API ì˜¤ë¥˜: {response.status_code}")
            print(f"âš ï¸ Ollama API ì˜¤ë¥˜: {response.status_code}")
    except Exception as e:
        logger.error(f"Ollama API ì—°ê²° ì‹¤íŒ¨: {e}")
        print(f"âš ï¸ Ollama API ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # ê¸°ë³¸ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ì—°ê²° ì‹¤íŒ¨ì‹œ)
    check_ollama_models._cached_models = []
    return []

# --- ëª¨ë¸ í‰ê°€ ê´€ë¦¬ í´ë˜ìŠ¤ ---
class ModelEvaluator:
    def __init__(self):
        """ëª¨ë¸ í‰ê°€ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.evaluation_file = EVALUATION_FILE
        self.evaluations = self._load_evaluations()
    
    def _load_evaluations(self) -> Dict:
        """ì €ì¥ëœ í‰ê°€ ë°ì´í„° ë¡œë“œ"""
        os.makedirs(os.path.dirname(self.evaluation_file), exist_ok=True)
        if os.path.exists(self.evaluation_file):
            try:
                with open(self.evaluation_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ í‰ê°€ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
                return {"evaluations": []}
        return {"evaluations": []}
    
    def _save_evaluations(self):
        """í‰ê°€ ë°ì´í„° ì €ì¥"""
        os.makedirs(os.path.dirname(self.evaluation_file), exist_ok=True)
        try:
            with open(self.evaluation_file, 'w', encoding='utf-8') as f:
                json.dump(self.evaluations, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def save_evaluation(self, question: str, context: str, answers: Dict[str, str], metadata: Dict = None) -> str:
        """ìƒˆ í‰ê°€ ì„¸ì…˜ ì €ì¥"""
        # í‰ê°€ ID ìƒì„±
        evaluation_id = f"eval_{int(time.time())}_{hashlib.md5(question.encode()).hexdigest()[:8]}"
        
        # ìƒˆ í‰ê°€ ë°ì´í„° ìƒì„±
        evaluation = {
            "id": evaluation_id,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "question": question,
            "context": context[:1000] + "..." if len(context) > 1000 else context,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            "answers": answers,
            "scores": {},
            "metadata": metadata or {}
        }
        
        # í‰ê°€ ëª©ë¡ì— ì¶”ê°€
        self.evaluations["evaluations"].append(evaluation)
        self._save_evaluations()
        
        return evaluation_id
    
    def add_evaluation_score(self, evaluation_id: str, model_name: str, score: int, comments: str = ""):
        """íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ í‰ê°€ ì ìˆ˜ ì¶”ê°€"""
        for eval_item in self.evaluations["evaluations"]:
            if eval_item["id"] == evaluation_id:
                eval_item["scores"][model_name] = {
                    "score": score,
                    "comments": comments,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                }
                self._save_evaluations()
                return True
        return False
    
    def get_evaluation(self, evaluation_id: str) -> Dict:
        """íŠ¹ì • í‰ê°€ ë°ì´í„° ì¡°íšŒ"""
        for eval_item in self.evaluations["evaluations"]:
            if eval_item["id"] == evaluation_id:
                return eval_item
        return {}
    
    def get_all_evaluations(self) -> List[Dict]:
        """ëª¨ë“  í‰ê°€ ë°ì´í„° ì¡°íšŒ"""
        return self.evaluations["evaluations"]
    
    def get_model_avg_scores(self) -> Dict[str, float]:
        """ëª¨ë¸ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°"""
        model_scores = {}
        model_counts = {}
        
        for eval_item in self.evaluations["evaluations"]:
            for model, score_data in eval_item["scores"].items():
                score = score_data.get("score", 0)
                if model not in model_scores:
                    model_scores[model] = 0
                    model_counts[model] = 0
                model_scores[model] += score
                model_counts[model] += 1
        
        # í‰ê·  ê³„ì‚°
        avg_scores = {}
        for model, total_score in model_scores.items():
            count = model_counts.get(model, 0)
            avg_scores[model] = round(total_score / count, 2) if count > 0 else 0
        
        return avg_scores

if __name__ == "__main__":
    try:
        # ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main í•¨ìˆ˜ í˜¸ì¶œ
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ ì¢…ë£Œ (Ctrl+C)")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        sys.exit(1)